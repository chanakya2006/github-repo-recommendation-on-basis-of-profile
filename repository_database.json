{
  "repositories": [
    {
      "owner": "chanakya2006",
      "name": "github-repo-recommendation-on-basis-of-profile",
      "url": "https://github.com/chanakya2006/github-repo-recommendation-on-basis-of-profile",
      "description": "",
      "readme_content": "Foss-Hackathon-2025\nWe Going Big with this one",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:43.755061"
    },
    {
      "owner": "chanakya2006",
      "name": "fitness_api",
      "url": "https://github.com/chanakya2006/fitness_api",
      "description": "",
      "readme_content": "My first time creating a backend in GO using ECHO.",
      "languages": {
        "Go": 1,
        "go": 1,
        "GO": 1
      },
      "topics": [
        "Go"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:44.625944"
    },
    {
      "owner": "chanakya2006",
      "name": "pdf_chatbot",
      "url": "https://github.com/chanakya2006/pdf_chatbot",
      "description": "",
      "readme_content": "",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:45.436771"
    },
    {
      "owner": "chanakya2006",
      "name": "python",
      "url": "https://github.com/chanakya2006/python",
      "description": "",
      "readme_content": "python",
      "languages": {
        "python": 1,
        "Python": 1
      },
      "topics": [
        "Python"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:46.090163"
    },
    {
      "owner": "spf13",
      "name": "cobra",
      "url": "https://github.com/spf13/cobra",
      "description": "A Commander for modern Go CLI interactions",
      "readme_content": "Cobra is a library for creating powerful modern CLI applications.\nCobra is used in many Go projects such as Kubernetes,\nHugo, and GitHub CLI to\nname a few. This list contains a more extensive list of projects using Cobra.\n\n\n\n\nOverview\nCobra is a library providing a simple interface to create powerful modern CLI\ninterfaces similar to git & go tools.\nCobra provides:\n\nEasy subcommand-based CLIs: app server, app fetch, etc.\nFully POSIX-compliant flags (including short & long versions)\nNested subcommands\nGlobal, local and cascading flags\nIntelligent suggestions (app srver... did you mean app server?)\nAutomatic help generation for commands and flags\nGrouping help for subcommands\nAutomatic help flag recognition of -h, --help, etc.\nAutomatically generated shell autocomplete for your application (bash, zsh, fish, powershell)\nAutomatically generated man pages for your application\nCommand aliases so you can change things without breaking them\nThe flexibility to define your own help, usage, etc.\nOptional seamless integration with viper for 12-factor apps\n\nConcepts\nCobra is built on a structure of commands, arguments & flags.\nCommands represent actions, Args are things and Flags are modifiers for those actions.\nThe best applications read like sentences when used, and as a result, users\nintuitively know how to interact with them.\nThe pattern to follow is\nAPPNAME VERB NOUN --ADJECTIVE\nor\nAPPNAME COMMAND ARG --FLAG.\nA few good real world examples may better illustrate this point.\nIn the following example, 'server' is a command, and 'port' is a flag:\nhugo server --port=1313\n\nIn this command we are telling Git to clone the url bare.\ngit clone URL --bare\n\nCommands\nCommand is the central point of the application. Each interaction that\nthe application supports will be contained in a Command. A command can\nhave children commands and optionally run an action.\nIn the example above, 'server' is the command.\nMore about cobra.Command\nFlags\nA flag is a way to modify the behavior of a command. Cobra supports\nfully POSIX-compliant flags as well as the Go flag package.\nA Cobra command can define flags that persist through to children commands\nand flags that are only available to that command.\nIn the example above, 'port' is the flag.\nFlag functionality is provided by the pflag\nlibrary, a fork of the flag standard library\nwhich maintains the same interface while adding POSIX compliance.\nInstalling\nUsing Cobra is easy. First, use go get to install the latest version\nof the library.\ngo get -u github.com/spf13/cobra@latest\n\nNext, include Cobra in your application:\nimport \"github.com/spf13/cobra\"\nUsage\ncobra-cli is a command line program to generate cobra applications and command files.\nIt will bootstrap your application scaffolding to rapidly\ndevelop a Cobra-based application. It is the easiest way to incorporate Cobra into your application.\nIt can be installed by running:\ngo install github.com/spf13/cobra-cli@latest\n\nFor complete details on using the Cobra-CLI generator, please read The Cobra Generator README\nFor complete details on using the Cobra library, please read The Cobra User Guide.\nLicense\nCobra is released under the Apache 2.0 license. See LICENSE.txt",
      "languages": {
        "Go": 1,
        "go": 1,
        "GO": 1,
        "Shell": 1
      },
      "topics": [
        "Go",
        "Kubernetes",
        "Shell"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:48.190199"
    },
    {
      "owner": "subtrace",
      "name": "subtrace",
      "url": "https://github.com/subtrace/subtrace",
      "description": "Wireshark for Docker containers",
      "readme_content": "Subtrace\nHome ‚Äî Docs ‚Äî Discord\nWireshark for Docker containers\n\nSubtrace is Wireshark for your Docker containers. It lets developers see all\nincoming and outgoing requests in their backend server so that they can resolve\nproduction issues faster.\nFeatures\n\nWorks out-of-the-box\nNo code changes needed\nSupports all languages (Python + Node + Go + everything else)\nSee full payload, headers, status code, and latency\nLess than 100¬µs performance overhead\nBuilt on Clickhouse\nOpen source\n\nCode Contributions\nWhile Subtrace is open source,\nwe're not currently accepting pull requests. This is because we're a startup\nwith a very small team and we don't have the resources or documentation\nnecessary to maintain a good open source community in a way that still allows\nus to move quickly. This will probably change in the future.\nWith that said, we welcome all feature requests and bug reports, so feel free\nto open an issue.",
      "languages": {
        "python": 1,
        "Python": 1,
        "go": 1,
        "GO": 1,
        "Go": 1
      },
      "topics": [
        "Go",
        "Python",
        "Docker"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:48.842550"
    },
    {
      "owner": "chaitin",
      "name": "SafeLine",
      "url": "https://github.com/chaitin/SafeLine",
      "description": "SafeLine is a self-hosted WAF(Web Application Firewall) / reverse proxy to protect your web apps from attacks and exploits.",
      "readme_content": "SafeLine - Make your web apps secure\n\n\nüè† Website ¬† | ¬†\n  üìñ Docs ¬† | ¬†\n  üîç Live Demo ¬† | ¬†\n  üôã‚Äç‚ôÇÔ∏è Discord ¬† | ¬†\n  ‰∏≠ÊñáÁâà\n\nüëã INTRODUCTION\nSafeLine is a self-hosted WAF(Web Application Firewall) to protect your web apps from attacks and exploits.\nA web application firewall helps protect web apps by filtering and monitoring HTTP traffic between a web application and the Internet. It typically protects web apps from attacks such as SQL injection, XSS, code injection, os command injection, CRLF injection, ldap injection, xpath injection, RCE, XXE, SSRF, path traversal, backdoor, bruteforce, http-flood, bot abused, among others.\nüí° How It Works\n\nBy deploying a WAF in front of a web application, a shield is placed between the web application and the Internet. While a proxy server protects a client machine‚Äôs identity by using an intermediary, a WAF is a type of reverse-proxy, protecting the server from exposure by having clients pass through the WAF before reaching the server.\nA WAF protects your web apps by filtering, monitoring, and blocking any malicious HTTP/S traffic traveling to the web application, and prevents any unauthorized data from leaving the app. It does this by adhering to a set of policies that help determine what traffic is malicious and what traffic is safe. Just as a proxy server acts as an intermediary to protect the identity of a client, a WAF operates in similar fashion but acting as a reverse proxy intermediary that protects the web app server from a potentially malicious client.\nits core capabilities include:\n\nDefenses for web attacks\nProactive bot abused defense\nHTML & JS code encryption\nIP-based rate limiting\nWeb Access Control List\n\n‚ö°Ô∏è Screenshots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGet Live Demo\nüî• FEATURES\nList of the main features as follows:\n\nBlock Web Attacks\n\nIt defenses for all of web attacks, such as SQL injection, XSS, code injection, os command injection, CRLF injection, XXE, SSRF, path traversal and so on.\n\n\nRate Limiting\n\nDefend your web apps against DoS attacks, bruteforce attempts, traffic surges, and other types of abuse by throttling traffic that exceeds defined limits.\n\n\nAnti-Bot Challenge\n\nAnti-Bot challenges to protect your website from bot attacks, humen users will be allowed, crawlers and bots will be blocked.\n\n\nAuthentication Challenge\n\nWhen authentication challenge turned on, visitors need to enter the password, otherwise they will be blocked.\n\n\nDynamic Protection\n\nWhen dynamic protection turned on, html and js codes in your web server will be dynamically encrypted by each time you visit.\n\n\n\nüß© Showcases\n\n\n\n\nLegitimate User\nMalicious User\n\n\n\n\nBlock Web Attacks\n\n\n\n\nRate Limiting\n\n\n\n\nAnti-Bot Challenge\n\n\n\n\nAuth Challenge\n\n\n\n\nHTML Dynamic Protection\n\n\n\n\nJS Dynamic Protection\n\n\n\n\n\nüöÄ Quickstart\nWarning‰∏≠ÂõΩÂ§ßÈôÜÁî®Êà∑ÂÆâË£ÖÂõΩÈôÖÁâàÂèØËÉΩ‰ºöÂØºËá¥Êó†Ê≥ïËøûÊé•‰∫ëÊúçÂä°ÔºåËØ∑Êü•Áúã ‰∏≠ÊñáÁâàÂÆâË£ÖÊñáÊ°£\n\nüì¶ Installing\nInformation on how to install SafeLine can be found in the Install Guide\n‚öôÔ∏è Protecting Web Apps\nto see Configuration\nüìã More Informations\nEffect Evaluation\n\n\n\nMetric\nModSecurity, Level 1\nCloudFlare, Free\nSafeLine, Balance\nSafeLine, Strict\n\n\n\n\nTotal Samples\n33669\n33669\n33669\n33669\n\n\nDetection\n69.74%\n10.70%\n71.65%\n76.17%\n\n\nFalse Positive\n17.58%\n0.07%\n0.07%\n0.22%\n\n\nAccuracy\n82.20%\n98.40%\n99.45%\n99.38%\n\n\n\nIs SafeLine Production-Ready?\nYes, SafeLine is production-ready.\n\nOver 180,000 installations worldwide\nProtecting over 1,000,000 Websites\nHandling over 30,000,000,000 HTTP Requests Daily\n\nüôã‚Äç‚ôÇÔ∏è Community\nJoin our Discord to get community support, the core team members are identified by the STAFF role in Discord.\n\nchannel #feedback: for new features discussion.\nchannel #FAQ: for FAQ.\nchannel #general: for any other questions.\n\nSeveral contact options exist for our community, the primary one being Discord. These are in addition to GitHub issues for creating a new issue.\n\n ¬†\n   ¬†\n  \n\nüí™ PRO Edition\nComing soon!\nüìù License\nSee LICENSE for details.",
      "languages": {
        "SQL": 1,
        "HTML": 1
      },
      "topics": [
        "SQL"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:49.553648"
    },
    {
      "owner": "keploy",
      "name": "keploy",
      "url": "https://github.com/keploy/keploy",
      "description": "Unit and Integration Test generation for Developers. Generate tests and stubs for your application that actually work!",
      "readme_content": "‚ö°Ô∏è API tests faster than unit tests, from user traffic ‚ö°Ô∏è\n\n\n\nüåü The must-have tool for developers in the AI-Gen era üåü\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKeploy is developer-centric API testing tool that creates tests along with built-in-mocks, faster than unit tests.\nKeploy not only records API calls, but also records database calls and replays them during testing, making it easy to use, powerful, and extensible.\n\n\nüê∞ Fun fact: Keploy uses itself for testing! Check out our swanky coverage badge:  ¬†\n\nüö® Here for  Unit Test Generator (ut-gen)?\nKeploy has newly launched the world's first unit test generator(ut-gen) implementation of Meta LLM research paper, it understands code semantics and generates meaningful unit tests, aiming to:\n\n\nAutomate unit test generation (UTG): Quickly generate comprehensive unit tests and reduce redundant manual effort.\n\n\nImprove edge cases: Extend and improve the scope of automated tests to cover more complex scenarios, often missed manually.\n\n\nBoost test coverage: As codebases grow, ensuring exhaustive coverage should become feasible, aligning with our mission.\n\n\nüìú Follow Unit Test Generator README! ‚úÖ\nüìò Documentation!\nBecome a Keploy pro with Keploy Documentation.\n\nüöÄ Quick Installation (API test generator)\nIntegrate Keploy by installing the agent locally. No code-changes required.\ncurl --silent -O -L https://keploy.io/install.sh && source install.sh\nüé¨ Recording Testcases\nStart your app with Keploy to convert API calls as Tests and Mocks/Stubs.\nkeploy record -c \"CMD_TO_RUN_APP\" \nFor example, if you're using a simple Python app the CMD_TO_RUN_APP would resemble to python main.py, for  Golang go run main.go, for java java -jar xyz.jar, for node npm start..\nkeploy record -c \"python main.py\"\nüß™ Running Tests\nShut down the databases, redis, kafka or any other services your application uses. Keploy doesn't need those during test.\nkeploy test -c \"CMD_TO_RUN_APP\" --delay 10\n‚úÖ Test Coverage Integration\nTo integrate with your unit-testing library and see combine test coverage, follow this test-coverage guide.\n\nIf You Had Fun: Please leave a üåü star on this repo! It's free and will bring a smile. üòÑ üëè\n\nOne-Click Setup üöÄ\nSetup and run keploy quickly, with no local machine installation required:\n\nü§î Questions?\nReach out to us. We're here to help!\n\n\n\n\nüåê Language Support\nFrom Go's gopher üêπ to Python's snake üêç, we support:\n\n\n\n\n\n\nü´∞ Keploy Adopters üß°\nSo you and your organisation are using Keploy? That‚Äôs great. Please add yourselves to this list, and we'll send you goodies! üíñ\nWe are happy and proud to have you all as part of our community! üíñ\nüé© How's the Magic Happen?\nKeploy proxy captures and replays ALL (CRUD operations, including non-idempotent APIs) of your app's network interactions.\nTake a journey to How Keploy Works? to discover the tricks behind the curtain!\nHere are Keploy's core features: üõ†\n\n\n‚ôªÔ∏è Combined Test Coverage: Merge your Keploy Tests with your fave testing libraries(JUnit, go-test, py-test, jest) to see a combined test coverage.\n\n\nü§ñ EBPF Instrumentation: Keploy uses EBPF like a secret sauce to make integration code-less, language-agnostic, and oh-so-lightweight.\n\n\nüåê CI/CD Integration: Run tests with mocks anywhere you like‚Äîlocally on the CLI, in your CI pipeline (Jenkins, Github Actions..) , or even across a Kubernetes cluster.\n\n\nüìΩÔ∏è Record-Replay Complex Flows: Keploy can record and replay complex, distributed API flows as mocks and stubs. It's like having a time machine for your tests‚Äîsaving you tons of time!\n\n\nüé≠ Multi-Purpose Mocks: You can also use keploy Mocks, as server Tests!\n\n\nüë®üèª‚Äçüíª Let's Build Together! üë©üèª‚Äçüíª\nWhether you're a newbie coder or a wizard üßô‚Äç‚ôÄÔ∏è, your perspective is golden. Take a peek at our:\nüìú Contribution Guidelines\n‚ù§Ô∏è Code of Conduct\nüê≤ Current Limitations!\n\nUnit Testing: While Keploy is designed to run alongside unit testing frameworks (Go test, JUnit..) and can add to the overall code coverage, it still generates integration tests.\nProduction Lands: Keploy is currently focused on generating tests for developers. These tests can be captured from any environment, but we have not tested it on high volume production environments. This would need robust deduplication to avoid too many redundant tests being captured. We do have ideas on building a robust deduplication system #27\n\n‚ú® Resources!\nü§î FAQs\nüïµÔ∏è‚ÄçÔ∏è Why Keploy\n‚öôÔ∏è Installation Guide\nüìñ Contribution Guide",
      "languages": {
        "python": 1,
        "Python": 1,
        "go": 1,
        "GO": 1,
        "Go": 1,
        "java": 1,
        "Java": 1
      },
      "topics": [
        "Python",
        "Redis",
        "Kubernetes",
        "Go",
        "Java"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:50.278157"
    },
    {
      "owner": "k8sgpt-ai",
      "name": "k8sgpt",
      "url": "https://github.com/k8sgpt-ai/k8sgpt",
      "description": "Giving Kubernetes Superpowers to everyone",
      "readme_content": "k8sgpt is a tool for scanning your Kubernetes clusters, diagnosing, and triaging issues in simple English.\nIt has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.\nOut of the box integration with OpenAI, Azure, Cohere, Amazon Bedrock, Google Gemini and local models.\n \n\nCLI Installation\nLinux/Mac via brew\n$ brew install k8sgpt\nor\nbrew tap k8sgpt-ai/k8sgpt\nbrew install k8sgpt\n\nRPM-based installation (RedHat/CentOS/Fedora)\n32 bit:\nsudo rpm -ivh https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.49/k8sgpt_386.rpm\n\n64 bit:\nsudo rpm -ivh https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.49/k8sgpt_amd64.rpm\n\n\n\nDEB-based installation (Ubuntu/Debian)\n32 bit:\ncurl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.49/k8sgpt_386.deb\nsudo dpkg -i k8sgpt_386.deb\n\n64 bit:\ncurl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.49/k8sgpt_amd64.deb\nsudo dpkg -i k8sgpt_amd64.deb\n\n\n\nAPK-based installation (Alpine)\n32 bit:\nwget https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.49/k8sgpt_386.apk\napk add --allow-untrusted k8sgpt_386.apk\n\n64 bit:\nwget https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.49/k8sgpt_amd64.apk\napk add --allow-untrusted k8sgpt_amd64.apk\n\n\n\nFailing Installation on WSL or Linux (missing gcc)\n  When installing Homebrew on WSL or Linux, you may encounter the following error:\n==> Installing k8sgpt from k8sgpt-ai/k8sgpt Error: The following formula cannot be installed from a bottle and must be\nbuilt from the source. k8sgpt Install Clang or run brew install gcc.\n\nIf you install gcc as suggested, the problem will persist. Therefore, you need to install the build-essential package.\n   sudo apt-get update\n   sudo apt-get install build-essential\n\n\nWindows\n\nDownload the latest Windows binaries of k8sgpt from the Release\ntab based on your system architecture.\nExtract the downloaded package to your desired location. Configure the system path variable with the binary location\n\nOperator Installation\nTo install within a Kubernetes cluster please use our k8sgpt-operator with installation instructions available here\nThis mode of operation is ideal for continuous monitoring of your cluster and can integrate with your existing monitoring such as Prometheus and Alertmanager.\nQuick Start\n\nCurrently, the default AI provider is OpenAI, you will need to generate an API key from OpenAI\n\nYou can do this by running k8sgpt generate to open a browser link to generate it\n\n\nRun k8sgpt auth add to set it in k8sgpt.\n\nYou can provide the password directly using the --password flag.\n\n\nRun k8sgpt filters to manage the active filters used by the analyzer. By default, all filters are executed during analysis.\nRun k8sgpt analyze to run a scan.\nAnd use k8sgpt analyze --explain to get a more detailed explanation of the issues.\nYou also run k8sgpt analyze --with-doc (with or without the explain flag) to get the official documentation from Kubernetes.\n\nAnalyzers\nK8sGPT uses analyzers to triage and diagnose issues in your cluster. It has a set of analyzers that are built in, but\nyou will be able to write your own analyzers.\nBuilt in analyzers\nEnabled by default\n\n podAnalyzer\n pvcAnalyzer\n rsAnalyzer\n serviceAnalyzer\n eventAnalyzer\n ingressAnalyzer\n statefulSetAnalyzer\n deploymentAnalyzer\n cronJobAnalyzer\n nodeAnalyzer\n mutatingWebhookAnalyzer\n validatingWebhookAnalyzer\n\nOptional\n\n hpaAnalyzer\n pdbAnalyzer\n networkPolicyAnalyzer\n gatewayClass\n gateway\n httproute\n logAnalyzer\n\nExamples\nRun a scan with the default analyzers\nk8sgpt generate\nk8sgpt auth add\nk8sgpt analyze --explain\nk8sgpt analyze --explain --with-doc\n\nFilter on resource\nk8sgpt analyze --explain --filter=Service\n\nFilter by namespace\nk8sgpt analyze --explain --filter=Pod --namespace=default\n\nOutput to JSON\nk8sgpt analyze --explain --filter=Service --output=json\n\nAnonymize during explain\nk8sgpt analyze --explain --filter=Service --output=json --anonymize\n\n\n Using filters \nList filters\nk8sgpt filters list\n\nAdd default filters\nk8sgpt filters add [filter(s)]\n\nExamples :\n\nSimple filter : k8sgpt filters add Service\nMultiple filters : k8sgpt filters add Ingress,Pod\n\nRemove default filters\nk8sgpt filters remove [filter(s)]\n\nExamples :\n\nSimple filter : k8sgpt filters remove Service\nMultiple filters : k8sgpt filters remove Ingress,Pod\n\n\n\n Additional commands \nList configured backends\nk8sgpt auth list\n\nUpdate configured backends\nk8sgpt auth update $MY_BACKEND1,$MY_BACKEND2..\n\nRemove configured backends\nk8sgpt auth remove -b $MY_BACKEND1,$MY_BACKEND2..\n\nList integrations\nk8sgpt integrations list\n\nActivate integrations\nk8sgpt integrations activate [integration(s)]\n\nUse integration\nk8sgpt analyze --filter=[integration(s)]\n\nDeactivate integrations\nk8sgpt integrations deactivate [integration(s)]\n\nServe mode\nk8sgpt serve\n\nAnalysis with serve mode\ngrpcurl -plaintext -d '{\"namespace\": \"k8sgpt\", \"explain\" : \"true\"}' localhost:8080 schema.v1.ServerAnalyzerService/Analyze\n{\n  \"status\": \"OK\"\n}\n\nAnalysis with custom headers\nk8sgpt analyze --explain --custom-headers CustomHeaderKey:CustomHeaderValue\n\nPrint analysis stats\nk8sgpt analyze -s\nThe stats mode allows for debugging and understanding the time taken by an analysis by displaying the statistics of each analyzer.\n- Analyzer Ingress took 47.125583ms \n- Analyzer PersistentVolumeClaim took 53.009167ms \n- Analyzer CronJob took 57.517792ms \n- Analyzer Deployment took 156.6205ms \n- Analyzer Node took 160.109833ms \n- Analyzer ReplicaSet took 245.938333ms \n- Analyzer StatefulSet took 448.0455ms \n- Analyzer Pod took 5.662594708s \n- Analyzer Service took 38.583359166s\n\nDiagnostic information\nTo collect diagnostic information use the following command to create a dump_<timestamp>_json in your local directory.\nk8sgpt dump\n\n\nLLM AI Backends\nK8sGPT uses the chosen LLM, generative AI provider when you want to explain the analysis results using --explain flag e.g. k8sgpt analyze --explain. You can use --backend flag to specify a configured provider (it's openai by default).\nYou can list available providers using k8sgpt auth list:\nDefault:\n> openai\nActive:\nUnused:\n> openai\n> localai\n> ollama\n> azureopenai\n> cohere\n> amazonbedrock\n> amazonsagemaker\n> google\n> huggingface\n> noopai\n> googlevertexai\n> ibmwatsonxai\n\nFor detailed documentation on how to configure and use each provider see here.\nTo set a new default provider\nk8sgpt auth default -p azureopenai\nDefault provider set to azureopenai\n\nKey Features\n\nWith this option, the data is anonymized before being sent to the AI Backend. During the analysis execution, k8sgpt retrieves sensitive data (Kubernetes object names, labels, etc.). This data is masked when sent to the AI backend and replaced by a key that can be used to de-anonymize the data when the solution is returned to the user.\n Anonymization \n\nError reported during analysis:\n\nError: HorizontalPodAutoscaler uses StatefulSet/fake-deployment as ScaleTargetRef which does not exist.\n\nPayload sent to the AI backend:\n\nError: HorizontalPodAutoscaler uses StatefulSet/tGLcCRcHa1Ce5Rs as ScaleTargetRef which does not exist.\n\nPayload returned by the AI:\n\nThe Kubernetes system is trying to scale a StatefulSet named tGLcCRcHa1Ce5Rs using the HorizontalPodAutoscaler, but it cannot find the StatefulSet. The solution is to verify that the StatefulSet name is spelled correctly and exists in the same namespace as the HorizontalPodAutoscaler.\n\nPayload returned to the user:\n\nThe Kubernetes system is trying to scale a StatefulSet named fake-deployment using the HorizontalPodAutoscaler, but it cannot find the StatefulSet. The solution is to verify that the StatefulSet name is spelled correctly and exists in the same namespace as the HorizontalPodAutoscaler.\nNote: Anonymization does not currently apply to events.\nFurther Details\nAnonymization does not currently apply to events.\nIn a few analysers like Pod, we feed to the AI backend the event messages which are not known beforehand thus we are not masking them for the time being.\n\n\nThe following is the list of analysers in which data is being masked:-\n\nStatefulset\nService\nPodDisruptionBudget\nNode\nNetworkPolicy\nIngress\nHPA\nDeployment\nCronjob\n\n\n\nThe following is the list of analysers in which data is not being masked:-\n\nRepicaSet\nPersistentVolumeClaim\nPod\nLog\n*Events\n\n\n\n*Note:\n\n\nk8gpt will not mask the above analysers because they do not send any identifying information except Events analyser.\n\n\nMasking for Events analyzer is scheduled in the near future as seen in this issue. Further research has to be made to understand the patterns and be able to mask the sensitive parts of an event like pod name, namespace etc.\n\n\nThe following is the list of fields which are not being masked:-\n\nDescribe\nObjectStatus\nReplicas\nContainerStatus\n*Event Message\nReplicaStatus\nCount (Pod)\n\n\n\n*Note:\n\nIt is quite possible the payload of the event message might have something like \"super-secret-project-pod-X crashed\" which we don't currently redact (scheduled in the near future as seen in this issue).\n\nProceed with care\n\nThe K8gpt team recommends using an entirely different backend (a local model) in critical production environments. By using a local model, you can rest assured that everything stays within your DMZ, and nothing is leaked.\nIf there is any uncertainty about the possibility of sending data to a public LLM (open AI, Azure AI) and it poses a risk to business-critical operations, then, in such cases, the use of public LLM should be avoided based on personal assessment and the jurisdiction of risks involved.\n\n\n\n Configuration management\nk8sgpt stores config data in the $XDG_CONFIG_HOME/k8sgpt/k8sgpt.yaml file. The data is stored in plain text, including your OpenAI key.\nConfig file locations:\n\n\n\nOS\nPath\n\n\n\n\nMacOS\n~/Library/Application Support/k8sgpt/k8sgpt.yaml\n\n\nLinux\n~/.config/k8sgpt/k8sgpt.yaml\n\n\nWindows\n%LOCALAPPDATA%/k8sgpt/k8sgpt.yaml\n\n\n\n\n\nThere may be scenarios where caching remotely is preferred.\nIn these scenarios K8sGPT supports AWS S3 or Azure Blob storage Integration.\n Remote caching \nNote: You can only configure and use only one remote cache at a time\nAdding a remote cache\n\nAWS S3\n\nAs a prerequisite AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are required as environmental variables.\nConfiguration, k8sgpt cache add s3 --region <aws region> --bucket <name>\nMinio Configuration with HTTP endpoint  k8sgpt cache add s3 --bucket <name> --endpoint <http://localhost:9000>\nMinio Configuration with HTTPs endpoint, skipping TLS verification  k8sgpt cache add s3 --bucket <name> --endpoint <https://localhost:9000> --insecure\n\nK8sGPT will create the bucket if it does not exist\n\n\n\n\nAzure Storage\n\nWe support a number of techniques to authenticate against Azure\nConfiguration, k8sgpt cache add azure --storageacc <storage account name> --container <container name>\n\nK8sGPT assumes that the storage account already exist and it will create the container if it does not exist\nIt is the user responsibility have to grant specific permissions to their identity in order to be able to upload blob files and create SA containers (e.g Storage Blob Data Contributor)\n\n\n\n\nGoogle Cloud Storage\n\nAs a prerequisite GOOGLE_APPLICATION_CREDENTIALS are required as environmental variables.\nConfiguration,  k8sgpt cache add gcs --region <gcp region> --bucket <name> --projectid <project id>\n\nK8sGPT will create the bucket if it does not exist\n\n\n\n\n\nListing cache items\nk8sgpt cache list\n\nPurging an object from the cache\nNote: purging an object using this command will delete upstream files, so it requires appropriate permissions.\nk8sgpt cache purge $OBJECT_NAME\n\nRemoving the remote cache\nNote: this will not delete the upstream S3 bucket or Azure storage container\nk8sgpt cache remove\n\n\n\n Custom Analyzers\nThere may be scenarios where you wish to write your own analyzer in a language of your choice.\nK8sGPT now supports the ability to do so by abiding by the schema and serving the analyzer for consumption.\nTo do so, define the analyzer within the K8sGPT configuration and it will add it into the scanning process.\nIn addition to this you will need to enable the following flag on analysis:\nk8sgpt analyze --custom-analysis\n\nHere is an example local host analyzer in Rust\nWhen this is run on localhost:8080 the K8sGPT config can pick it up with the following additions:\ncustom_analyzers:\n  - name: host-analyzer\n    connection:\n      url: localhost\n      port: 8080\n\nThis now gives the ability to pass through hostOS information ( from this analyzer example ) to K8sGPT to use as context with normal analysis.\nSee the docs on how to write a custom analyzer\nListing custom analyzers configured\nk8sgpt custom-analyzer list\n\nAdding custom analyzer without install\nk8sgpt custom-analyzer add --name my-custom-analyzer --port 8085\n\nRemoving custom analyzer\nk8sgpt custom-analyzer remove --names \"my-custom-analyzer,my-custom-analyzer-2\"\n\n\nDocumentation\nFind our official documentation available here\nContributing\nPlease read our contributing guide.\nCommunity\nFind us on Slack\n\n\n\nLicense",
      "languages": {
        "rust": 1,
        "Rust": 1
      },
      "topics": [
        "Rust",
        "Azure",
        "AWS",
        "Kubernetes"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:51.009545"
    },
    {
      "owner": "gitleaks",
      "name": "gitleaks",
      "url": "https://github.com/gitleaks/gitleaks",
      "description": "Find secrets with Gitleaks üîë",
      "readme_content": "Gitleaks\n‚îå‚îÄ‚óã‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ ‚îÇ‚ï≤  ‚îÇ\n‚îÇ ‚îÇ ‚óã ‚îÇ\n‚îÇ ‚óã ‚ñë ‚îÇ\n‚îî‚îÄ‚ñë‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJoin our Discord! \nGitleaks is a tool for detecting secrets like passwords, API keys, and tokens in git repos, files, and whatever else you wanna throw at it via stdin.\n‚ûú  ~/code(master) gitleaks git -v\n\n    ‚óã\n    ‚îÇ‚ï≤\n    ‚îÇ ‚óã\n    ‚óã ‚ñë\n    ‚ñë    gitleaks\n\n\nFinding:     \"export BUNDLE_ENTERPRISE__CONTRIBSYS__COM=cafebabe:deadbeef\",\nSecret:      cafebabe:deadbeef\nRuleID:      sidekiq-secret\nEntropy:     2.609850\nFile:        cmd/generate/config/rules/sidekiq.go\nLine:        23\nCommit:      cd5226711335c68be1e720b318b7bc3135a30eb2\nAuthor:      John\nEmail:       john@users.noreply.github.com\nDate:        2022-08-03T12:31:40Z\nFingerprint: cd5226711335c68be1e720b318b7bc3135a30eb2:cmd/generate/config/rules/sidekiq.go:sidekiq-secret:23\n\nGetting Started\nGitleaks can be installed using Homebrew, Docker, or Go. Gitleaks is also available in binary form for many popular platforms and OS types on the releases page. In addition, Gitleaks can be implemented as a pre-commit hook directly in your repo or as a GitHub action using Gitleaks-Action.\nInstalling\n# MacOS\nbrew install gitleaks\n\n# Docker (DockerHub)\ndocker pull zricethezav/gitleaks:latest\ndocker run -v ${path_to_host_folder_to_scan}:/path zricethezav/gitleaks:latest [COMMAND] [OPTIONS] [SOURCE_PATH]\n\n# Docker (ghcr.io)\ndocker pull ghcr.io/gitleaks/gitleaks:latest\ndocker run -v ${path_to_host_folder_to_scan}:/path ghcr.io/gitleaks/gitleaks:latest [COMMAND] [OPTIONS] [SOURCE_PATH]\n\n# From Source (make sure `go` is installed)\ngit clone https://github.com/gitleaks/gitleaks.git\ncd gitleaks\nmake build\nGitHub Action\nCheck out the official Gitleaks GitHub Action\nname: gitleaks\non: [pull_request, push, workflow_dispatch]\njobs:\n  scan:\n    name: gitleaks\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n      - uses: gitleaks/gitleaks-action@v2\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          GITLEAKS_LICENSE: ${{ secrets.GITLEAKS_LICENSE}} # Only required for Organizations, not personal accounts.\n\nPre-Commit\n\n\nInstall pre-commit from https://pre-commit.com/#install\n\n\nCreate a .pre-commit-config.yaml file at the root of your repository with the following content:\nrepos:\n  - repo: https://github.com/gitleaks/gitleaks\n    rev: v8.23.1\n    hooks:\n      - id: gitleaks\n\nfor a native execution of GitLeaks or use the gitleaks-docker pre-commit ID for executing GitLeaks using the official Docker images\n\n\nAuto-update the config to the latest repos' versions by executing pre-commit autoupdate\n\n\nInstall with pre-commit install\n\n\nNow you're all set!\n\n\n‚ûú git commit -m \"this commit contains a secret\"\nDetect hardcoded secrets.................................................Failed\n\nNote: to disable the gitleaks pre-commit hook you can prepend SKIP=gitleaks to the commit command\nand it will skip running gitleaks\n‚ûú SKIP=gitleaks git commit -m \"skip gitleaks check\"\nDetect hardcoded secrets................................................Skipped\n\nUsage\nUsage:\n  gitleaks [command]\n\nAvailable Commands:\n  completion  generate the autocompletion script for the specified shell\n  dir         scan directories or files for secrets\n  git         scan git repositories for secrets\n  help        Help about any command\n  stdin       detect secrets from stdin\n  version     display gitleaks version\n\nFlags:\n  -b, --baseline-path string          path to baseline with issues that can be ignored\n  -c, --config string                 config file path\n                                      order of precedence:\n                                      1. --config/-c\n                                      2. env var GITLEAKS_CONFIG\n                                      3. (target path)/.gitleaks.toml\n                                      If none of the three options are used, then gitleaks will use the default config\n      --enable-rule strings           only enable specific rules by id\n      --exit-code int                 exit code when leaks have been encountered (default 1)\n  -i, --gitleaks-ignore-path string   path to .gitleaksignore file or folder containing one (default \".\")\n  -h, --help                          help for gitleaks\n      --ignore-gitleaks-allow         ignore gitleaks:allow comments\n  -l, --log-level string              log level (trace, debug, info, warn, error, fatal) (default \"info\")\n      --max-decode-depth int          allow recursive decoding up to this depth (default \"0\", no decoding is done)\n      --max-target-megabytes int      files larger than this will be skipped\n      --no-banner                     suppress banner\n      --no-color                      turn off color for verbose output\n      --redact uint[=100]             redact secrets from logs and stdout. To redact only parts of the secret just apply a percent value from 0..100. For example --redact=20 (default 100%)\n  -f, --report-format string          output format (json, csv, junit, sarif) (default \"json\")\n  -r, --report-path string            report file\n      --report-template string        template file used to generate the report (implies --report-format=template)\n  -v, --verbose                       show verbose output from scan\n      --version                       version for gitleaks\n\nUse \"gitleaks [command] --help\" for more information about a command.\n\nCommands\n‚ö†Ô∏è v8.19.0 introduced a change that deprecated detect and protect. Those commands are still available but\nare hidden in the --help menu. Take a look at this gist for easy command translations.\nIf you find v8.19.0 broke an existing command (detect/protect), please open an issue.\nThere are three scanning modes: git, dir, and stdin.\nGit\nThe git command lets you scan local git repos. Under the hood, gitleaks uses the git log -p command to scan patches.\nYou can configure the behavior of git log -p with the log-opts option.\nFor example, if you wanted to run gitleaks on a range of commits you could use the following\ncommand: gitleaks git -v --log-opts=\"--all commitA..commitB\" path_to_repo. See the git log documentation for more information.\nIf there is no target specified as a positional argument, then gitleaks will attempt to scan the current working directory as a git repo.\nDir\nThe dir (aliases include files, directory) command lets you scan directories and files. Example: gitleaks dir -v path_to_directory_or_file.\nIf there is no target specified as a positional argument, then gitleaks will scan the current working directory.\nStdin\nYou can also stream data to gitleaks with the stdin command. Example: cat some_file | gitleaks -v stdin\nCreating a baseline\nWhen scanning large repositories or repositories with a long history, it can be convenient to use a baseline. When using a baseline,\ngitleaks will ignore any old findings that are present in the baseline. A baseline can be any gitleaks report. To create a gitleaks report, run gitleaks with the --report-path parameter.\ngitleaks git --report-path gitleaks-report.json # This will save the report in a file called gitleaks-report.json\n\nOnce as baseline is created it can be applied when running the detect command again:\ngitleaks git --baseline-path gitleaks-report.json --report-path findings.json\n\nAfter running the detect command with the --baseline-path parameter, report output (findings.json) will only contain new issues.\nPre-Commit hook\nYou can run Gitleaks as a pre-commit hook by copying the example pre-commit.py script into\nyour .git/hooks/ directory.\nConfiguration\nGitleaks offers a configuration format you can follow to write your own secret detection rules:\n# Title for the gitleaks configuration file.\ntitle = \"Custom Gitleaks configuration\"\n\n# You have basically two options for your custom configuration:\n#\n# 1. define your own configuration, default rules do not apply\n#\n#    use e.g., the default configuration as starting point:\n#    https://github.com/gitleaks/gitleaks/blob/master/config/gitleaks.toml\n#\n# 2. extend a configuration, the rules are overwritten or extended\n#\n#    When you extend a configuration the extended rules take precedence over the\n#    default rules. I.e., if there are duplicate rules in both the extended\n#    configuration and the default configuration the extended rules or\n#    attributes of them will override the default rules.\n#    Another thing to know with extending configurations is you can chain\n#    together multiple configuration files to a depth of 2. Allowlist arrays are\n#    appended and can contain duplicates.\n\n# useDefault and path can NOT be used at the same time. Choose one.\n[extend]\n# useDefault will extend the default gitleaks config built in to the binary\n# the latest version is located at:\n# https://github.com/gitleaks/gitleaks/blob/master/config/gitleaks.toml\nuseDefault = true\n# or you can provide a path to a configuration to extend from.\n# The path is relative to where gitleaks was invoked,\n# not the location of the base config.\n# path = \"common_config.toml\"\n# If there are any rules you don't want to inherit, they can be specified here.\ndisabledRules = [ \"generic-api-key\"]\n\n# An array of tables that contain information that define instructions\n# on how to detect secrets\n[[rules]]\n\n# Unique identifier for this rule\nid = \"awesome-rule-1\"\n\n# Short human readable description of the rule.\ndescription = \"awesome rule 1\"\n\n# Golang regular expression used to detect secrets. Note Golang's regex engine\n# does not support lookaheads.\nregex = '''one-go-style-regex-for-this-rule'''\n\n# Int used to extract secret from regex match and used as the group that will have\n# its entropy checked if `entropy` is set.\nsecretGroup = 3\n\n# Float representing the minimum shannon entropy a regex group must have to be considered a secret.\nentropy = 3.5\n\n# Golang regular expression used to match paths. This can be used as a standalone rule or it can be used\n# in conjunction with a valid `regex` entry.\npath = '''a-file-path-regex'''\n\n# Keywords are used for pre-regex check filtering. Rules that contain\n# keywords will perform a quick string compare check to make sure the\n# keyword(s) are in the content being scanned. Ideally these values should\n# either be part of the identiifer or unique strings specific to the rule's regex\n# (introduced in v8.6.0)\nkeywords = [\n  \"auth\",\n  \"password\",\n  \"token\",\n]\n\n# Array of strings used for metadata and reporting purposes.\ntags = [\"tag\",\"another tag\"]\n\n    # ‚ö†Ô∏è In v8.21.0 `[rules.allowlist]` was replaced with `[[rules.allowlists]]`.\n    # This change was backwards-compatible: instances of `[rules.allowlist]` still  work.\n    #\n    # You can define multiple allowlists for a rule to reduce false positives.\n    # A finding will be ignored if _ANY_ `[[rules.allowlists]]` matches.\n    [[rules.allowlists]]\n    description = \"ignore commit A\"\n    # When multiple criteria are defined the default condition is \"OR\".\n    # e.g., this can match on |commits| OR |paths| OR |stopwords|.\n    condition = \"OR\"\n    commits = [ \"commit-A\", \"commit-B\"]\n    paths = [\n      '''go\\.mod''',\n      '''go\\.sum'''\n    ]\n    # note: stopwords targets the extracted secret, not the entire regex match\n    # like 'regexes' does. (stopwords introduced in 8.8.0)\n    stopwords = [\n      '''client''',\n      '''endpoint''',\n    ]\n\n    [[rules.allowlists]]\n    # The \"AND\" condition can be used to make sure all criteria match.\n    # e.g., this matches if |regexes| AND |paths| are satisfied.\n    condition = \"AND\"\n    # note: |regexes| defaults to check the _Secret_ in the finding.\n    # Acceptable values for |regexTarget| are \"secret\" (default), \"match\", and \"line\".\n    regexTarget = \"match\"\n    regexes = [ '''(?i)parseur[il]''' ]\n    paths = [ '''package-lock\\.json''' ]\n\n# You can extend a particular rule from the default config. e.g., gitlab-pat\n# if you have defined a custom token prefix on your GitLab instance\n[[rules]]\nid = \"gitlab-pat\"\n# all the other attributes from the default rule are inherited\n\n    [[rules.allowlists]]\n    regexTarget = \"line\"\n    regexes = [ '''MY-glpat-''' ]\n\n# This is a global allowlist which has a higher order of precedence than rule-specific allowlists.\n# If a commit listed in the `commits` field below is encountered then that commit will be skipped and no\n# secrets will be detected for said commit. The same logic applies for regexes and paths.\n[allowlist]\ndescription = \"global allow list\"\ncommits = [ \"commit-A\", \"commit-B\", \"commit-C\"]\npaths = [\n  '''gitleaks\\.toml''',\n  '''(.*?)(jpg|gif|doc)'''\n]\n\n# note: (global) regexTarget defaults to check the _Secret_ in the finding.\n# if regexTarget is not specified then _Secret_ will be used.\n# Acceptable values for regexTarget are \"match\" and \"line\"\nregexTarget = \"match\"\nregexes = [\n  '''219-09-9999''',\n  '''078-05-1120''',\n  '''(9[0-9]{2}|666)-\\d{2}-\\d{4}''',\n]\n# note: stopwords targets the extracted secret, not the entire regex match\n# like 'regexes' does. (stopwords introduced in 8.8.0)\nstopwords = [\n  '''client''',\n  '''endpoint''',\n]\nRefer to the default gitleaks config for examples or follow the contributing guidelines if you would like to contribute to the default configuration. Additionally, you can check out this gitleaks blog post which covers advanced configuration setups.\nAdditional Configuration\ngitleaks:allow\nIf you are knowingly committing a test secret that gitleaks will catch you can add a gitleaks:allow comment to that line which will instruct gitleaks\nto ignore that secret. Ex:\nclass CustomClass:\n    discord_client_secret = '8dyfuiRyq=vVc3RRr_edRk-fK__JItpZ'  #gitleaks:allow\n\n\n.gitleaksignore\nYou can ignore specific findings by creating a .gitleaksignore file at the root of your repo. In release v8.10.0 Gitleaks added a Fingerprint value to the Gitleaks report. Each leak, or finding, has a Fingerprint that uniquely identifies a secret. Add this fingerprint to the .gitleaksignore file to ignore that specific secret. See Gitleaks' .gitleaksignore for an example. Note: this feature is experimental and is subject to change in the future.\nDecoding\nSometimes secrets are encoded in a way that can make them difficult to find\nwith just regex. Now you can tell gitleaks to automatically find and decode\nencoded text. The flag --max-decode-depth enables this feature (the default\nvalue \"0\" means the feature is disabled by default).\nRecursive decoding is supported since decoded text can also contain encoded\ntext.  The flag --max-decode-depth sets the recursion limit. Recursion stops\nwhen there are no new segments of encoded text to decode, so setting a really\nhigh max depth doesn't mean it will make that many passes. It will only make as\nmany as it needs to decode the text. Overall, decoding only minimally increases\nscan times.\nThe findings for encoded text differ from normal findings in the following\nways:\n\nThe location points the bounds of the encoded text\n\nIf the rule matches outside the encoded text, the bounds are adjusted to\ninclude that as well\n\n\nThe match and secret contain the decoded value\nTwo tags are added decoded:<encoding> and decode-depth:<depth>\n\nCurrently supported encodings:\n\nbase64 (both standard and base64url)\n\nReporting\nGitleaks has built-in support for several report formats: json, csv, junit, and sarif.\nIf none of these formats fit your need, you can create your own report format with a Go text/template .tmpl file and the --report-template flag. The template can use extended functionality from the Masterminds/sprig template library.\nFor example, the following template provides a custom JSON output:\n# jsonextra.tmpl\n[{{ $lastFinding := (sub (len . ) 1) }}\n{{- range $i, $finding := . }}{{with $finding}}\n    {\n        \"Description\": {{ quote .Description }},\n        \"StartLine\": {{ .StartLine }},\n        \"EndLine\": {{ .EndLine }},\n        \"StartColumn\": {{ .StartColumn }},\n        \"EndColumn\": {{ .EndColumn }},\n        \"Line\": {{ quote .Line }},\n        \"Match\": {{ quote .Match }},\n        \"Secret\": {{ quote .Secret }},\n        \"File\": \"{{ .File }}\",\n        \"SymlinkFile\": {{ quote .SymlinkFile }},\n        \"Commit\": {{ quote .Commit }},\n        \"Entropy\": {{ .Entropy }},\n        \"Author\": {{ quote .Author }},\n        \"Email\": {{ quote .Email }},\n        \"Date\": {{ quote .Date }},\n        \"Message\": {{ quote .Message }},\n        \"Tags\": [{{ $lastTag := (sub (len .Tags ) 1) }}{{ range $j, $tag := .Tags }}{{ quote . }}{{ if ne $j $lastTag }},{{ end }}{{ end }}],\n        \"RuleID\": {{ quote .RuleID }},\n        \"Fingerprint\": {{ quote .Fingerprint }}\n    }{{ if ne $i $lastFinding }},{{ end }}\n{{- end}}{{ end }}\n]\n\nUsage:\n$ gitleaks dir ~/leaky-repo/ --report-path \"report.json\" --report-format template --report-template testdata/report/jsonextra.tmpl\nSponsorships\n\ncoderabbit.ai\n\n\n\n\nExit Codes\nYou can always set the exit code when leaks are encountered with the --exit-code flag. Default exit codes below:\n0 - no leaks present\n1 - leaks or error encountered\n126 - unknown flag",
      "languages": {
        "R": 1,
        "go": 1,
        "GO": 1,
        "Shell": 1,
        "Go": 1
      },
      "topics": [
        "Go",
        "Docker",
        "R",
        "Shell"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:51.899875"
    },
    {
      "owner": "danielmiessler",
      "name": "fabric",
      "url": "https://github.com/danielmiessler/fabric",
      "description": "fabric is an open-source framework for augmenting humans using AI. It provides a modular framework for solving specific problems using a crowdsourced set of AI prompts that can be used anywhere.",
      "readme_content": "fabric\n\n\n\n\n\n\nfabric is an open-source framework for augmenting humans using AI.\n\nUpdates ‚Ä¢\nWhat and Why ‚Ä¢\nPhilosophy ‚Ä¢\nInstallation ‚Ä¢\nUsage ‚Ä¢\nExamples ‚Ä¢\nJust Use the Patterns ‚Ä¢\nCustom Patterns ‚Ä¢\nHelper Apps ‚Ä¢\nMeta\n\n\nNavigation\n\nfabric\n\nNavigation\nUpdates\nIntro videos\nWhat and why\nPhilosophy\n\nBreaking problems into components\nToo many prompts\n\n\nInstallation\n\nGet Latest Release Binaries\nFrom Source\nEnvironment Variables\nSetup\nAdd aliases for all patterns\n\nSave your files in markdown using aliases\n\n\nMigration\nUpgrading\n\n\nUsage\nOur approach to prompting\nExamples\nJust use the Patterns\nCustom Patterns\nHelper Apps\n\nto_pdf\nto_pdf Installation\n\n\npbpaste\nWeb Interface\nMeta\n\nPrimary contributors\n\n\n\n\n\n\nUpdates\nNoteFebruary 5, 2025\n\nRemember that fabric supports o1 and o3 models, but you need to 1) not use -s, and 2) use the --raw flag because the o1 and o3 models don't support the --stream option or temperature settings.\n\n\nWhat and why\nSince the start of 2023 and GenAI we've seen a massive number of AI applications for accomplishing tasks. It's powerful, but it's not easy to integrate this functionality into our lives.\n\nIn other words, AI doesn't have a capabilities problem‚Äîit has an integration problem.\n\nFabric was created to address this by enabling everyone to granularly apply AI to everyday challenges.\nIntro videos\nKeep in mind that many of these were recorded when Fabric was Python-based, so remember to use the current install instructions below.\n\nNetwork Chuck\nDavid Bombal\nMy Own Intro to the Tool\nMore Fabric YouTube Videos\n\nPhilosophy\n\nAI isn't a thing; it's a magnifier of a thing. And that thing is human creativity.\n\nWe believe the purpose of technology is to help humans flourish, so when we talk about AI we start with the human problems we want to solve.\nBreaking problems into components\nOur approach is to break problems into individual pieces (see below) and then apply AI to them one at a time. See below for some examples.\n\nToo many prompts\nPrompts are good for this, but the biggest challenge I faced in 2023‚Äî‚Äîwhich still exists today‚Äîis the sheer number of AI prompts out there. We all have prompts that are useful, but it's hard to discover new ones, know if they are good or not, and manage different versions of the ones we like.\nOne of fabric's primary features is helping people collect and integrate prompts, which we call Patterns, into various parts of their lives.\nFabric has Patterns for all sorts of life and work activities, including:\n\nExtracting the most interesting parts of YouTube videos and podcasts\nWriting an essay in your own voice with just an idea as an input\nSummarizing opaque academic papers\nCreating perfectly matched AI art prompts for a piece of writing\nRating the quality of content to see if you want to read/watch the whole thing\nGetting summaries of long, boring content\nExplaining code to you\nTurning bad documentation into usable documentation\nCreating social media posts from any content input\nAnd a million more‚Ä¶\n\nInstallation\nTo install Fabric, you can use the latest release binaries or install it from the source.\nGet Latest Release Binaries\nWindows:\nhttps://github.com/danielmiessler/fabric/releases/latest/download/fabric-windows-amd64.exe\nMacOS (arm64):\ncurl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-darwin-arm64 > fabric && chmod +x fabric && ./fabric --version\nMacOS (amd64):\ncurl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-darwin-amd64 > fabric && chmod +x fabric && ./fabric --version\nLinux (amd64):\ncurl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-linux-amd64 > fabric && chmod +x fabric && ./fabric --version\nLinux (arm64):\ncurl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-linux-arm64 > fabric && chmod +x fabric && ./fabric --version\nFrom Source\nTo install Fabric, make sure Go is installed, and then run the following command.\n# Install Fabric directly from the repo\ngo install github.com/danielmiessler/fabric@latest\nEnvironment Variables\nYou may need to set some environment variables in your ~/.bashrc on linux or ~/.zshrc file on mac to be able to run the fabric command. Here is an example of what you can add:\nFor Intel based macs or linux\n# Golang environment variables\nexport GOROOT=/usr/local/go\nexport GOPATH=$HOME/go\n\n# Update PATH to include GOPATH and GOROOT binaries\nexport PATH=$GOPATH/bin:$GOROOT/bin:$HOME/.local/bin:$PATH\nfor Apple Silicon based macs\n# Golang environment variables\nexport GOROOT=$(brew --prefix go)/libexec\nexport GOPATH=$HOME/go\nexport PATH=$GOPATH/bin:$GOROOT/bin:$HOME/.local/bin:$PATH\nSetup\nNow run the following command\n# Run the setup to set up your directories and keys\nfabric --setup\nIf everything works you are good to go.\nAdd aliases for all patterns\nIn order to add aliases for all your patterns and use them directly as commands ie. summarize instead of fabric --pattern summarize\nYou can add the following to your .zshrc or .bashrc file.\n# Loop through all files in the ~/.config/fabric/patterns directory\nfor pattern_file in $HOME/.config/fabric/patterns/*; do\n    # Get the base name of the file (i.e., remove the directory path)\n    pattern_name=$(basename \"$pattern_file\")\n\n    # Create an alias in the form: alias pattern_name=\"fabric --pattern pattern_name\"\n    alias_command=\"alias $pattern_name='fabric --pattern $pattern_name'\"\n\n    # Evaluate the alias command to add it to the current shell\n    eval \"$alias_command\"\ndone\n\nyt() {\n    local video_link=\"$1\"\n    fabric -y \"$video_link\" --transcript\n}\nYou can add the below code for the equivalent aliases inside PowerShell by running notepad $PROFILE inside a PowerShell window:\n# Path to the patterns directory\n$patternsPath = Join-Path $HOME \".config/fabric/patterns\"\nforeach ($patternDir in Get-ChildItem -Path $patternsPath -Directory) {\n    $patternName = $patternDir.Name\n\n    # Dynamically define a function for each pattern\n    $functionDefinition = @\"\nfunction $patternName {\n    [CmdletBinding()]\n    param(\n        [Parameter(ValueFromPipeline = `$true)]\n        [string] `$InputObject,\n\n        [Parameter(ValueFromRemainingArguments = `$true)]\n        [String[]] `$patternArgs\n    )\n\n    begin {\n        # Initialize an array to collect pipeline input\n        `$collector = @()\n    }\n\n    process {\n        # Collect pipeline input objects\n        if (`$InputObject) {\n            `$collector += `$InputObject\n        }\n    }\n\n    end {\n        # Join all pipeline input into a single string, separated by newlines\n        `$pipelineContent = `$collector -join \"`n\"\n\n        # If there's pipeline input, include it in the call to fabric\n        if (`$pipelineContent) {\n            `$pipelineContent | fabric --pattern $patternName `$patternArgs\n        } else {\n            # No pipeline input; just call fabric with the additional args\n            fabric --pattern $patternName `$patternArgs\n        }\n    }\n}\n\"@\n    # Add the function to the current session\n    Invoke-Expression $functionDefinition\n}\n\n# Define the 'yt' function as well\nfunction yt {\n    [CmdletBinding()]\n    param(\n        [Parameter(Mandatory = $true)]\n        [string]$videoLink\n    )\n    fabric -y $videoLink --transcript\n}\nThis also creates a yt alias that allows you to use yt https://www.youtube.com/watch?v=4b0iet22VIk to get transcripts, comments, and metadata.\nSave your files in markdown using aliases\nIf in addition to the above aliases you would like to have the option to save the output to your favourite markdown note vault like Obsidian then instead of the above add the following to your .zshrc or .bashrc file:\n# Define the base directory for Obsidian notes\nobsidian_base=\"/path/to/obsidian\"\n\n# Loop through all files in the ~/.config/fabric/patterns directory\nfor pattern_file in ~/.config/fabric/patterns/*; do\n    # Get the base name of the file (i.e., remove the directory path)\n    pattern_name=$(basename \"$pattern_file\")\n\n    # Unalias any existing alias with the same name\n    unalias \"$pattern_name\" 2>/dev/null\n\n    # Define a function dynamically for each pattern\n    eval \"\n    $pattern_name() {\n        local title=\\$1\n        local date_stamp=\\$(date +'%Y-%m-%d')\n        local output_path=\\\"\\$obsidian_base/\\${date_stamp}-\\${title}.md\\\"\n\n        # Check if a title was provided\n        if [ -n \\\"\\$title\\\" ]; then\n            # If a title is provided, use the output path\n            fabric --pattern \\\"$pattern_name\\\" -o \\\"\\$output_path\\\"\n        else\n            # If no title is provided, use --stream\n            fabric --pattern \\\"$pattern_name\\\" --stream\n        fi\n    }\n    \"\ndone\n\nyt() {\n    local video_link=\"$1\"\n    fabric -y \"$video_link\" --transcript\n}\nThis will allow you to use the patterns as aliases like in the above for example summarize instead of fabric --pattern summarize --stream, however if you pass in an extra argument like this summarize \"my_article_title\" your output will be saved in the destination that you set in obsidian_base=\"/path/to/obsidian\" in the following format YYYY-MM-DD-my_article_title.md where the date gets autogenerated for you.\nYou can tweak the date format by tweaking the date_stamp format.\nMigration\nIf you have the Legacy (Python) version installed and want to migrate to the Go version, here's how you do it. It's basically two steps: 1) uninstall the Python version, and 2) install the Go version.\n# Uninstall Legacy Fabric\npipx uninstall fabric\n\n# Clear any old Fabric aliases\n(check your .bashrc, .zshrc, etc.)\n# Install the Go version\ngo install github.com/danielmiessler/fabric@latest\n# Run setup for the new version. Important because things have changed\nfabric --setup\nThen set your environmental variables as shown above.\nUpgrading\nThe great thing about Go is that it's super easy to upgrade. Just run the same command you used to install it in the first place and you'll always get the latest version.\ngo install github.com/danielmiessler/fabric@latest\nUsage\nOnce you have it all set up, here's how to use it.\nfabric -h\nUsage:\n  fabric [OPTIONS]\n\nApplication Options:\n  -p, --pattern=             Choose a pattern from the available patterns\n  -v, --variable=            Values for pattern variables, e.g. -v=#role:expert -v=#points:30\"\n  -C, --context=             Choose a context from the available contexts\n      --session=             Choose a session from the available sessions\n  -a, --attachment=          Attachment path or URL (e.g. for OpenAI image recognition messages)\n  -S, --setup                Run setup for all reconfigurable parts of fabric\n  -t, --temperature=         Set temperature (default: 0.7)\n  -T, --topp=                Set top P (default: 0.9)\n  -s, --stream               Stream\n  -P, --presencepenalty=     Set presence penalty (default: 0.0)\n  -r, --raw                  Use the defaults of the model without sending chat options (like temperature etc.) and use the user role instead of the system role for patterns.\n  -F, --frequencypenalty=    Set frequency penalty (default: 0.0)\n  -l, --listpatterns         List all patterns\n  -L, --listmodels           List all available models\n  -x, --listcontexts         List all contexts\n  -X, --listsessions         List all sessions\n  -U, --updatepatterns       Update patterns\n  -c, --copy                 Copy to clipboard\n  -m, --model=               Choose model\n  -o, --output=              Output to file\n      --output-session       Output the entire session (also a temporary one) to the output file\n  -n, --latest=              Number of latest patterns to list (default: 0)\n  -d, --changeDefaultModel   Change default model\n  -y, --youtube=             YouTube video \"URL\" to grab transcript, comments from it and send to chat\n      --transcript           Grab transcript from YouTube video and send to chat (it used per default).\n      --comments             Grab comments from YouTube video and send to chat\n      --metadata             Grab metadata from YouTube video and send to chat\n  -g, --language=            Specify the Language Code for the chat, e.g. -g=en -g=zh\n  -u, --scrape_url=          Scrape website URL to markdown using Jina AI\n  -q, --scrape_question=     Search question using Jina AI\n  -e, --seed=                Seed to be used for LMM generation\n  -w, --wipecontext=         Wipe context\n  -W, --wipesession=         Wipe session\n      --printcontext=        Print context\n      --printsession=        Print session\n      --readability          Convert HTML input into a clean, readable view\n      --serve                Initiate the API server\n      --dry-run              Show what would be sent to the model without actually sending it\n      --version              Print current version\n\nHelp Options:\n  -h, --help                 Show this help message\n\nOur approach to prompting\nFabric Patterns are different than most prompts you'll see.\n\nFirst, we use Markdown to help ensure maximum readability and editability. This not only helps the creator make a good one, but also anyone who wants to deeply understand what it does. Importantly, this also includes the AI you're sending it to!\n\nHere's an example of a Fabric Pattern.\nhttps://github.com/danielmiessler/fabric/blob/main/patterns/extract_wisdom/system.md\n\n\n\nNext, we are extremely clear in our instructions, and we use the Markdown structure to emphasize what we want the AI to do, and in what order.\n\n\nAnd finally, we tend to use the System section of the prompt almost exclusively. In over a year of being heads-down with this stuff, we've just seen more efficacy from doing that. If that changes, or we're shown data that says otherwise, we will adjust.\n\n\nExamples\n\nThe following examples use the macOS pbpaste to paste from the clipboard. See the pbpaste section below for Windows and Linux alternatives.\n\nNow let's look at some things you can do with Fabric.\n\nRun the summarize Pattern based on input from stdin. In this case, the body of an article.\n\npbpaste | fabric --pattern summarize\n\nRun the analyze_claims Pattern with the --stream option to get immediate and streaming results.\n\npbpaste | fabric --stream --pattern analyze_claims\n\nRun the extract_wisdom Pattern with the --stream option to get immediate and streaming results from any Youtube video (much like in the original introduction video).\n\nfabric -y \"https://youtube.com/watch?v=uXs-zPc63kM\" --stream --pattern extract_wisdom\n\n\nCreate patterns- you must create a .md file with the pattern and save it to ~/.config/fabric/patterns/[yourpatternname].\n\n\nRun a analyze_claims pattern on a website. Fabric uses Jina AI to scrape the URL into markdown format before sending it to the model.\n\n\nfabric -u https://github.com/danielmiessler/fabric/ -p analyze_claims\nJust use the Patterns\n\n\n\nIf you're not looking to do anything fancy, and you just want a lot of great prompts, you can navigate to the /patterns directory and start exploring!\nWe hope that if you used nothing else from Fabric, the Patterns by themselves will make the project useful.\nYou can use any of the Patterns you see there in any AI application that you have, whether that's ChatGPT or some other app or website. Our plan and prediction is that people will soon be sharing many more than those we've published, and they will be way better than ours.\nThe wisdom of crowds for the win.\nCustom Patterns\nYou may want to use Fabric to create your own custom Patterns‚Äîbut not share them with others. No problem!\nJust make a directory in ~/.config/custompatterns/ (or wherever) and put your .md files in there.\nWhen you're ready to use them, copy them into:\n~/.config/fabric/patterns/\n\nYou can then use them like any other Patterns, but they won't be public unless you explicitly submit them as Pull Requests to the Fabric project. So don't worry‚Äîthey're private to you.\nHelper Apps\nFabric also makes use of some core helper apps (tools) to make it easier to integrate with your various workflows. Here are some examples:\nto_pdf\nto_pdf is a helper command that converts LaTeX files to PDF format. You can use it like this:\nto_pdf input.tex\nThis will create a PDF file from the input LaTeX file in the same directory.\nYou can also use it with stdin which works perfectly with the write_latex pattern:\necho \"ai security primer\" | fabric --pattern write_latex | to_pdf\nThis will create a PDF file named output.pdf in the current directory.\nto_pdf Installation\nTo install to_pdf, install it the same way as you install Fabric, just with a different repo name.\ngo install github.com/danielmiessler/fabric/plugins/tools/to_pdf@latest\nMake sure you have a LaTeX distribution (like TeX Live or MiKTeX) installed on your system, as to_pdf requires pdflatex to be available in your system's PATH.\npbpaste\nThe examples use the macOS program pbpaste to paste content from the clipboard to pipe into fabric as the input. pbpaste is not available on Windows or Linux, but there are alternatives.\nOn Windows, you can use the PowerShell command Get-Clipboard from a PowerShell command prompt. If you like, you can also alias it to pbpaste. If you are using classic PowerShell, edit the file ~\\Documents\\WindowsPowerShell\\.profile.ps1, or if you are using PowerShell Core, edit ~\\Documents\\PowerShell\\.profile.ps1 and add the alias,\nSet-Alias pbpaste Get-Clipboard\nOn Linux, you can use xclip -selection clipboard -o to paste from the clipboard. You will likely need to install xclip with your package manager. For Debian based systems including Ubuntu,\nsudo apt update\nsudo apt install xclip -y\nYou can also create an alias by editing ~/.bashrc or ~/.zshrc and adding the alias,\nalias pbpaste='xclip -selection clipboard -o'\nWeb Interface\nFabric now includes a built-in web interface that provides a GUI alternative to the command-line interface and an out-of-the-box website for those who want to get started with web development or blogging.\nYou can use this app as a GUI interface for Fabric, a ready to go blog-site, or a website template for your own projects.\nThe web/src/lib/content directory includes starter .obsidian/ and templates/ directories, allowing you to open up the web/src/lib/content/ directory as an Obsidian.md vault. You can place your posts in the posts directory when you're ready to publish.\nInstalling\nThe GUI can be installed by navigating to the¬†web¬†directory and using¬†npm install,¬†pnpm install, or your favorite package manager. Then simply run¬†the development server to start the app.\nYou will need to run fabric in a separate terminal with the¬†fabric --serve¬†command.\nFrom the fabric project web/ directory:\nnpm run dev\n\n## or ##\n\npnpm run dev\n\n## or your equivalent\nStreamlit UI\nTo run the Streamlit user interface:\n# Install required dependencies\npip install streamlit pandas matplotlib seaborn numpy python-dotenv\n\n# Run the Streamlit app\nstreamlit run streamlit.py\nThe Streamlit UI provides a user-friendly interface for:\n\nRunning and chaining patterns\nManaging pattern outputs\nCreating and editing patterns\nAnalyzing pattern results\n\nMeta\nNoteSpecial thanks to the following people for their inspiration and contributions!\n\n\nJonathan Dunn for being the absolute MVP dev on the project, including spearheading the new Go version, as well as the GUI! All this while also being a full-time medical doctor!\nCaleb Sima for pushing me over the edge of whether to make this a public project or not.\nEugen Eisler and Frederick Ros for their invaluable contributions to the Go version\nDavid Peters for his work on the web interface.\nJoel Parish for super useful input on the project's Github directory structure..\nJoseph Thacker for the idea of a -c context flag that adds pre-created context in the ./config/fabric/ directory to all Pattern queries.\nJason Haddix for the idea of a stitch (chained Pattern) to filter content using a local model before sending on to a cloud model, i.e., cleaning customer data using llama2 before sending on to gpt-4 for analysis.\nAndre Guerra for assisting with numerous components to make things simpler and more maintainable.\n\nPrimary contributors\n\n\n\n\nfabric was created by Daniel Miessler in January of 2024.",
      "languages": {
        "python": 1,
        "Python": 1,
        "Shell": 1,
        "HTML": 1,
        "go": 1,
        "GO": 1,
        "R": 1,
        "Go": 1
      },
      "topics": [
        "Python",
        "Shell",
        "Web Development",
        "R",
        "Go"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:52.606790"
    },
    {
      "owner": "kubernetes",
      "name": "test-infra",
      "url": "https://github.com/kubernetes/test-infra",
      "description": "Test infrastructure for the Kubernetes project.",
      "readme_content": "test-infra\n\n\nThis repository contains tools and configuration files for the testing and\nautomation needs of the Kubernetes project.\nOur architecture diagram provides an (updated #13063)\noverview of how the different tools and services interact.\nCI Job Management\nKubernetes uses a prow instance at prow.k8s.io to handle CI and\nautomation for the entire project. Everyone can participate in a\nself-service PR-based workflow, where changes are automatically deployed\nafter they have been reviewed. All job configs are located in config/jobs\n\nAdd or update job configs\nDelete job configs\nTest job configs locally\nTrigger jobs on PRs using bot commands\n\nDashboards\nTest Result Dashboards\n\nTestgrid shows historical test results over time (testgrid)\nTriage shows clusters of similar test failures across all jobs (triage)\n\nJob and PR Dashboards\n\nDeck shows what jobs are running or have recently run in prow (prow/cmd/deck)\nGubernator's PR Dashboard shows which PRs need your review (gubernator)\nPR Status shows what needs to be done to get PRs matching a GitHub Query to merge (prow/cmd/tide)\nTide History shows what actions tide has taken over time to trigger tests and merge PRs (prow/cmd/tide)\nTide Status shows what PRs are in tide pools to be tested and merged (prow/cmd/tide)\n\nOther Tools\n\nboskos manages pools of resources; our CI leases GCP projects from these pools\nexperiment is a catchall directory for one-shot tools or scripts\ngcsweb is a UI we use to display test artifacts stored in public GCS buckets\nghproxy is a GitHub-aware reverse proxy cache to help keep our GitHub API token usage within rate limits\ngopherage is a tool for manipulating Go coverage files\ngreenhouse is a shared bazel cache we use to ensure faster build and test presubmit jobs\nlabel_sync creates, updates and migrates GitHub labels across orgs and repos based on labels.yaml file\nkettle extracts test results from GCS and puts them into bigquery\nkubetest is how our CI creates and e2e tests kubernetes clusters\nmaintenance/migratestatus is used to migrate or retire GitHub status contexts on PRs across orgs and repos\nmetrics runs queries against bigquery to generate metrics based on test results\nrobots/commenter is used by some of our jobs to comment on GitHub issues\n\nContributing\nPlease see CONTRIBUTING.MD",
      "languages": {
        "Go": 1,
        "go": 1,
        "GO": 1
      },
      "topics": [
        "Go",
        "Kubernetes"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:53.444681"
    },
    {
      "owner": "trufflesecurity",
      "name": "trufflehog",
      "url": "https://github.com/trufflesecurity/trufflehog",
      "description": "Find, verify, and analyze leaked credentials",
      "readme_content": "TruffleHog\nFind leaked credentials.\n\n\n\n\n\n\n\n\nüîé Now Scanning\n\n\n...and more\nTo learn more about about TruffleHog and its features and capabilities, visit our product page.\n\nüåê TruffleHog Enterprise\nAre you interested in continuously monitoring Git, Jira, Slack, Confluence, Microsoft Teams, Sharepoint, and more.. for credentials? We have an enterprise product that can help! Learn more at https://trufflesecurity.com/trufflehog-enterprise.\nWe take the revenue from the enterprise product to fund more awesome open source projects that the whole community can benefit from.\nWhat is TruffleHog üêΩ\nTruffleHog is the most powerful secrets Discovery, Classification, Validation, and Analysis tool. In this context secret refers to a credential a machine uses to authenticate itself to another machine. This includes API keys, database passwords, private encryption keys, and more...\nDiscovery üîç\nTruffleHog can look for secrets in many places including Git, chats, wikis, logs, API testing platforms, object stores, filesystems and more\nClassification üìÅ\nTruffleHog classifies over 800 secret types, mapping them back to the specific identity they belong to. Is it an AWS secret? Stripe secret? Cloudflare secret? Postgres password? SSL Private key? Sometimes its hard to tell looking at it, so TruffleHog classifies everything it finds.\nValidation ‚úÖ\nFor every secret TruffleHog can classify, it can also log in to confirm if that secret is live or not. This step is critical to know if there‚Äôs an active present danger or not.\nAnalysis üî¨\nFor the 20 some of the most commonly leaked out credential types, instead of sending one request to check if the secret can log in, TruffleHog can send many requests to learn everything there is to know about the secret. Who created it? What resources can it access? What permissions does it have on those resources?\nüì¢ Join Our Community\nHave questions? Feedback? Jump in slack or discord and hang out with us\nJoin our Slack Community\nJoin the Secret Scanning Discord\nüì∫ Demo\n\ndocker run --rm -it -v \"$PWD:/pwd\" trufflesecurity/trufflehog:latest github --org=trufflesecurity\nüíæ Installation\nSeveral options available for you:\nMacOS users\nbrew install trufflehog\nDocker:\nEnsure Docker engine is running before executing the following commands:\n¬†¬†¬†¬†Unix\ndocker run --rm -it -v \"$PWD:/pwd\" trufflesecurity/trufflehog:latest github --repo https://github.com/trufflesecurity/test_keys\n¬†¬†¬†¬†Windows Command Prompt\ndocker run --rm -it -v \"%cd:/=\\%:/pwd\" trufflesecurity/trufflehog:latest github --repo https://github.com/trufflesecurity/test_keys\n¬†¬†¬†¬†Windows PowerShell\ndocker run --rm -it -v \"${PWD}:/pwd\" trufflesecurity/trufflehog github --repo https://github.com/trufflesecurity/test_keys\n¬†¬†¬†¬†M1 and M2 Mac\ndocker run --platform linux/arm64 --rm -it -v \"$PWD:/pwd\" trufflesecurity/trufflehog:latest github --repo https://github.com/trufflesecurity/test_keys\nBinary releases\nDownload and unpack from https://github.com/trufflesecurity/trufflehog/releases\nCompile from source\ngit clone https://github.com/trufflesecurity/trufflehog.git\ncd trufflehog; go install\nUsing installation script\ncurl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -b /usr/local/bin\nUsing installation script, verify checksum signature (requires cosign to be installed)\ncurl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -v -b /usr/local/bin\nUsing installation script to install a specific version\ncurl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -b /usr/local/bin <ReleaseTag like v3.56.0>\nüîê Verifying the artifacts\nChecksums are applied to all artifacts, and the resulting checksum file is signed using cosign.\nYou need the following tool to verify signature:\n\nCosign\n\nVerification steps are as follow:\n\n\nDownload the artifact files you want, and the following files from the releases page.\n\ntrufflehog_{version}_checksums.txt\ntrufflehog_{version}_checksums.txt.pem\ntrufflehog_{version}_checksums.txt.sig\n\n\n\nVerify the signature:\ncosign verify-blob <path to trufflehog_{version}_checksums.txt> \\\n--certificate <path to trufflehog_{version}_checksums.txt.pem> \\\n--signature <path to trufflehog_{version}_checksums.txt.sig> \\\n--certificate-identity-regexp 'https://github\\.com/trufflesecurity/trufflehog/\\.github/workflows/.+' \\\n--certificate-oidc-issuer \"https://token.actions.githubusercontent.com\"\n\n\nOnce the signature is confirmed as valid, you can proceed to validate that the SHA256 sums align with the downloaded artifact:\nsha256sum --ignore-missing -c trufflehog_{version}_checksums.txt\n\n\nReplace {version} with the downloaded files version\nAlternatively, if you are using installation script, pass -v option to perform signature verification.\nThis required Cosign binary to be installed prior to running installation script.\nüöÄ Quick Start\n1: Scan a repo for only verified secrets\nCommand:\ntrufflehog git https://github.com/trufflesecurity/test_keys --results=verified,unknown\nExpected output:\nüê∑üîëüê∑  TruffleHog. Unearth your secrets. üê∑üîëüê∑\n\nFound verified result üê∑üîë\nDetector Type: AWS\nDecoder Type: PLAIN\nRaw result: AKIAYVP4CIPPERUVIFXG\nLine: 4\nCommit: fbc14303ffbf8fb1c2c1914e8dda7d0121633aca\nFile: keys\nEmail: counter <counter@counters-MacBook-Air.local>\nRepository: https://github.com/trufflesecurity/test_keys\nTimestamp: 2022-06-16 10:17:40 -0700 PDT\n...\n\n2: Scan a GitHub Org for only verified secrets\ntrufflehog github --org=trufflesecurity --results=verified,unknown\n3: Scan a GitHub Repo for only verified keys and get JSON output\nCommand:\ntrufflehog git https://github.com/trufflesecurity/test_keys --results=verified,unknown --json\nExpected output:\n{\"SourceMetadata\":{\"Data\":{\"Git\":{\"commit\":\"fbc14303ffbf8fb1c2c1914e8dda7d0121633aca\",\"file\":\"keys\",\"email\":\"counter \\u003ccounter@counters-MacBook-Air.local\\u003e\",\"repository\":\"https://github.com/trufflesecurity/test_keys\",\"timestamp\":\"2022-06-16 10:17:40 -0700 PDT\",\"line\":4}}},\"SourceID\":0,\"SourceType\":16,\"SourceName\":\"trufflehog - git\",\"DetectorType\":2,\"DetectorName\":\"AWS\",\"DecoderName\":\"PLAIN\",\"Verified\":true,\"Raw\":\"AKIAYVP4CIPPERUVIFXG\",\"Redacted\":\"AKIAYVP4CIPPERUVIFXG\",\"ExtraData\":{\"account\":\"595918472158\",\"arn\":\"arn:aws:iam::595918472158:user/canarytokens.com@@mirux23ppyky6hx3l6vclmhnj\",\"user_id\":\"AIDAYVP4CIPPJ5M54LRCY\"},\"StructuredData\":null}\n...\n\n4: Scan a GitHub Repo + its Issues and Pull Requests\ntrufflehog github --repo=https://github.com/trufflesecurity/test_keys --issue-comments --pr-comments\n5: Scan an S3 bucket for verified keys\ntrufflehog s3 --bucket=<bucket name> --results=verified,unknown\n6: Scan S3 buckets using IAM Roles\ntrufflehog s3 --role-arn=<iam role arn>\n7: Scan a Github Repo using SSH authentication in docker\ndocker run --rm -v \"$HOME/.ssh:/root/.ssh:ro\" trufflesecurity/trufflehog:latest git ssh://github.com/trufflesecurity/test_keys\n8: Scan individual files or directories\ntrufflehog filesystem path/to/file1.txt path/to/file2.txt path/to/dir\n9: Scan a local git repo\nClone the git repo. For example test keys repo.\n$ git clone git@github.com:trufflesecurity/test_keys.git\nRun trufflehog from the parent directory (outside the git repo).\n$ trufflehog git file://test_keys --results=verified,unknown\n10: Scan GCS buckets for verified secrets\ntrufflehog gcs --project-id=<project-ID> --cloud-environment --results=verified,unknown\n11: Scan a Docker image for verified secrets\nUse the --image flag multiple times to scan multiple images.\ntrufflehog docker --image trufflesecurity/secrets --results=verified,unknown\n12: Scan in CI\nSet the --since-commit flag to your default branch that people merge into (ex: \"main\"). Set the --branch flag to your PR's branch name (ex: \"feature-1\"). Depending on the CI/CD platform you use, this value can be pulled in dynamically (ex: CIRCLE_BRANCH in Circle CI and TRAVIS_PULL_REQUEST_BRANCH in Travis CI). If the repo is cloned and the target branch is already checked out during the CI/CD workflow, then --branch HEAD should be sufficient. The --fail flag will return an 183 error code if valid credentials are found.\ntrufflehog git file://. --since-commit main --branch feature-1 --results=verified,unknown --fail\n13: Scan a Postman workspace\nUse the --workspace-id, --collection-id, --environment flags multiple times to scan multiple targets.\ntrufflehog postman --token=<postman api token> --workspace-id=<workspace id>\n14: Scan a Jenkins server\ntrufflehog jenkins --url https://jenkins.example.com --username admin --password admin\n15: Scan an Elasticsearch server\nScan a Local Cluster\nThere are two ways to authenticate to a local cluster with TruffleHog: (1) username and password, (2) service token.\nConnect to a local cluster with username and password\ntrufflehog elasticsearch --nodes 192.168.14.3 192.168.14.4 --username truffle --password hog\nConnect to a local cluster with a service token\ntrufflehog elasticsearch --nodes 192.168.14.3 192.168.14.4 --service-token ‚ÄòAAEWVaWM...Rva2VuaSDZ‚Äô\nScan an Elastic Cloud Cluster\nTo scan a cluster on Elastic Cloud, you‚Äôll need a Cloud ID and API key.\ntrufflehog elasticsearch \\\n  --cloud-id 'search-prod:dXMtY2Vx...YjM1ODNlOWFiZGRlNjI0NA==' \\\n  --api-key 'MlVtVjBZ...ZSYlduYnF1djh3NG5FQQ=='\n16. Scan a GitHub Repository for Cross Fork Object References and Deleted Commits\nThe following command will enumerate deleted and hidden commits on a GitHub repository and then scan them for secrets. This is an alpha release feature.\ntrufflehog github-experimental --repo https://github.com/<USER>/<REPO>.git --object-discovery\nIn addition to the normal TruffleHog output, the --object-discovery flag creates two files in a new $HOME/.trufflehog directory: valid_hidden.txt and invalid.txt. These are used to track state during commit enumeration, as well as to provide users with a complete list of all hidden and deleted commits (valid_hidden.txt). If you'd like to automatically remove these files after scanning, please add the flag --delete-cached-data.\nNote: Enumerating all valid commits on a repository using this method takes between 20 minutes and a few hours, depending on the size of your repository. We added a progress bar to keep you updated on how long the enumeration will take. The actual secret scanning runs extremely fast.\nFor more information on Cross Fork Object References, please read our blog post.\n17. Scan Hugging Face\nScan a Hugging Face Model, Dataset or Space\ntrufflehog huggingface --model <model_id> --space <space_id> --dataset <dataset_id>\nScan all Models, Datasets and Spaces belonging to a Hugging Face Organization or User\ntrufflehog huggingface --org <orgname> --user <username>\n(Optionally) When scanning an organization or user, you can skip an entire class of resources with --skip-models, --skip-datasets, --skip-spaces OR a particular resource with --ignore-models <model_id>, --ignore-datasets <dataset_id>, --ignore-spaces <space_id>.\nScan Discussion and PR Comments\ntrufflehog huggingface --model <model_id> --include-discussions --include-prs\n‚ùì FAQ\n\nAll I see is üê∑üîëüê∑  TruffleHog. Unearth your secrets. üê∑üîëüê∑ and the program exits, what gives?\n\nThat means no secrets were detected\n\n\nWhy is the scan taking a long time when I scan a GitHub org\n\nUnauthenticated GitHub scans have rate limits. To improve your rate limits, include the --token flag with a personal access token\n\n\nIt says a private key was verified, what does that mean?\n\nCheck out our Driftwood blog post to learn how to do this, in short we've confirmed the key can be used live for SSH or SSL Blog post\n\n\nIs there an easy way to ignore specific secrets?\n\nIf the scanned source supports line numbers, then you can add a trufflehog:ignore comment on the line containing the secret to ignore that secrets.\n\n\n\nüì∞ What's new in v3?\nTruffleHog v3 is a complete rewrite in Go with many new powerful features.\n\nWe've added over 700 credential detectors that support active verification against their respective APIs.\nWe've also added native support for scanning GitHub, GitLab, Docker, filesystems, S3, GCS, Circle CI and Travis CI.\nInstantly verify private keys against millions of github users and billions of TLS certificates using our Driftwood technology.\nScan binaries, documents, and other file formats\nAvailable as a GitHub Action and a pre-commit hook\n\nWhat is credential verification?\nFor every potential credential that is detected, we've painstakingly implemented programmatic verification against the API that we think it belongs to. Verification eliminates false positives. For example, the AWS credential detector performs a GetCallerIdentity API call against the AWS API to verify if an AWS credential is active.\nüìù Usage\nTruffleHog has a sub-command for each source of data that you may want to scan:\n\ngit\ngithub\ngitlab\ndocker\ns3\nfilesystem (files and directories)\nsyslog\ncircleci\ntravisci\ngcs (Google Cloud Storage)\npostman\njenkins\nelasticsearch\n\nEach subcommand can have options that you can see with the --help flag provided to the sub command:\n$ trufflehog git --help\nusage: TruffleHog git [<flags>] <uri>\n\nFind credentials in git repositories.\n\nFlags:\n  -h, --help                Show context-sensitive help (also try --help-long and --help-man).\n      --log-level=0         Logging verbosity on a scale of 0 (info) to 5 (trace). Can be disabled with \"-1\".\n      --profile             Enables profiling and sets a pprof and fgprof server on :18066.\n  -j, --json                Output in JSON format.\n      --json-legacy         Use the pre-v3.0 JSON format. Only works with git, gitlab, and github sources.\n      --github-actions      Output in GitHub Actions format.\n      --concurrency=20           Number of concurrent workers.\n      --no-verification     Don't verify the results.\n      --results=RESULTS          Specifies which type(s) of results to output: verified, unknown, unverified, filtered_unverified. Defaults to all types.\n      --allow-verification-overlap\n                                 Allow verification of similar credentials across detectors\n      --filter-unverified   Only output first unverified result per chunk per detector if there are more than one results.\n      --filter-entropy=FILTER-ENTROPY\n                                 Filter unverified results with Shannon entropy. Start with 3.0.\n      --config=CONFIG            Path to configuration file.\n      --print-avg-detector-time\n                                 Print the average time spent on each detector.\n      --no-update           Don't check for updates.\n      --fail                Exit with code 183 if results are found.\n      --verifier=VERIFIER ...    Set custom verification endpoints.\n      --custom-verifiers-only   Only use custom verification endpoints.\n      --archive-max-size=ARCHIVE-MAX-SIZE\n                                 Maximum size of archive to scan. (Byte units eg. 512B, 2KB, 4MB)\n      --archive-max-depth=ARCHIVE-MAX-DEPTH\n                                 Maximum depth of archive to scan.\n      --archive-timeout=ARCHIVE-TIMEOUT\n                                 Maximum time to spend extracting an archive.\n      --include-detectors=\"all\"  Comma separated list of detector types to include. Protobuf name or IDs may be used, as well as ranges.\n      --exclude-detectors=EXCLUDE-DETECTORS\n                                 Comma separated list of detector types to exclude. Protobuf name or IDs may be used, as well as ranges. IDs defined here take precedence over the include list.\n      --version             Show application version.\n  -i, --include-paths=INCLUDE-PATHS\n                                 Path to file with newline separated regexes for files to include in scan.\n  -x, --exclude-paths=EXCLUDE-PATHS\n                                 Path to file with newline separated regexes for files to exclude in scan.\n      --exclude-globs=EXCLUDE-GLOBS\n                                 Comma separated list of globs to exclude in scan. This option filters at the `git log` level, resulting in faster scans.\n      --since-commit=SINCE-COMMIT\n                                 Commit to start scan from.\n      --branch=BRANCH            Branch to scan.\n      --max-depth=MAX-DEPTH      Maximum depth of commits to scan.\n      --bare                Scan bare repository (e.g. useful while using in pre-receive hooks)\n\nArgs:\n  <uri>  Git repository URL. https://, file://, or ssh:// schema expected.\n\nFor example, to scan a git repository, start with\ntrufflehog git https://github.com/trufflesecurity/trufflehog.git\n\nS3\nThe S3 source supports assuming IAM roles for scanning in addition to IAM users. This makes it easier for users to scan multiple AWS accounts without needing to rely on hardcoded credentials for each account.\nThe IAM identity that TruffleHog uses initially will need to have AssumeRole privileges as a principal in the trust policy of each IAM role to assume.\nTo scan a specific bucket using locally set credentials or instance metadata if on an EC2 instance:\ntrufflehog s3 --bucket=<bucket-name>\nTo scan a specific bucket using an assumed role:\ntrufflehog s3 --bucket=<bucket-name> --role-arn=<iam-role-arn>\nMultiple roles can be passed as separate arguments. The following command will attempt to scan every bucket each role has permissions to list in the S3 API:\ntrufflehog s3 --role-arn=<iam-role-arn-1> --role-arn=<iam-role-arn-2>\nExit Codes:\n\n0: No errors and no results were found.\n1: An error was encountered. Sources may not have completed scans.\n183: No errors were encountered, but results were found. Will only be returned if --fail flag is used.\n\n TruffleHog Github Action\nGeneral Usage\non:\n  push:\n    branches:\n      - main\n  pull_request:\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v4\n      with:\n        fetch-depth: 0\n    - name: Secret Scanning\n      uses: trufflesecurity/trufflehog@main\n      with:\n        extra_args: --results=verified,unknown\n\nIn the example config above, we're scanning for live secrets in all PRs and Pushes to main. Only code changes in the referenced commits are scanned. If you'd like to scan an entire branch, please see the \"Advanced Usage\" section below.\nShallow Cloning\nIf you're incorporating TruffleHog into a standalone workflow and aren't running any other CI/CD tooling alongside TruffleHog, then we recommend using Shallow Cloning to speed up your workflow. Here's an example for how to do it:\n...\n      - shell: bash\n        run: |\n          if [ \"${{ github.event_name }}\" == \"push\" ]; then\n            echo \"depth=$(($(jq length <<< '${{ toJson(github.event.commits) }}') + 2))\" >> $GITHUB_ENV\n            echo \"branch=${{ github.ref_name }}\" >> $GITHUB_ENV\n          fi\n          if [ \"${{ github.event_name }}\" == \"pull_request\" ]; then\n            echo \"depth=$((${{ github.event.pull_request.commits }}+2))\" >> $GITHUB_ENV\n            echo \"branch=${{ github.event.pull_request.head.ref }}\" >> $GITHUB_ENV\n          fi\n      - uses: actions/checkout@v3\n        with:\n          ref: ${{env.branch}}\n          fetch-depth: ${{env.depth}}\n      - uses: trufflesecurity/trufflehog@main\n        with:\n          extra_args: --results=verified,unknown\n...\n\nDepending on the event type (push or PR), we calculate the number of commits present. Then we add 2, so that we can reference a base commit before our code changes. We pass that integer value to the fetch-depth flag in the checkout action in addition to the relevant branch. Now our checkout process should be much shorter.\nCanary detection\nTruffleHog statically detects https://canarytokens.org/ and lets you know when they're present without setting them off. You can learn more here: https://trufflesecurity.com/canaries\n\nAdvanced Usage\n- name: TruffleHog\n  uses: trufflesecurity/trufflehog@main\n  with:\n    # Repository path\n    path:\n    # Start scanning from here (usually main branch).\n    base:\n    # Scan commits until here (usually dev branch).\n    head: # optional\n    # Extra args to be passed to the trufflehog cli.\n    extra_args: --log-level=2 --results=verified,unknown\nIf you'd like to specify specific base and head refs, you can use the base argument (--since-commit flag in TruffleHog CLI) and the head argument (--branch flag in the TruffleHog CLI). We only recommend using these arguments for very specific use cases, where the default behavior does not work.\nAdvanced Usage: Scan entire branch\n- name: scan-push\n        uses: trufflesecurity/trufflehog@main\n        with:\n          base: \"\"\n          head: ${{ github.ref_name }}\n          extra_args: --results=verified,unknown\n\nTruffleHog GitLab CI\nExample Usage\nstages:\n  - security\n\nsecurity-secrets:\n  stage: security\n  allow_failure: false\n  image: alpine:latest\n  variables:\n    SCAN_PATH: \".\" # Set the relative path in the repo to scan\n  before_script:\n    - apk add --no-cache git curl jq\n    - curl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -b /usr/local/bin\n  script:\n    - trufflehog filesystem \"$SCAN_PATH\" --results=verified,unknown --fail --json | jq\n  rules:\n    - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'\nIn the example pipeline above, we're scanning for live secrets in all repository directories and files. This job runs only when the pipeline source is a merge request event, meaning it's triggered when a new merge request is created.\nPre-commit Hook\nTruffleHog can be used in a pre-commit hook to prevent credentials from leaking before they ever leave your computer.\nKey Usage Note:\n\nFor optimal hook efficacy, execute git add followed by git commit separately. This ensures TruffleHog analyzes all intended changes.\nAvoid using git commit -am, as it might bypass pre-commit hook execution for unstaged modifications.\n\nAn example .pre-commit-config.yaml is provided (see pre-commit.com for installation).\nrepos:\n  - repo: local\n    hooks:\n      - id: trufflehog\n        name: TruffleHog\n        description: Detect secrets in your data.\n        entry: bash -c 'trufflehog git file://. --since-commit HEAD --results=verified,unknown --fail'\n        # For running trufflehog in docker, use the following entry instead:\n        # entry: bash -c 'docker run --rm -v \"$(pwd):/workdir\" -i --rm trufflesecurity/trufflehog:latest git file:///workdir --since-commit HEAD --results=verified,unknown --fail'\n        language: system\n        stages: [\"commit\", \"push\"]\nRegex Detector (alpha)\nTruffleHog supports detection and verification of custom regular expressions.\nFor detection, at least one regular expression and keyword is required.\nA keyword is a fixed literal string identifier that appears in or around\nthe regex to be detected. To allow maximum flexibility for verification, a\nwebhook is used containing the regular expression matches.\nTruffleHog will send a JSON POST request containing the regex matches to a\nconfigured webhook endpoint. If the endpoint responds with a 200 OK response\nstatus code, the secret is considered verified.\nCustom Detectors support a few different filtering mechanisms: entropy, regex targeting the entire match, regex targeting the captured secret,\nand excluded word lists checked against the secret (captured group if present, entire match if capture group is not present). Note that if\nyour custom detector has multiple regex set (in this example hogID, and hogToken), then the filters get applied to each regex. Here is an example of a custom detector using these filters.\nNB: This feature is alpha and subject to change.\nRegex Detector Example\n# config.yaml\ndetectors:\n  - name: HogTokenDetector\n    keywords:\n      - hog\n    regex:\n      hogID: '\\b(HOG[0-9A-Z]{17})\\b'\n      hogToken: '[^A-Za-z0-9+\\/]{0,1}([A-Za-z0-9+\\/]{40})[^A-Za-z0-9+\\/]{0,1}'\n    verify:\n      - endpoint: http://localhost:8000/\n        # unsafe must be set if the endpoint is HTTP\n        unsafe: true\n        headers:\n          - \"Authorization: super secret authorization header\"\n$ trufflehog filesystem /tmp --config config.yaml --results=verified,unknown\nüê∑üîëüê∑  TruffleHog. Unearth your secrets. üê∑üîëüê∑\n\nFound verified result üê∑üîë\nDetector Type: CustomRegex\nDecoder Type: PLAIN\nRaw result: HOGAAIUNNWHAHJJWUQYR\nFile: /tmp/hog-facts.txt\n\nData structure sent to the custom verification server:\n{\n    \"HogTokenDetector\": {\n        \"HogID\": [\"HOGAAIUNNWHAHJJWUQYR\"],\n        \"HogSecret\": [\"sD9vzqdSsAOxntjAJ/qZ9sw+8PvEYg0r7D1Hhh0C\"],\n    }\n}\n\nVerification Server Example (Python)\nUnless you run a verification server, secrets found by the custom regex\ndetector will be unverified. Here is an example Python implementation of a\nverification server for the above config.yaml file.\nimport json\nfrom http.server import BaseHTTPRequestHandler, HTTPServer\n\nAUTH_HEADER = 'super secret authorization header'\n\n\nclass Verifier(BaseHTTPRequestHandler):\n    def do_GET(self):\n        self.send_response(405)\n        self.end_headers()\n\n    def do_POST(self):\n        try:\n            if self.headers['Authorization'] != AUTH_HEADER:\n                self.send_response(401)\n                self.end_headers()\n                return\n\n            # read the body\n            length = int(self.headers['Content-Length'])\n            request = json.loads(self.rfile.read(length))\n            self.log_message(\"%s\", request)\n\n            # check the match, you'll need to implement validateToken, which takes an array of ID's and Secrets\n            if not validateTokens(request['HogTokenDetector']['hogID'], request['HogTokenDetector']['hogSecret']):\n                self.send_response(200)\n                self.end_headers()\n            else:\n                # any other response besides 200\n                self.send_response(406)\n                self.end_headers()\n        except Exception:\n            self.send_response(400)\n            self.end_headers()\n\n\nwith HTTPServer(('', 8000), Verifier) as server:\n    try:\n        server.serve_forever()\n    except KeyboardInterrupt:\n        pass\nüîç Analyze\nTruffleHog supports running a deeper analysis of a credential to view its permissions and the resources it has access to.\ntrufflehog analyze\n‚ù§Ô∏è Contributors\nThis project exists thanks to all the people who contribute. [Contribute].\n\n\n\nüíª Contributing\nContributions are very welcome! Please see our contribution guidelines first.\nWe no longer accept contributions to TruffleHog v2, but that code is available in the v2 branch.\nAdding new secret detectors\nWe have published some documentation and tooling to get started on adding new secret detectors. Let's improve detection together!\nUse as a library\nCurrently, trufflehog is in heavy development and no guarantees can be made on\nthe stability of the public APIs at this time.\nLicense Change\nSince v3.0, TruffleHog is released under a AGPL 3 license, included in LICENSE. TruffleHog v3.0 uses none of the previous codebase, but care was taken to preserve backwards compatibility on the command line interface. The work previous to this release is still available licensed under GPL 2.0 in the history of this repository and the previous package releases and tags. A completed CLA is required for us to accept contributions going forward.",
      "languages": {
        "python": 1,
        "Python": 1,
        "go": 1,
        "GO": 1,
        "Shell": 1,
        "Go": 1
      },
      "topics": [
        "Python",
        "AWS",
        "Docker",
        "Shell",
        "Go"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:54.549258"
    },
    {
      "owner": "gruntwork-io",
      "name": "terragrunt",
      "url": "https://github.com/gruntwork-io/terragrunt",
      "description": "Terragrunt is a flexible orchestration tool that allows Infrastructure as Code written in OpenTofu/Terraform to scale.",
      "readme_content": "Terragrunt\n\n\n\n\n\nTerragrunt is a flexible orchestration tool that allows Infrastructure as Code written in OpenTofu/Terraform to scale.\nPlease see the following for more info, including install instructions and complete documentation:\n\nTerragrunt Website\nGetting started with Terragrunt\nTerragrunt Documentation\nContributing to Terragrunt\nCommercial Support\n\nJoin the Discord!\nJoin our community for discussions, support, and contributions:\n\nLicense\nThis code is released under the MIT License. See LICENSE.txt.",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:55.285918"
    },
    {
      "owner": "jaseemuddinn",
      "name": "onnoff_revamped",
      "url": "https://github.com/jaseemuddinn/onnoff_revamped",
      "description": "",
      "readme_content": "This is a [Next.js](https://nextjs.org) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).\n\n## Getting Started\n\nFirst, run the development server:\n\n```bash\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\n```\n\nOpen [http://localhost:3000](http://localhost:3000) with your browser to see the result.\n\nYou can start editing the page by modifying `app/page.js`. The page auto-updates as you edit the file.\n\nThis project uses [`next/font`](https://nextjs.org/docs/app/building-your-application/optimizing/fonts) to automatically optimize and load [Geist](https://vercel.com/font), a new font family for Vercel.\n\n## Learn More\n\nTo learn more about Next.js, take a look at the following resources:\n\n- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.\n- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.\n\nYou can check out [the Next.js GitHub repository](https://github.com/vercel/next.js) - your feedback and contributions are welcome!\n\n## Deploy on Vercel\n\nThe easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.\n\nCheck out our [Next.js deployment documentation](https://nextjs.org/docs/app/building-your-application/deploying) for more details.\n# onnoff_revamped",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T13:02:52.095105"
    },
    {
      "owner": "jaseemuddinn",
      "name": "TheAce",
      "url": "https://github.com/jaseemuddinn/TheAce",
      "description": "",
      "readme_content": "React + Vite\nThis template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.\nCurrently, two official plugins are available:\n\n@vitejs/plugin-react uses Babel for Fast Refresh\n@vitejs/plugin-react-swc uses SWC for Fast Refresh",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T13:02:52.871128"
    },
    {
      "owner": "jaseemuddinn",
      "name": "calenderApp",
      "url": "https://github.com/jaseemuddinn/calenderApp",
      "description": "",
      "readme_content": "This is a [Next.js](https://nextjs.org) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).\n\n## Getting Started\n\nFirst, run the development server:\n\n```bash\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\n```\n\nOpen [http://localhost:3000](http://localhost:3000) with your browser to see the result.\n\nYou can start editing the page by modifying `app/page.js`. The page auto-updates as you edit the file.\n\nThis project uses [`next/font`](https://nextjs.org/docs/app/building-your-application/optimizing/fonts) to automatically optimize and load [Geist](https://vercel.com/font), a new font family for Vercel.\n\n## Learn More\n\nTo learn more about Next.js, take a look at the following resources:\n\n- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.\n- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.\n\nYou can check out [the Next.js GitHub repository](https://github.com/vercel/next.js) - your feedback and contributions are welcome!\n\n## Deploy on Vercel\n\nThe easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.\n\nCheck out our [Next.js deployment documentation](https://nextjs.org/docs/app/building-your-application/deploying) for more details.\n# calenderApp",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T13:02:53.547052"
    },
    {
      "owner": "jaseemuddinn",
      "name": "onnoff",
      "url": "https://github.com/jaseemuddinn/onnoff",
      "description": "",
      "readme_content": "# React + Vite\n\nThis template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.\n\nCurrently, two official plugins are available:\n\n- [@vitejs/plugin-react](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react/README.md) uses [Babel](https://babeljs.io/) for Fast Refresh\n- [@vitejs/plugin-react-swc](https://github.com/vitejs/vite-plugin-react-swc) uses [SWC](https://swc.rs/) for Fast Refresh\n# onnoff",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T13:02:54.265153"
    },
    {
      "owner": "jaseemuddinn",
      "name": "face_recog",
      "url": "https://github.com/jaseemuddinn/face_recog",
      "description": "",
      "readme_content": "This is a [Next.js](https://nextjs.org/) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).\n\n## Getting Started\n\nFirst, run the development server:\n\n```bash\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\n```\n\nOpen [http://localhost:3000](http://localhost:3000) with your browser to see the result.\n\nYou can start editing the page by modifying `app/page.js`. The page auto-updates as you edit the file.\n\nThis project uses [`next/font`](https://nextjs.org/docs/basic-features/font-optimization) to automatically optimize and load Inter, a custom Google Font.\n\n## Learn More\n\nTo learn more about Next.js, take a look at the following resources:\n\n- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.\n- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.\n\nYou can check out [the Next.js GitHub repository](https://github.com/vercel/next.js/) - your feedback and contributions are welcome!\n\n## Deploy on Vercel\n\nThe easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.\n\nCheck out our [Next.js deployment documentation](https://nextjs.org/docs/deployment) for more details.\n# face_recog",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T13:02:54.942298"
    },
    {
      "owner": "jaseemuddinn",
      "name": "theace_v2",
      "url": "https://github.com/jaseemuddinn/theace_v2",
      "description": "Under Development",
      "readme_content": "This is a Next.js project bootstrapped with create-next-app.\nGetting Started\nFirst, run the development server:\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\nOpen http://localhost:3000 with your browser to see the result.\nYou can start editing the page by modifying app/page.js. The page auto-updates as you edit the file.\nThis project uses next/font to automatically optimize and load Inter, a custom Google Font.\nLearn More\nTo learn more about Next.js, take a look at the following resources:\n\nNext.js Documentation - learn about Next.js features and API.\nLearn Next.js - an interactive Next.js tutorial.\n\nYou can check out the Next.js GitHub repository - your feedback and contributions are welcome!\nDeploy on Vercel\nThe easiest way to deploy your Next.js app is to use the Vercel Platform from the creators of Next.js.\nCheck out our Next.js deployment documentation for more details.",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T13:02:55.611905"
    },
    {
      "owner": "jaseemuddinn",
      "name": "Edulnnova",
      "url": "https://github.com/jaseemuddinn/Edulnnova",
      "description": "",
      "readme_content": "React + Vite\nThis template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.\nCurrently, two official plugins are available:\n\n@vitejs/plugin-react uses Babel for Fast Refresh\n@vitejs/plugin-react-swc uses SWC for Fast Refresh",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T13:02:56.309913"
    },
    {
      "owner": "jaseemuddinn",
      "name": "VeraLink",
      "url": "https://github.com/jaseemuddinn/VeraLink",
      "description": "",
      "readme_content": "",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T13:02:57.066188"
    },
    {
      "owner": "jaseemuddinn",
      "name": "pdsalon",
      "url": "https://github.com/jaseemuddinn/pdsalon",
      "description": "",
      "readme_content": "React + Vite\nThis template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.\nCurrently, two official plugins are available:\n\n@vitejs/plugin-react uses Babel for Fast Refresh\n@vitejs/plugin-react-swc uses SWC for Fast Refresh",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T13:02:57.749018"
    },
    {
      "owner": "jaseemuddinn",
      "name": "irada_revamped",
      "url": "https://github.com/jaseemuddinn/irada_revamped",
      "description": "",
      "readme_content": "React + Vite\nThis template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.\nCurrently, two official plugins are available:\n\n@vitejs/plugin-react uses Babel for Fast Refresh\n@vitejs/plugin-react-swc uses SWC for Fast Refresh",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T13:02:58.523216"
    },
    {
      "owner": "404avinotfound",
      "name": "Coding-and-Decoding",
      "url": "https://github.com/404avinotfound/Coding-and-Decoding",
      "description": "Project on Coding Decoding",
      "readme_content": "Coding-and-Decoding\nProject on Coding Decoding",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T13:05:15.130099"
    }
  ],
  "user_profiles": {
    "chanakya2006": {
      "username": "chanakya2006",
      "bio": "",
      "readme_content": "",
      "repositories": [
        "github-repo-recommendation-on-basis-of-profile",
        "fitness_api",
        "pdf_chatbot",
        "python"
      ],
      "top_languages": {
        "Go": 1,
        "go": 1,
        "GO": 1,
        "python": 1,
        "Python": 1
      },
      "top_topics": {
        "Go": 1,
        "Python": 1
      },
      "last_updated": "2025-02-22T12:46:46.092174"
    },
    "jaseemuddinn": {
      "username": "jaseemuddinn",
      "bio": "",
      "readme_content": "Hi there, I am Jaseemuddin Naseem üëã\n\n\n\n  \n\n\nüéØ Portfolio website: Portfolio\n‚ö° Fun fact: The first rule of programming- if it works, don‚Äôt touch it.ü§ì\n\nüíª Thing(s) I love\n\nWeb Development \n(Figuring out) ü§ì\n  \n\n\nüõ†Tech Stack\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚≠ê : Show some ¬†‚ù§Ô∏è¬† to my repositories!",
      "repositories": [
        "onnoff_revamped",
        "TheAce",
        "calenderApp",
        "onnoff",
        "face_recog",
        "theace_v2",
        "Edulnnova",
        "VeraLink",
        "pdsalon",
        "irada_revamped"
      ],
      "top_languages": {},
      "top_topics": {
        "Web Development": 1
      },
      "last_updated": "2025-02-22T13:02:58.527129"
    },
    "404avinotfound": {
      "username": "404avinotfound",
      "bio": "//Greatings\n#include <stdio.h>\n\nvoid main() {\n\n     printf(\"Hi\");   \n     \n     return 0;\n}",
      "readme_content": "",
      "repositories": [
        "Coding-and-Decoding"
      ],
      "top_languages": {},
      "top_topics": {},
      "last_updated": "2025-02-22T13:05:15.135008"
    }
  }
}