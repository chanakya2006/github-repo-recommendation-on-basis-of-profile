{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78 repositories and 6 user profiles from database\n",
      "GitHub Project Recommender\n",
      "==========================\n",
      "Database contains 78 repositories and 6 user profiles\n",
      "\n",
      "Options:\n",
      "1. Get recommendations for a GitHub user\n",
      "2. Search and recommend from specific topics/languages\n",
      "3. Clean old database entries\n",
      "4. Show database statistics\n",
      "5. Exit\n",
      "\n",
      "Invalid choice. Please enter a number between 1 and 5.\n",
      "\n",
      "Options:\n",
      "1. Get recommendations for a GitHub user\n",
      "2. Search and recommend from specific topics/languages\n",
      "3. Clean old database entries\n",
      "4. Show database statistics\n",
      "5. Exit\n",
      "\n",
      "Getting recommendations...\n",
      "Getting 5 recommendations from database...\n",
      "Getting 5 recommendations from GitHub search...\n",
      "Using cached data for repository spf13/cobra\n",
      "Using cached data for repository subtrace/subtrace\n",
      "Using cached data for repository chaitin/SafeLine\n",
      "Using cached data for repository keploy/keploy\n",
      "Using cached data for repository k8sgpt-ai/k8sgpt\n",
      "Using cached data for repository gitleaks/gitleaks\n",
      "Using cached data for repository danielmiessler/fabric\n",
      "Using cached data for repository kubernetes/test-infra\n",
      "Using cached data for repository trufflesecurity/trufflehog\n",
      "Using cached data for repository gruntwork-io/terragrunt\n",
      "\n",
      "Top 10 recommended repositories for chanakya2006:\n",
      "\n",
      "1. Asabeneh/30-Days-Of-Python\n",
      "   Description: 30 days of Python programming challenge is a step-by-step guide to learn the Python programming language in 30 days. This challenge may take more than100 days, follow your own pace. These videos may help too: https://www.youtube.com/channel/UC7PNRuno1rzYPb1xLa4yktw\n",
      "   Languages: Python, JavaScript, javascript, Shell, Go, python, GO, go\n",
      "   Topics: Data Science, Machine Learning, Python, JavaScript, Shell, Go, MongoDB\n",
      "   Similarity Score: 0.55\n",
      "   URL: https://github.com/Asabeneh/30-Days-Of-Python\n",
      "\n",
      "2. Go/bridgecrew-test\n",
      "   Description: \n",
      "   Languages: \n",
      "   Topics: \n",
      "   Similarity Score: 0.55\n",
      "   URL: https://github.com/Go/bridgecrew-test\n",
      "\n",
      "3. Go/dotfiles\n",
      "   Description: my dot files\n",
      "   Languages: \n",
      "   Topics: \n",
      "   Similarity Score: 0.53\n",
      "   URL: https://github.com/Go/dotfiles\n",
      "\n",
      "4. mexanik619/github-repo-recommendation-on-basis-of-profile\n",
      "   Description: \n",
      "   Languages: \n",
      "   Topics: \n",
      "   Similarity Score: 0.51\n",
      "   URL: https://github.com/mexanik619/github-repo-recommendation-on-basis-of-profile\n",
      "\n",
      "5. go-gorm/gorm\n",
      "   Description: The fantastic ORM library for Golang, aims to be developer friendly\n",
      "   Languages: HTML, SQL\n",
      "   Topics: API, Database\n",
      "   Similarity Score: 0.51\n",
      "   URL: https://github.com/go-gorm/gorm\n",
      "\n",
      "6. spf13/cobra\n",
      "   Description: A Commander for modern Go CLI interactions\n",
      "   Languages: Go, Shell\n",
      "   Topics: \n",
      "   Similarity Score: 0.51\n",
      "   URL: https://github.com/spf13/cobra\n",
      "\n",
      "7. gruntwork-io/terragrunt\n",
      "   Description: Terragrunt is a flexible orchestration tool that allows Infrastructure as Code written in OpenTofu/Terraform to scale.\n",
      "   Languages: \n",
      "   Topics: \n",
      "   Similarity Score: 0.42\n",
      "   URL: https://github.com/gruntwork-io/terragrunt\n",
      "\n",
      "8. kubernetes/test-infra\n",
      "   Description: Test infrastructure for the Kubernetes project.\n",
      "   Languages: Go, go, GO\n",
      "   Topics: Go, Kubernetes\n",
      "   Similarity Score: 0.40\n",
      "   URL: https://github.com/kubernetes/test-infra\n",
      "\n",
      "9. subtrace/subtrace\n",
      "   Description: Wireshark for Docker containers\n",
      "   Languages: Python, Go, python, GO, go\n",
      "   Topics: Go, Docker, Python\n",
      "   Similarity Score: 0.38\n",
      "   URL: https://github.com/subtrace/subtrace\n",
      "\n",
      "10. keploy/keploy\n",
      "   Description: Unit and Integration Test generation for Developers. Generate tests and stubs for your application that actually work!\n",
      "   Languages: Java, Python, java, Go, python, GO, go\n",
      "   Topics: Java, Python, Redis, Kubernetes, Go\n",
      "   Similarity Score: 0.37\n",
      "   URL: https://github.com/keploy/keploy\n",
      "\n",
      "Options:\n",
      "1. Get recommendations for a GitHub user\n",
      "2. Search and recommend from specific topics/languages\n",
      "3. Clean old database entries\n",
      "4. Show database statistics\n",
      "5. Exit\n",
      "\n",
      "Invalid choice. Please enter a number between 1 and 5.\n",
      "\n",
      "Options:\n",
      "1. Get recommendations for a GitHub user\n",
      "2. Search and recommend from specific topics/languages\n",
      "3. Clean old database entries\n",
      "4. Show database statistics\n",
      "5. Exit\n",
      "\n",
      "Exiting GitHub Project Recommender. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class GitHubProjectRecommender:\n",
    "    def __init__(self, db_file=\"repository_database.json\"):\n",
    "        self.model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        # Store scraped repositories in memory\n",
    "        self.stored_repositories = []\n",
    "        self.user_profiles = {}\n",
    "        \n",
    "        # Database file path\n",
    "        self.db_file = db_file\n",
    "        \n",
    "        # Load existing database if it exists\n",
    "        self.load_database()\n",
    "        \n",
    "    def load_database(self):\n",
    "        \"\"\"Load repositories database from JSON file if it exists\"\"\"\n",
    "        if os.path.exists(self.db_file):\n",
    "            try:\n",
    "                with open(self.db_file, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    self.stored_repositories = data.get('repositories', [])\n",
    "                    self.user_profiles = data.get('user_profiles', {})\n",
    "                print(f\"Loaded {len(self.stored_repositories)} repositories and {len(self.user_profiles)} user profiles from database\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading database: {e}\")\n",
    "                # Initialize empty database if loading fails\n",
    "                self.stored_repositories = []\n",
    "                self.user_profiles = {}\n",
    "        \n",
    "    def save_database(self):\n",
    "        \"\"\"Save repositories and user profiles to JSON database\"\"\"\n",
    "        try:\n",
    "            data = {\n",
    "                'repositories': self.stored_repositories,\n",
    "                'user_profiles': self.user_profiles\n",
    "            }\n",
    "            with open(self.db_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Database saved with {len(self.stored_repositories)} repositories and {len(self.user_profiles)} user profiles\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving database: {e}\")\n",
    "        \n",
    "    def extract_languages_from_readme(self, readme_text):\n",
    "        \"\"\"Extract programming languages from README content\"\"\"\n",
    "        # Common programming languages to look for\n",
    "        common_languages = {\n",
    "            'Python','python', 'JavaScript','javascript', 'Java','java', 'C++','c++', 'C#','c#', 'Ruby','ruby', 'PHP','php', 'Swift','swift',\n",
    "            'Go','GO','go', 'Rust','rust', 'TypeScript','typescript','Typescript', 'Kotlin','kotlin', 'R', 'MATLAB','matlab', 'Scala', 'sacala',\n",
    "            'HTML', 'CSS', 'SQL', 'Shell', 'Perl', 'Haskell', 'Julia'\n",
    "        }\n",
    "        \n",
    "        # Find languages in text\n",
    "        found_languages = set()\n",
    "        for lang in common_languages:\n",
    "            # Look for language mentions with word boundaries\n",
    "            pattern = r'\\b' + re.escape(lang) + r'\\b'\n",
    "            if re.search(pattern, readme_text, re.IGNORECASE):\n",
    "                found_languages.add(lang)\n",
    "                \n",
    "        return list(found_languages)\n",
    "\n",
    "    def extract_topics_from_readme(self, readme_text):\n",
    "        \"\"\"Extract potential topics/interests from README content\"\"\"\n",
    "        common_topics = {\n",
    "            # Programming Languages\n",
    "    \"Python\", \"JavaScript\", \"Java\", \"C++\", \"C#\", \"TypeScript\", \"Go\", \"Rust\",\n",
    "    \"PHP\", \"Swift\", \"Kotlin\", \"Ruby\", \"Dart\", \"R\", \"Shell\", \"Perl\", \"Lua\",\n",
    "    \"Scala\", \"Haskell\",\n",
    "\n",
    "    # Project Types\n",
    "    \"Web Development\", \"Mobile Apps\", \"Machine Learning\", \"Artificial Intelligence\",\n",
    "    \"Data Science\", \"Big Data\", \"Blockchain\", \"Cryptocurrency\", \"Cybersecurity\",\n",
    "    \"Game Development\", \"Automation & Scripting\", \"Internet of Things (IoT)\",\n",
    "    \"Embedded Systems\", \"Cloud Computing\", \"DevOps\",\n",
    "\n",
    "    # Software Development Topics\n",
    "    \"API Development\", \"RESTful APIs\", \"GraphQL\", \"gRPC\", \"DevOps\", \"CI/CD Pipelines\",\n",
    "    \"Docker\", \"Kubernetes\", \"AWS\", \"Azure\", \"Google Cloud Platform (GCP)\", \"Firebase\",\n",
    "    \"Microservices Architecture\", \"Backend Development\", \"Frontend Development\",\n",
    "    \"Full Stack Development\", \"Database Management\", \"SQL\", \"MySQL\", \"PostgreSQL\",\n",
    "    \"NoSQL\", \"MongoDB\", \"Redis\", \"Cassandra\", \"Serverless Computing\", \"Edge Computing\"\n",
    "        }\n",
    "        \n",
    "        found_topics = set()\n",
    "        for topic in common_topics:\n",
    "            if re.search(r'\\b' + re.escape(topic) + r'\\b', readme_text, re.IGNORECASE):\n",
    "                found_topics.add(topic)\n",
    "                \n",
    "        return list(found_topics)\n",
    "\n",
    "    def scrape_repository(self, repo_owner, repo_name):\n",
    "        \"\"\"Scrape detailed information about a GitHub repository\"\"\"\n",
    "        # Check if repository already exists in database\n",
    "        for repo in self.stored_repositories:\n",
    "            if repo.get('owner') == repo_owner and repo.get('name') == repo_name:\n",
    "                print(f\"Using cached data for repository {repo_owner}/{repo_name}\")\n",
    "                return repo\n",
    "                \n",
    "        repo_url = f'https://github.com/{repo_owner}/{repo_name}'\n",
    "        \n",
    "        try:\n",
    "            # Get repository page\n",
    "            repo_page = requests.get(repo_url, headers=self.headers)\n",
    "            if repo_page.status_code != 200:\n",
    "                print(f\"Failed to access repository: {repo_url}\")\n",
    "                return None\n",
    "                \n",
    "            repo_soup = BeautifulSoup(repo_page.text, 'html.parser')\n",
    "            \n",
    "            # Extract basic repository info\n",
    "            about_section = repo_soup.find('div', {'class': 'BorderGrid-row'})\n",
    "            description = \"\"\n",
    "            if about_section:\n",
    "                desc_p = about_section.find('p', {'class': 'f4'})\n",
    "                if desc_p:\n",
    "                    description = desc_p.text.strip()\n",
    "            \n",
    "            # Extract README content\n",
    "            readme_content = \"\"\n",
    "            article = repo_soup.find('article', {'class': 'markdown-body'})\n",
    "            if article:\n",
    "                readme_content = article.text.strip()\n",
    "            \n",
    "            # Extract languages\n",
    "            languages = {}\n",
    "            lang_bar = repo_soup.find('div', {'class': 'repository-lang-stats-graph'})\n",
    "            if lang_bar:\n",
    "                lang_items = lang_bar.find_all('span', {'class': 'language-color'})\n",
    "                for item in lang_items:\n",
    "                    if item.has_attr('aria-label'):\n",
    "                        lang_info = item['aria-label'].split()\n",
    "                        if len(lang_info) >= 3:\n",
    "                            lang_name = lang_info[0]\n",
    "                            lang_percent = float(lang_info[1].replace('%', ''))\n",
    "                            languages[lang_name] = lang_percent\n",
    "            \n",
    "            # If languages not found through language bar, try extracting from README\n",
    "            if not languages:\n",
    "                langs_from_readme = self.extract_languages_from_readme(readme_content)\n",
    "                for lang in langs_from_readme:\n",
    "                    languages[lang] = 1  # Placeholder percentage\n",
    "            \n",
    "            # Extract topics/tags\n",
    "            topics = []\n",
    "            topics_div = repo_soup.find('div', {'class': 'topic-tag-list'})\n",
    "            if topics_div:\n",
    "                topic_links = topics_div.find_all('a', {'class': 'topic-tag'})\n",
    "                topics = [t.text.strip() for t in topic_links]\n",
    "            \n",
    "            # If no topics found, extract potential topics from README\n",
    "            if not topics:\n",
    "                topics = self.extract_topics_from_readme(readme_content)\n",
    "            \n",
    "            # Extract stars/forks for popularity\n",
    "            stars = 0\n",
    "            forks = 0\n",
    "            \n",
    "            social_count = repo_soup.find_all('a', {'class': 'social-count'})\n",
    "            if len(social_count) >= 2:\n",
    "                try:\n",
    "                    stars = int(social_count[0].text.strip().replace(',', ''))\n",
    "                    forks = int(social_count[1].text.strip().replace(',', ''))\n",
    "                except (ValueError, IndexError):\n",
    "                    pass\n",
    "            \n",
    "            repository_data = {\n",
    "                'owner': repo_owner,\n",
    "                'name': repo_name,\n",
    "                'url': repo_url,\n",
    "                'description': description,\n",
    "                'readme_content': readme_content,\n",
    "                'languages': languages,\n",
    "                'topics': topics,\n",
    "                'stars': stars,\n",
    "                'forks': forks,\n",
    "                'last_updated': self._get_current_timestamp()\n",
    "            }\n",
    "            \n",
    "            # Add to stored repositories\n",
    "            self.stored_repositories.append(repository_data)\n",
    "            # Save updated database\n",
    "            self.save_database()\n",
    "            \n",
    "            return repository_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping repository {repo_owner}/{repo_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _get_current_timestamp(self):\n",
    "        \"\"\"Get current timestamp for database entries\"\"\"\n",
    "        from datetime import datetime\n",
    "        return datetime.now().isoformat()\n",
    "\n",
    "    def scrape_user_profile(self, username):\n",
    "        \"\"\"Scrape GitHub user profile and their repositories\"\"\"\n",
    "        # Check if user profile already exists in database\n",
    "        if username in self.user_profiles:\n",
    "            print(f\"Using cached profile for user {username}\")\n",
    "            return self.user_profiles[username]\n",
    "            \n",
    "        profile_url = f'https://github.com/{username}'\n",
    "        repos_url = f'https://github.com/{username}?tab=repositories'\n",
    "        \n",
    "        try:\n",
    "            # Get profile page\n",
    "            profile_page = requests.get(profile_url, headers=self.headers)\n",
    "            if profile_page.status_code != 200:\n",
    "                print(f\"Failed to access profile: {profile_url}\")\n",
    "                return None\n",
    "                \n",
    "            profile_soup = BeautifulSoup(profile_page.text, 'html.parser')\n",
    "            \n",
    "            # Extract basic profile info\n",
    "            bio_div = profile_soup.find('div', {'class': 'p-note'})\n",
    "            bio = bio_div.text.strip() if bio_div else \"\"\n",
    "            \n",
    "            # Check for README profile\n",
    "            readme_content = \"\"\n",
    "            readme_url = f'https://github.com/{username}/{username}'\n",
    "            readme_page = requests.get(readme_url, headers=self.headers)\n",
    "            if readme_page.status_code == 200:\n",
    "                readme_soup = BeautifulSoup(readme_page.text, 'html.parser')\n",
    "                article = readme_soup.find('article', {'class': 'markdown-body'})\n",
    "                if article:\n",
    "                    readme_content = article.text.strip()\n",
    "            \n",
    "            # Get repositories page\n",
    "            repos_page = requests.get(repos_url, headers=self.headers)\n",
    "            repos_soup = BeautifulSoup(repos_page.text, 'html.parser')\n",
    "            \n",
    "            # Extract repositories\n",
    "            repos = []\n",
    "            repo_list = repos_soup.find_all('li', {'class': 'source'})\n",
    "            if not repo_list:  # Try alternative class if not found\n",
    "                repo_list = repos_soup.find_all('li', {'class': 'col-12'})\n",
    "            \n",
    "            # If still no repos found, try a different approach\n",
    "            if not repo_list:\n",
    "                repo_items = repos_soup.find_all('div', {'class': 'wb-break-word'})\n",
    "                for item in repo_items:\n",
    "                    repo_link = item.find('a')\n",
    "                    if repo_link and '/' in repo_link.text:\n",
    "                        repo_parts = repo_link.text.strip().split('/')\n",
    "                        if len(repo_parts) == 2 and repo_parts[0] == username:\n",
    "                            repo_name = repo_parts[1]\n",
    "                            repos.append({\n",
    "                                'name': repo_name,\n",
    "                                'owner': username,\n",
    "                                'url': f'https://github.com/{username}/{repo_name}'\n",
    "                            })\n",
    "            else:\n",
    "                for repo in repo_list[:10]:\n",
    "                    name_elem = repo.find('a', {'itemprop': 'name codeRepository'})\n",
    "                    if not name_elem:\n",
    "                        name_elem = repo.find('a', {'class': 'mr-2'})\n",
    "                    \n",
    "                    if not name_elem:\n",
    "                        continue\n",
    "                        \n",
    "                    desc_elem = repo.find('p', {'class': 'pinned-item-desc'}) or repo.find('p', {'class': 'mb-0'})\n",
    "                    lang_elem = repo.find('span', {'itemprop': 'programmingLanguage'}) or repo.find('span', {'class': 'ml-0'})\n",
    "                    \n",
    "                    name = name_elem.text.strip()\n",
    "                    description = desc_elem.text.strip() if desc_elem else \"\"\n",
    "                    language = lang_elem.text.strip() if lang_elem else \"\"\n",
    "                    \n",
    "                    repos.append({\n",
    "                        'name': name,\n",
    "                        'owner': username,\n",
    "                        'url': f'https://github.com/{username}/{name}',\n",
    "                        'description': description,\n",
    "                        'language': language\n",
    "                    })\n",
    "            \n",
    "            # Scrape detailed info for each repository\n",
    "            detailed_repos = []\n",
    "            for repo in repos[:10]:  # Limit to first 10 repos to avoid rate limiting\n",
    "                detailed_repo = self.scrape_repository(username, repo['name'])\n",
    "                if detailed_repo:\n",
    "                    detailed_repos.append(detailed_repo)\n",
    "            \n",
    "            # Extract languages from repos and README\n",
    "            languages_from_readme = self.extract_languages_from_readme(readme_content)\n",
    "            languages_from_repos = []\n",
    "            for repo in detailed_repos:\n",
    "                languages_from_repos.extend(list(repo['languages'].keys()))\n",
    "            \n",
    "            # Count language occurrences\n",
    "            all_languages = languages_from_readme + languages_from_repos\n",
    "            language_counter = Counter(all_languages)\n",
    "            \n",
    "            # Extract topics from repos and README\n",
    "            topics_from_readme = self.extract_topics_from_readme(readme_content)\n",
    "            topics_from_repos = []\n",
    "            for repo in detailed_repos:\n",
    "                topics_from_repos.extend(repo['topics'])\n",
    "            \n",
    "            # Count topic occurrences\n",
    "            all_topics = topics_from_readme + topics_from_repos\n",
    "            topic_counter = Counter(all_topics)\n",
    "            \n",
    "            # Create user profile\n",
    "            user_profile = {\n",
    "                'username': username,\n",
    "                'bio': bio,\n",
    "                'readme_content': readme_content,\n",
    "                'repositories': [repo['name'] for repo in detailed_repos],  # Store just repo names to avoid duplication\n",
    "                'top_languages': dict(language_counter.most_common(10)),\n",
    "                'top_topics': dict(topic_counter.most_common(10)),\n",
    "                'last_updated': self._get_current_timestamp()\n",
    "            }\n",
    "            \n",
    "            # Cache user profile\n",
    "            self.user_profiles[username] = user_profile\n",
    "            \n",
    "            # Save updated database\n",
    "            self.save_database()\n",
    "            \n",
    "            return user_profile\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping profile for {username}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_project_embedding(self, repo_data):\n",
    "        \"\"\"Create text embedding for a repository\"\"\"\n",
    "        project_text = f\"\"\"\n",
    "        Repository: {repo_data['name']}\n",
    "        Owner: {repo_data['owner']}\n",
    "        Description: {repo_data['description']}\n",
    "        Languages: {', '.join(repo_data['languages'].keys())}\n",
    "        Topics: {', '.join(repo_data['topics'])}\n",
    "        README Content: {repo_data.get('readme_content', '')}\n",
    "        \"\"\"\n",
    "        return self.model.encode([project_text])[0]\n",
    "\n",
    "    def create_user_preference_embedding(self, user_profile):\n",
    "        \"\"\"Create embedding representing user's preferences based on their profile\"\"\"\n",
    "        # Combine bio, readme, top languages and topics\n",
    "        preference_text = f\"\"\"\n",
    "        Bio: {user_profile['bio']}\n",
    "        Top Languages: {', '.join(user_profile['top_languages'].keys())}\n",
    "        Top Topics: {', '.join(user_profile['top_topics'].keys())}\n",
    "        README Content: {user_profile['readme_content']}\n",
    "        \"\"\"\n",
    "        return self.model.encode([preference_text])[0]\n",
    "\n",
    "    def get_recommendations_from_database(self, user_profile, max_recommendations=5):\n",
    "        \"\"\"\n",
    "        Get project recommendations for a user from the stored database\n",
    "        \n",
    "        Parameters:\n",
    "        user_profile (dict): User profile data\n",
    "        max_recommendations (int): Maximum number of recommendations to return\n",
    "        \n",
    "        Returns:\n",
    "        list: Recommended repositories from database\n",
    "        \"\"\"\n",
    "        if not self.stored_repositories:\n",
    "            return []\n",
    "            \n",
    "        # Skip user's own repositories\n",
    "        username = user_profile['username']\n",
    "        candidate_repos = [repo for repo in self.stored_repositories \n",
    "                          if repo['owner'] != username]\n",
    "        \n",
    "        if not candidate_repos:\n",
    "            return []\n",
    "        \n",
    "        # Get embedding for user preferences\n",
    "        user_embedding = self.create_user_preference_embedding(user_profile)\n",
    "        \n",
    "        # Create embeddings for candidate repositories\n",
    "        repo_embeddings = np.array([\n",
    "            self.create_project_embedding(repo) for repo in candidate_repos\n",
    "        ])\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity([user_embedding], repo_embeddings)[0]\n",
    "        \n",
    "        # Get top similar repositories\n",
    "        top_indices = np.argsort(similarities)[-max_recommendations:][::-1]\n",
    "        \n",
    "        # Create recommendation objects\n",
    "        recommendations = []\n",
    "        for idx, similarity in zip(top_indices, similarities[top_indices]):\n",
    "            repo = candidate_repos[idx]\n",
    "            recommendations.append({\n",
    "                'owner': repo['owner'],\n",
    "                'name': repo['name'],\n",
    "                'description': repo['description'],\n",
    "                'url': repo['url'],\n",
    "                'languages': list(repo['languages'].keys()),\n",
    "                'topics': repo['topics'],\n",
    "                'similarity_score': float(similarity),\n",
    "                'stars': repo.get('stars', 0),\n",
    "                'forks': repo.get('forks', 0),\n",
    "                'from_database': True\n",
    "            })\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "    def scrape_trending_repositories(self, language=None):\n",
    "        \"\"\"Scrape trending repositories for additional recommendations\"\"\"\n",
    "        trending_url = 'https://github.com/trending'\n",
    "        if language:\n",
    "            trending_url += f'/{language}'\n",
    "        \n",
    "        try:\n",
    "            trending_page = requests.get(trending_url, headers=self.headers)\n",
    "            trending_soup = BeautifulSoup(trending_page.text, 'html.parser')\n",
    "            \n",
    "            trending_repos = []\n",
    "            repo_articles = trending_soup.find_all('article', {'class': 'Box-row'})\n",
    "            \n",
    "            for article in repo_articles[:10]:\n",
    "                repo_link = article.find('h2').find('a')\n",
    "                if not repo_link:\n",
    "                    continue\n",
    "                \n",
    "                repo_path = repo_link['href'].strip('/')\n",
    "                if '/' in repo_path:\n",
    "                    owner, name = repo_path.split('/')\n",
    "                    \n",
    "                    description_p = article.find('p')\n",
    "                    description = description_p.text.strip() if description_p else \"\"\n",
    "                    \n",
    "                    language_span = article.find('span', {'itemprop': 'programmingLanguage'})\n",
    "                    language = language_span.text.strip() if language_span else \"\"\n",
    "                    \n",
    "                    trending_repos.append({\n",
    "                        'owner': owner,\n",
    "                        'name': name,\n",
    "                        'description': description,\n",
    "                        'language': language,\n",
    "                        'url': f'https://github.com/{repo_path}'\n",
    "                    })\n",
    "            \n",
    "            return trending_repos\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping trending repositories: {e}\")\n",
    "            return []\n",
    "\n",
    "    def search_repositories(self, query, language=None):\n",
    "        \"\"\"Search for repositories based on query\"\"\"\n",
    "        search_url = f'https://github.com/search?q={query}'\n",
    "        if language:\n",
    "            search_url += f'+language:{language}'\n",
    "        search_url += '&type=repositories'\n",
    "        \n",
    "        try:\n",
    "            search_page = requests.get(search_url, headers=self.headers)\n",
    "            search_soup = BeautifulSoup(search_page.text, 'html.parser')\n",
    "            \n",
    "            search_results = []\n",
    "            result_items = search_soup.find_all('li', {'class': 'repo-list-item'})\n",
    "            \n",
    "            # If the repo-list-item class isn't found, try another selector\n",
    "            if not result_items:\n",
    "                result_items = search_soup.select('div.Box-row')\n",
    "            \n",
    "            for item in result_items[:10]:\n",
    "                # Try different selectors to find repository links\n",
    "                repo_link = None\n",
    "                if item.find('a', {'class': 'v-align-middle'}):\n",
    "                    repo_link = item.find('a', {'class': 'v-align-middle'})\n",
    "                elif item.find('a', {'data-hydro-click-hmac'}):\n",
    "                    links = item.find_all('a')\n",
    "                    for link in links:\n",
    "                        if '/' in link.text and 'github.com' not in link.text:\n",
    "                            repo_link = link\n",
    "                            break\n",
    "                \n",
    "                if not repo_link or not repo_link.has_attr('href'):\n",
    "                    continue\n",
    "                \n",
    "                repo_path = repo_link['href'].strip('/')\n",
    "                if '/' in repo_path:\n",
    "                    owner, name = repo_path.split('/')\n",
    "                    \n",
    "                    # Try different selectors for description\n",
    "                    description_p = item.find('p', {'class': 'mb-1'}) or item.find('p', {'class': 'col-9'})\n",
    "                    description = description_p.text.strip() if description_p else \"\"\n",
    "                    \n",
    "                    search_results.append({\n",
    "                        'owner': owner,\n",
    "                        'name': name,\n",
    "                        'description': description,\n",
    "                        'url': f'https://github.com/{repo_path}'\n",
    "                    })\n",
    "            \n",
    "            return search_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error searching repositories: {e}\")\n",
    "            return []   \n",
    "    \n",
    "    def get_recommendations_for_topic(self, topic, language=None, max_recommendations=10):\n",
    "        \"\"\"\n",
    "        Get project recommendations related to a specific topic or language\n",
    "        \n",
    "        Parameters:\n",
    "        topic (str): Topic or keyword to search for\n",
    "        language (str, optional): Programming language to filter by\n",
    "        max_recommendations (int): Maximum number of recommendations to return\n",
    "        \n",
    "        Returns:\n",
    "        list: Recommended repositories\n",
    "        \"\"\"\n",
    "        # Search for repositories on GitHub\n",
    "        search_results = self.search_repositories(topic, language)\n",
    "        candidate_repos = []\n",
    "        \n",
    "        # Get detailed repository data\n",
    "        for repo in search_results:\n",
    "            detailed_repo = self.scrape_repository(repo['owner'], repo['name'])\n",
    "            if detailed_repo:\n",
    "                candidate_repos.append(detailed_repo)\n",
    "        \n",
    "        # If not enough from search, include trending repositories\n",
    "        if len(candidate_repos) < max_recommendations and language:\n",
    "            trending_repos = self.scrape_trending_repositories(language)\n",
    "            for repo in trending_repos:\n",
    "                detailed_repo = self.scrape_repository(repo['owner'], repo['name'])\n",
    "                if detailed_repo and detailed_repo not in candidate_repos:\n",
    "                    candidate_repos.append(detailed_repo)\n",
    "        \n",
    "        # If still not enough, add from database that match the topic/language\n",
    "        if len(candidate_repos) < max_recommendations:\n",
    "            for repo in self.stored_repositories:\n",
    "                # Check if repo matches topic\n",
    "                topic_match = (\n",
    "                    topic.lower() in repo['name'].lower() or\n",
    "                    topic.lower() in repo['description'].lower() or\n",
    "                    any(topic.lower() in t.lower() for t in repo['topics'])\n",
    "                )\n",
    "                \n",
    "                # Check if repo matches language (if specified)\n",
    "                lang_match = True\n",
    "                if language:\n",
    "                    lang_match = any(language.lower() == lang.lower() for lang in repo['languages'])\n",
    "                \n",
    "                if topic_match and lang_match and repo not in candidate_repos:\n",
    "                    candidate_repos.append(repo)\n",
    "                    if len(candidate_repos) >= max_recommendations * 2:\n",
    "                        break\n",
    "        \n",
    "        # If no candidates found, return empty list\n",
    "        if not candidate_repos:\n",
    "            return []\n",
    "        \n",
    "        # Create topic embedding\n",
    "        topic_text = f\"Topic: {topic}\"\n",
    "        if language:\n",
    "            topic_text += f\"\\nLanguage: {language}\"\n",
    "        topic_embedding = self.model.encode([topic_text])[0]\n",
    "        \n",
    "        # Create embeddings for candidate repositories\n",
    "        repo_embeddings = np.array([\n",
    "            self.create_project_embedding(repo) for repo in candidate_repos\n",
    "        ])\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity([topic_embedding], repo_embeddings)[0]\n",
    "        \n",
    "        # Get top similar repositories\n",
    "        top_indices = np.argsort(similarities)[-max_recommendations:][::-1]\n",
    "        \n",
    "        # Create recommendation objects\n",
    "        recommendations = []\n",
    "        for idx, similarity in zip(top_indices, similarities[top_indices]):\n",
    "            repo = candidate_repos[idx]\n",
    "            recommendations.append({\n",
    "                'owner': repo['owner'],\n",
    "                'name': repo['name'],\n",
    "                'description': repo['description'],\n",
    "                'url': repo['url'],\n",
    "                'languages': list(repo['languages'].keys()),\n",
    "                'topics': repo['topics'],\n",
    "                'similarity_score': float(similarity),\n",
    "                'stars': repo.get('stars', 0),\n",
    "                'forks': repo.get('forks', 0)\n",
    "            })\n",
    "        \n",
    "        return recommendations\n",
    "            \n",
    "    def get_recommendations_for_user(self, username, max_recommendations=10, web_search_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Get project recommendations for a user based on their profile and repositories,\n",
    "        with a specified ratio coming from database vs web search\n",
    "        \n",
    "        Parameters:\n",
    "        username (str): GitHub username\n",
    "        max_recommendations (int): Total number of recommendations to return\n",
    "        web_search_ratio (float): Ratio of recommendations to get from web search (0-1)\n",
    "        \n",
    "        Returns:\n",
    "        list: Recommended repositories\n",
    "        \"\"\"\n",
    "        # Scrape user profile if not already cached\n",
    "        if username not in self.user_profiles:\n",
    "            user_profile = self.scrape_user_profile(username)\n",
    "            if not user_profile:\n",
    "                print(f\"Could not retrieve profile for {username}\")\n",
    "                return []\n",
    "        else:\n",
    "            user_profile = self.user_profiles[username]\n",
    "        \n",
    "        # Calculate how many recommendations to get from each source\n",
    "        db_count = int(max_recommendations * (1 - web_search_ratio))\n",
    "        web_count = max_recommendations - db_count\n",
    "        \n",
    "        all_recommendations = []\n",
    "        \n",
    "        # Get recommendations from database\n",
    "        if self.stored_repositories and db_count > 0:\n",
    "            print(f\"Getting {db_count} recommendations from database...\")\n",
    "            db_recommendations = self.get_recommendations_from_database(user_profile, max_recommendations=db_count)\n",
    "            all_recommendations.extend(db_recommendations)\n",
    "        \n",
    "        # Get recommendations from web search\n",
    "        if web_count > 0:\n",
    "            print(f\"Getting {web_count} recommendations from GitHub search...\")\n",
    "            \n",
    "            # Prepare repositories for recommendation\n",
    "            candidate_repos = []\n",
    "            \n",
    "            # Include trending repositories based on user's top languages\n",
    "            if user_profile['top_languages']:\n",
    "                top_language = list(user_profile['top_languages'].keys())[0]\n",
    "                trending_repos = self.scrape_trending_repositories(language=top_language)\n",
    "                for repo in trending_repos:\n",
    "                    detailed_repo = self.scrape_repository(repo['owner'], repo['name'])\n",
    "                    if detailed_repo:\n",
    "                        candidate_repos.append(detailed_repo)\n",
    "            \n",
    "            # If not enough candidates, search for repos based on user's top topics\n",
    "            if len(candidate_repos) < web_count * 2 and user_profile['top_topics']:\n",
    "                top_topic = list(user_profile['top_topics'].keys())[0]\n",
    "                search_results = self.search_repositories(top_topic)\n",
    "                for repo in search_results:\n",
    "                    detailed_repo = self.scrape_repository(repo['owner'], repo['name'])\n",
    "                    if detailed_repo and detailed_repo['owner'] != username:\n",
    "                        candidate_repos.append(detailed_repo)\n",
    "            \n",
    "            # If still not enough, do a general search based on username\n",
    "            if len(candidate_repos) < web_count * 2:\n",
    "                search_results = self.search_repositories(username)\n",
    "                for repo in search_results:\n",
    "                    detailed_repo = self.scrape_repository(repo['owner'], repo['name'])\n",
    "                    if detailed_repo and detailed_repo['owner'] != username:\n",
    "                        candidate_repos.append(detailed_repo)\n",
    "            \n",
    "            # If have candidates, get web search recommendations\n",
    "            if candidate_repos:\n",
    "                # Get embedding for user preferences\n",
    "                user_embedding = self.create_user_preference_embedding(user_profile)\n",
    "                \n",
    "                # Create embeddings for candidate repositories\n",
    "                repo_embeddings = np.array([\n",
    "                    self.create_project_embedding(repo) for repo in candidate_repos\n",
    "                ])\n",
    "                \n",
    "                # Calculate similarities\n",
    "                similarities = cosine_similarity([user_embedding], repo_embeddings)[0]\n",
    "                \n",
    "                # Get top similar repositories\n",
    "                top_indices = np.argsort(similarities)[-web_count:][::-1]\n",
    "                \n",
    "                # Add web search recommendations\n",
    "                for idx, similarity in zip(top_indices, similarities[top_indices]):\n",
    "                    repo = candidate_repos[idx]\n",
    "                    all_recommendations.append({\n",
    "                        'owner': repo['owner'],\n",
    "                        'name': repo['name'],\n",
    "                        'description': repo['description'],\n",
    "                        'url': repo['url'],\n",
    "                        'languages': list(repo['languages'].keys()),\n",
    "                        'topics': repo['topics'],\n",
    "                        'similarity_score': float(similarity),\n",
    "                        'stars': repo.get('stars', 0),\n",
    "                        'forks': repo.get('forks', 0),\n",
    "                        'from_database': False\n",
    "                    })\n",
    "        \n",
    "        # Return the recommendations, limiting to requested number\n",
    "        return all_recommendations[:max_recommendations]\n",
    "    \n",
    "    def clean_old_database_entries(self, days_threshold=30):\n",
    "        \"\"\"Clean old entries from the database based on age threshold\"\"\"\n",
    "        from datetime import datetime, timedelta\n",
    "        \n",
    "        threshold_date = datetime.now() - timedelta(days=days_threshold)\n",
    "        \n",
    "        # Clean repositories\n",
    "        before_count = len(self.stored_repositories)\n",
    "        self.stored_repositories = [\n",
    "            repo for repo in self.stored_repositories\n",
    "            if not ('last_updated' in repo and \n",
    "                   datetime.fromisoformat(repo['last_updated']) < threshold_date)\n",
    "        ]\n",
    "        \n",
    "        # Clean user profiles\n",
    "        profiles_before = len(self.user_profiles)\n",
    "        for username in list(self.user_profiles.keys()):\n",
    "            profile = self.user_profiles[username]\n",
    "            if 'last_updated' in profile and datetime.fromisoformat(profile['last_updated']) < threshold_date:\n",
    "                del self.user_profiles[username]\n",
    "        \n",
    "        # Report changes\n",
    "        repos_removed = before_count - len(self.stored_repositories)\n",
    "        profiles_removed = profiles_before - len(self.user_profiles)\n",
    "        \n",
    "        if repos_removed > 0 or profiles_removed > 0:\n",
    "            print(f\"Database cleaned: removed {repos_removed} repositories and {profiles_removed} user profiles\")\n",
    "            # Save the cleaned database\n",
    "            self.save_database()\n",
    "            \n",
    "        return repos_removed, profiles_removed\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    recommender = GitHubProjectRecommender()\n",
    "    \n",
    "    print(\"GitHub Project Recommender\")\n",
    "    print(\"==========================\")\n",
    "    print(f\"Database contains {len(recommender.stored_repositories)} repositories and {len(recommender.user_profiles)} user profiles\")\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\nOptions:\")\n",
    "        print(\"1. Get recommendations for a GitHub user\")\n",
    "        print(\"2. Search and recommend from specific topics/languages\")\n",
    "        print(\"3. Clean old database entries\")\n",
    "        print(\"4. Show database statistics\")\n",
    "        print(\"5. Exit\")\n",
    "        \n",
    "        choice = input(\"\\nEnter your choice (1-5): \").strip()\n",
    "        \n",
    "        if choice == '1':\n",
    "            username = input(\"\\nEnter GitHub username: \").strip()\n",
    "            \n",
    "            # Ask for number of recommendations\n",
    "            while True:\n",
    "                try:\n",
    "                    num_recommendations = int(input(\"How many recommendations would you like? (1-20): \"))\n",
    "                    if 1 <= num_recommendations <= 20:\n",
    "                        break\n",
    "                    print(\"Please enter a number between 1 and 20.\")\n",
    "                except ValueError:\n",
    "                    print(\"Please enter a valid number.\")\n",
    "            \n",
    "            print(\"\\nGetting recommendations...\")\n",
    "            recommendations = recommender.get_recommendations_for_user(\n",
    "                username, \n",
    "                max_recommendations=num_recommendations\n",
    "            )\n",
    "            \n",
    "            if recommendations:\n",
    "                print(f\"\\nTop {len(recommendations)} recommended repositories for {username}:\")\n",
    "                for i, rec in enumerate(recommendations, 1):\n",
    "                    print(f\"\\n{i}. {rec['owner']}/{rec['name']}\")\n",
    "                    print(f\"   Description: {rec['description']}\")\n",
    "                    print(f\"   Languages: {', '.join(rec['languages'])}\")\n",
    "                    print(f\"   Topics: {', '.join(rec['topics'])}\")\n",
    "                    print(f\"   Similarity Score: {rec['similarity_score']:.2f}\")\n",
    "\n",
    "                    print(f\"   URL: {rec['url']}\")\n",
    "            else:\n",
    "                print(f\"\\nNo recommendations found for {username}\")\n",
    "                \n",
    "        elif choice == '2':\n",
    "            topic = input(\"\\nEnter topic or keyword to search for: \").strip()\n",
    "            language = input(\"Enter programming language (or press Enter to skip): \").strip()\n",
    "            \n",
    "            if not language:\n",
    "                language = None\n",
    "                \n",
    "            while True:\n",
    "                try:\n",
    "                    num_recommendations = int(input(\"How many recommendations would you like? (1-20): \"))\n",
    "                    if 1 <= num_recommendations <= 20:\n",
    "                        break\n",
    "                    print(\"Please enter a number between 1 and 20.\")\n",
    "                except ValueError:\n",
    "                    print(\"Please enter a valid number.\")\n",
    "            \n",
    "            print(\"\\nSearching for recommendations...\")\n",
    "            recommendations = recommender.get_recommendations_for_topic(\n",
    "                topic,\n",
    "                language=language,\n",
    "                max_recommendations=num_recommendations\n",
    "            )\n",
    "            \n",
    "            if recommendations:\n",
    "                print(f\"\\nTop {len(recommendations)} recommended repositories for topic '{topic}'{f' in {language}' if language else ''}:\")\n",
    "                for i, rec in enumerate(recommendations, 1):\n",
    "                    print(f\"\\n{i}. {rec['owner']}/{rec['name']}\")\n",
    "                    print(f\"   Description: {rec['description']}\")\n",
    "                    print(f\"   Languages: {', '.join(rec['languages'])}\")\n",
    "                    print(f\"   Topics: {', '.join(rec['topics'])}\")\n",
    "                    print(f\"   Similarity Score: {rec['similarity_score']:.2f}\")\n",
    "                    print(f\"   Stars: {rec['stars']}, Forks: {rec['forks']}\")\n",
    "                    print(f\"   URL: {rec['url']}\")\n",
    "            else:\n",
    "                print(f\"\\nNo recommendations found for topic '{topic}'{f' in {language}' if language else ''}\")\n",
    "                \n",
    "        elif choice == '3':\n",
    "            while True:\n",
    "                try:\n",
    "                    days = int(input(\"\\nEnter age threshold in days (7-90): \"))\n",
    "                    if 7 <= days <= 90:\n",
    "                        break\n",
    "                    print(\"Please enter a number between 7 and 90.\")\n",
    "                except ValueError:\n",
    "                    print(\"Please enter a valid number.\")\n",
    "            \n",
    "            repos_removed, profiles_removed = recommender.clean_old_database_entries(days_threshold=days)\n",
    "            print(f\"\\nCleaned database: removed {repos_removed} repositories and {profiles_removed} user profiles\")\n",
    "            \n",
    "        elif choice == '4':\n",
    "            print(\"\\nDatabase Statistics:\")\n",
    "            print(f\"Total repositories: {len(recommender.stored_repositories)}\")\n",
    "            print(f\"Total user profiles: {len(recommender.user_profiles)}\")\n",
    "            \n",
    "            if recommender.stored_repositories:\n",
    "                languages = {}\n",
    "                topics = {}\n",
    "                total_stars = 0\n",
    "                total_forks = 0\n",
    "                \n",
    "                for repo in recommender.stored_repositories:\n",
    "                    # Count languages\n",
    "                    for lang in repo['languages']:\n",
    "                        languages[lang] = languages.get(lang, 0) + 1\n",
    "                    \n",
    "                    # Count topics\n",
    "                    for topic in repo['topics']:\n",
    "                        topics[topic] = topics.get(topic, 0) + 1\n",
    "                    \n",
    "                    # Sum stars and forks\n",
    "                    total_stars += repo.get('stars', 0)\n",
    "                    total_forks += repo.get('forks', 0)\n",
    "                \n",
    "                print(\"\\nTop Languages:\")\n",
    "                for lang, count in sorted(languages.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "                    print(f\"- {lang}: {count} repositories\")\n",
    "                \n",
    "                print(\"\\nTop Topics:\")\n",
    "                for topic, count in sorted(topics.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "                    print(f\"- {topic}: {count} repositories\")\n",
    "                \n",
    "                print(f\"\\nTotal Stars: {total_stars}\")\n",
    "                print(f\"Total Forks: {total_forks}\")\n",
    "                \n",
    "        elif choice == '5':\n",
    "            print(\"\\nExiting GitHub Project Recommender. Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            print(\"\\nInvalid choice. Please enter a number between 1 and 5.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
