{
  "repositories": [
    {
      "owner": "chanakya2006",
      "name": "github-repo-recommendation-on-basis-of-profile",
      "url": "https://github.com/chanakya2006/github-repo-recommendation-on-basis-of-profile",
      "description": "",
      "readme_content": "Foss-Hackathon-2025\nWe Going Big with this one",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:43.755061"
    },
    {
      "owner": "chanakya2006",
      "name": "fitness_api",
      "url": "https://github.com/chanakya2006/fitness_api",
      "description": "",
      "readme_content": "My first time creating a backend in GO using ECHO.",
      "languages": {
        "Go": 1,
        "go": 1,
        "GO": 1
      },
      "topics": [
        "Go"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:44.625944"
    },
    {
      "owner": "chanakya2006",
      "name": "pdf_chatbot",
      "url": "https://github.com/chanakya2006/pdf_chatbot",
      "description": "",
      "readme_content": "",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:45.436771"
    },
    {
      "owner": "chanakya2006",
      "name": "python",
      "url": "https://github.com/chanakya2006/python",
      "description": "",
      "readme_content": "python",
      "languages": {
        "python": 1,
        "Python": 1
      },
      "topics": [
        "Python"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:46.090163"
    },
    {
      "owner": "spf13",
      "name": "cobra",
      "url": "https://github.com/spf13/cobra",
      "description": "A Commander for modern Go CLI interactions",
      "readme_content": "Cobra is a library for creating powerful modern CLI applications.\nCobra is used in many Go projects such as Kubernetes,\nHugo, and GitHub CLI to\nname a few. This list contains a more extensive list of projects using Cobra.\n\n\n\n\nOverview\nCobra is a library providing a simple interface to create powerful modern CLI\ninterfaces similar to git & go tools.\nCobra provides:\n\nEasy subcommand-based CLIs: app server, app fetch, etc.\nFully POSIX-compliant flags (including short & long versions)\nNested subcommands\nGlobal, local and cascading flags\nIntelligent suggestions (app srver... did you mean app server?)\nAutomatic help generation for commands and flags\nGrouping help for subcommands\nAutomatic help flag recognition of -h, --help, etc.\nAutomatically generated shell autocomplete for your application (bash, zsh, fish, powershell)\nAutomatically generated man pages for your application\nCommand aliases so you can change things without breaking them\nThe flexibility to define your own help, usage, etc.\nOptional seamless integration with viper for 12-factor apps\n\nConcepts\nCobra is built on a structure of commands, arguments & flags.\nCommands represent actions, Args are things and Flags are modifiers for those actions.\nThe best applications read like sentences when used, and as a result, users\nintuitively know how to interact with them.\nThe pattern to follow is\nAPPNAME VERB NOUN --ADJECTIVE\nor\nAPPNAME COMMAND ARG --FLAG.\nA few good real world examples may better illustrate this point.\nIn the following example, 'server' is a command, and 'port' is a flag:\nhugo server --port=1313\n\nIn this command we are telling Git to clone the url bare.\ngit clone URL --bare\n\nCommands\nCommand is the central point of the application. Each interaction that\nthe application supports will be contained in a Command. A command can\nhave children commands and optionally run an action.\nIn the example above, 'server' is the command.\nMore about cobra.Command\nFlags\nA flag is a way to modify the behavior of a command. Cobra supports\nfully POSIX-compliant flags as well as the Go flag package.\nA Cobra command can define flags that persist through to children commands\nand flags that are only available to that command.\nIn the example above, 'port' is the flag.\nFlag functionality is provided by the pflag\nlibrary, a fork of the flag standard library\nwhich maintains the same interface while adding POSIX compliance.\nInstalling\nUsing Cobra is easy. First, use go get to install the latest version\nof the library.\ngo get -u github.com/spf13/cobra@latest\n\nNext, include Cobra in your application:\nimport \"github.com/spf13/cobra\"\nUsage\ncobra-cli is a command line program to generate cobra applications and command files.\nIt will bootstrap your application scaffolding to rapidly\ndevelop a Cobra-based application. It is the easiest way to incorporate Cobra into your application.\nIt can be installed by running:\ngo install github.com/spf13/cobra-cli@latest\n\nFor complete details on using the Cobra-CLI generator, please read The Cobra Generator README\nFor complete details on using the Cobra library, please read The Cobra User Guide.\nLicense\nCobra is released under the Apache 2.0 license. See LICENSE.txt",
      "languages": {
        "Go": 1,
        "go": 1,
        "GO": 1,
        "Shell": 1
      },
      "topics": [
        "Go",
        "Kubernetes",
        "Shell"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:48.190199"
    },
    {
      "owner": "subtrace",
      "name": "subtrace",
      "url": "https://github.com/subtrace/subtrace",
      "description": "Wireshark for Docker containers",
      "readme_content": "Subtrace\nHome ‚Äî Docs ‚Äî Discord\nWireshark for Docker containers\n\nSubtrace is Wireshark for your Docker containers. It lets developers see all\nincoming and outgoing requests in their backend server so that they can resolve\nproduction issues faster.\nFeatures\n\nWorks out-of-the-box\nNo code changes needed\nSupports all languages (Python + Node + Go + everything else)\nSee full payload, headers, status code, and latency\nLess than 100¬µs performance overhead\nBuilt on Clickhouse\nOpen source\n\nCode Contributions\nWhile Subtrace is open source,\nwe're not currently accepting pull requests. This is because we're a startup\nwith a very small team and we don't have the resources or documentation\nnecessary to maintain a good open source community in a way that still allows\nus to move quickly. This will probably change in the future.\nWith that said, we welcome all feature requests and bug reports, so feel free\nto open an issue.",
      "languages": {
        "python": 1,
        "Python": 1,
        "go": 1,
        "GO": 1,
        "Go": 1
      },
      "topics": [
        "Go",
        "Python",
        "Docker"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:48.842550"
    },
    {
      "owner": "chaitin",
      "name": "SafeLine",
      "url": "https://github.com/chaitin/SafeLine",
      "description": "SafeLine is a self-hosted WAF(Web Application Firewall) / reverse proxy to protect your web apps from attacks and exploits.",
      "readme_content": "SafeLine - Make your web apps secure\n\n\nüè† Website ¬† | ¬†\n  üìñ Docs ¬† | ¬†\n  üîç Live Demo ¬† | ¬†\n  üôã‚Äç‚ôÇÔ∏è Discord ¬† | ¬†\n  ‰∏≠ÊñáÁâà\n\nüëã INTRODUCTION\nSafeLine is a self-hosted WAF(Web Application Firewall) to protect your web apps from attacks and exploits.\nA web application firewall helps protect web apps by filtering and monitoring HTTP traffic between a web application and the Internet. It typically protects web apps from attacks such as SQL injection, XSS, code injection, os command injection, CRLF injection, ldap injection, xpath injection, RCE, XXE, SSRF, path traversal, backdoor, bruteforce, http-flood, bot abused, among others.\nüí° How It Works\n\nBy deploying a WAF in front of a web application, a shield is placed between the web application and the Internet. While a proxy server protects a client machine‚Äôs identity by using an intermediary, a WAF is a type of reverse-proxy, protecting the server from exposure by having clients pass through the WAF before reaching the server.\nA WAF protects your web apps by filtering, monitoring, and blocking any malicious HTTP/S traffic traveling to the web application, and prevents any unauthorized data from leaving the app. It does this by adhering to a set of policies that help determine what traffic is malicious and what traffic is safe. Just as a proxy server acts as an intermediary to protect the identity of a client, a WAF operates in similar fashion but acting as a reverse proxy intermediary that protects the web app server from a potentially malicious client.\nits core capabilities include:\n\nDefenses for web attacks\nProactive bot abused defense\nHTML & JS code encryption\nIP-based rate limiting\nWeb Access Control List\n\n‚ö°Ô∏è Screenshots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGet Live Demo\nüî• FEATURES\nList of the main features as follows:\n\nBlock Web Attacks\n\nIt defenses for all of web attacks, such as SQL injection, XSS, code injection, os command injection, CRLF injection, XXE, SSRF, path traversal and so on.\n\n\nRate Limiting\n\nDefend your web apps against DoS attacks, bruteforce attempts, traffic surges, and other types of abuse by throttling traffic that exceeds defined limits.\n\n\nAnti-Bot Challenge\n\nAnti-Bot challenges to protect your website from bot attacks, humen users will be allowed, crawlers and bots will be blocked.\n\n\nAuthentication Challenge\n\nWhen authentication challenge turned on, visitors need to enter the password, otherwise they will be blocked.\n\n\nDynamic Protection\n\nWhen dynamic protection turned on, html and js codes in your web server will be dynamically encrypted by each time you visit.\n\n\n\nüß© Showcases\n\n\n\n\nLegitimate User\nMalicious User\n\n\n\n\nBlock Web Attacks\n\n\n\n\nRate Limiting\n\n\n\n\nAnti-Bot Challenge\n\n\n\n\nAuth Challenge\n\n\n\n\nHTML Dynamic Protection\n\n\n\n\nJS Dynamic Protection\n\n\n\n\n\nüöÄ Quickstart\nWarning‰∏≠ÂõΩÂ§ßÈôÜÁî®Êà∑ÂÆâË£ÖÂõΩÈôÖÁâàÂèØËÉΩ‰ºöÂØºËá¥Êó†Ê≥ïËøûÊé•‰∫ëÊúçÂä°ÔºåËØ∑Êü•Áúã ‰∏≠ÊñáÁâàÂÆâË£ÖÊñáÊ°£\n\nüì¶ Installing\nInformation on how to install SafeLine can be found in the Install Guide\n‚öôÔ∏è Protecting Web Apps\nto see Configuration\nüìã More Informations\nEffect Evaluation\n\n\n\nMetric\nModSecurity, Level 1\nCloudFlare, Free\nSafeLine, Balance\nSafeLine, Strict\n\n\n\n\nTotal Samples\n33669\n33669\n33669\n33669\n\n\nDetection\n69.74%\n10.70%\n71.65%\n76.17%\n\n\nFalse Positive\n17.58%\n0.07%\n0.07%\n0.22%\n\n\nAccuracy\n82.20%\n98.40%\n99.45%\n99.38%\n\n\n\nIs SafeLine Production-Ready?\nYes, SafeLine is production-ready.\n\nOver 180,000 installations worldwide\nProtecting over 1,000,000 Websites\nHandling over 30,000,000,000 HTTP Requests Daily\n\nüôã‚Äç‚ôÇÔ∏è Community\nJoin our Discord to get community support, the core team members are identified by the STAFF role in Discord.\n\nchannel #feedback: for new features discussion.\nchannel #FAQ: for FAQ.\nchannel #general: for any other questions.\n\nSeveral contact options exist for our community, the primary one being Discord. These are in addition to GitHub issues for creating a new issue.\n\n ¬†\n   ¬†\n  \n\nüí™ PRO Edition\nComing soon!\nüìù License\nSee LICENSE for details.",
      "languages": {
        "SQL": 1,
        "HTML": 1
      },
      "topics": [
        "SQL"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:49.553648"
    },
    {
      "owner": "keploy",
      "name": "keploy",
      "url": "https://github.com/keploy/keploy",
      "description": "Unit and Integration Test generation for Developers. Generate tests and stubs for your application that actually work!",
      "readme_content": "‚ö°Ô∏è API tests faster than unit tests, from user traffic ‚ö°Ô∏è\n\n\n\nüåü The must-have tool for developers in the AI-Gen era üåü\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKeploy is developer-centric API testing tool that creates tests along with built-in-mocks, faster than unit tests.\nKeploy not only records API calls, but also records database calls and replays them during testing, making it easy to use, powerful, and extensible.\n\n\nüê∞ Fun fact: Keploy uses itself for testing! Check out our swanky coverage badge:  ¬†\n\nüö® Here for  Unit Test Generator (ut-gen)?\nKeploy has newly launched the world's first unit test generator(ut-gen) implementation of Meta LLM research paper, it understands code semantics and generates meaningful unit tests, aiming to:\n\n\nAutomate unit test generation (UTG): Quickly generate comprehensive unit tests and reduce redundant manual effort.\n\n\nImprove edge cases: Extend and improve the scope of automated tests to cover more complex scenarios, often missed manually.\n\n\nBoost test coverage: As codebases grow, ensuring exhaustive coverage should become feasible, aligning with our mission.\n\n\nüìú Follow Unit Test Generator README! ‚úÖ\nüìò Documentation!\nBecome a Keploy pro with Keploy Documentation.\n\nüöÄ Quick Installation (API test generator)\nIntegrate Keploy by installing the agent locally. No code-changes required.\ncurl --silent -O -L https://keploy.io/install.sh && source install.sh\nüé¨ Recording Testcases\nStart your app with Keploy to convert API calls as Tests and Mocks/Stubs.\nkeploy record -c \"CMD_TO_RUN_APP\" \nFor example, if you're using a simple Python app the CMD_TO_RUN_APP would resemble to python main.py, for  Golang go run main.go, for java java -jar xyz.jar, for node npm start..\nkeploy record -c \"python main.py\"\nüß™ Running Tests\nShut down the databases, redis, kafka or any other services your application uses. Keploy doesn't need those during test.\nkeploy test -c \"CMD_TO_RUN_APP\" --delay 10\n‚úÖ Test Coverage Integration\nTo integrate with your unit-testing library and see combine test coverage, follow this test-coverage guide.\n\nIf You Had Fun: Please leave a üåü star on this repo! It's free and will bring a smile. üòÑ üëè\n\nOne-Click Setup üöÄ\nSetup and run keploy quickly, with no local machine installation required:\n\nü§î Questions?\nReach out to us. We're here to help!\n\n\n\n\nüåê Language Support\nFrom Go's gopher üêπ to Python's snake üêç, we support:\n\n\n\n\n\n\nü´∞ Keploy Adopters üß°\nSo you and your organisation are using Keploy? That‚Äôs great. Please add yourselves to this list, and we'll send you goodies! üíñ\nWe are happy and proud to have you all as part of our community! üíñ\nüé© How's the Magic Happen?\nKeploy proxy captures and replays ALL (CRUD operations, including non-idempotent APIs) of your app's network interactions.\nTake a journey to How Keploy Works? to discover the tricks behind the curtain!\nHere are Keploy's core features: üõ†\n\n\n‚ôªÔ∏è Combined Test Coverage: Merge your Keploy Tests with your fave testing libraries(JUnit, go-test, py-test, jest) to see a combined test coverage.\n\n\nü§ñ EBPF Instrumentation: Keploy uses EBPF like a secret sauce to make integration code-less, language-agnostic, and oh-so-lightweight.\n\n\nüåê CI/CD Integration: Run tests with mocks anywhere you like‚Äîlocally on the CLI, in your CI pipeline (Jenkins, Github Actions..) , or even across a Kubernetes cluster.\n\n\nüìΩÔ∏è Record-Replay Complex Flows: Keploy can record and replay complex, distributed API flows as mocks and stubs. It's like having a time machine for your tests‚Äîsaving you tons of time!\n\n\nüé≠ Multi-Purpose Mocks: You can also use keploy Mocks, as server Tests!\n\n\nüë®üèª‚Äçüíª Let's Build Together! üë©üèª‚Äçüíª\nWhether you're a newbie coder or a wizard üßô‚Äç‚ôÄÔ∏è, your perspective is golden. Take a peek at our:\nüìú Contribution Guidelines\n‚ù§Ô∏è Code of Conduct\nüê≤ Current Limitations!\n\nUnit Testing: While Keploy is designed to run alongside unit testing frameworks (Go test, JUnit..) and can add to the overall code coverage, it still generates integration tests.\nProduction Lands: Keploy is currently focused on generating tests for developers. These tests can be captured from any environment, but we have not tested it on high volume production environments. This would need robust deduplication to avoid too many redundant tests being captured. We do have ideas on building a robust deduplication system #27\n\n‚ú® Resources!\nü§î FAQs\nüïµÔ∏è‚ÄçÔ∏è Why Keploy\n‚öôÔ∏è Installation Guide\nüìñ Contribution Guide",
      "languages": {
        "python": 1,
        "Python": 1,
        "go": 1,
        "GO": 1,
        "Go": 1,
        "java": 1,
        "Java": 1
      },
      "topics": [
        "Python",
        "Redis",
        "Kubernetes",
        "Go",
        "Java"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:50.278157"
    },
    {
      "owner": "k8sgpt-ai",
      "name": "k8sgpt",
      "url": "https://github.com/k8sgpt-ai/k8sgpt",
      "description": "Giving Kubernetes Superpowers to everyone",
      "readme_content": "k8sgpt is a tool for scanning your Kubernetes clusters, diagnosing, and triaging issues in simple English.\nIt has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.\nOut of the box integration with OpenAI, Azure, Cohere, Amazon Bedrock, Google Gemini and local models.\n \n\nCLI Installation\nLinux/Mac via brew\n$ brew install k8sgpt\nor\nbrew tap k8sgpt-ai/k8sgpt\nbrew install k8sgpt\n\nRPM-based installation (RedHat/CentOS/Fedora)\n32 bit:\nsudo rpm -ivh https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.49/k8sgpt_386.rpm\n\n64 bit:\nsudo rpm -ivh https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.49/k8sgpt_amd64.rpm\n\n\n\nDEB-based installation (Ubuntu/Debian)\n32 bit:\ncurl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.49/k8sgpt_386.deb\nsudo dpkg -i k8sgpt_386.deb\n\n64 bit:\ncurl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.49/k8sgpt_amd64.deb\nsudo dpkg -i k8sgpt_amd64.deb\n\n\n\nAPK-based installation (Alpine)\n32 bit:\nwget https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.49/k8sgpt_386.apk\napk add --allow-untrusted k8sgpt_386.apk\n\n64 bit:\nwget https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.49/k8sgpt_amd64.apk\napk add --allow-untrusted k8sgpt_amd64.apk\n\n\n\nFailing Installation on WSL or Linux (missing gcc)\n  When installing Homebrew on WSL or Linux, you may encounter the following error:\n==> Installing k8sgpt from k8sgpt-ai/k8sgpt Error: The following formula cannot be installed from a bottle and must be\nbuilt from the source. k8sgpt Install Clang or run brew install gcc.\n\nIf you install gcc as suggested, the problem will persist. Therefore, you need to install the build-essential package.\n   sudo apt-get update\n   sudo apt-get install build-essential\n\n\nWindows\n\nDownload the latest Windows binaries of k8sgpt from the Release\ntab based on your system architecture.\nExtract the downloaded package to your desired location. Configure the system path variable with the binary location\n\nOperator Installation\nTo install within a Kubernetes cluster please use our k8sgpt-operator with installation instructions available here\nThis mode of operation is ideal for continuous monitoring of your cluster and can integrate with your existing monitoring such as Prometheus and Alertmanager.\nQuick Start\n\nCurrently, the default AI provider is OpenAI, you will need to generate an API key from OpenAI\n\nYou can do this by running k8sgpt generate to open a browser link to generate it\n\n\nRun k8sgpt auth add to set it in k8sgpt.\n\nYou can provide the password directly using the --password flag.\n\n\nRun k8sgpt filters to manage the active filters used by the analyzer. By default, all filters are executed during analysis.\nRun k8sgpt analyze to run a scan.\nAnd use k8sgpt analyze --explain to get a more detailed explanation of the issues.\nYou also run k8sgpt analyze --with-doc (with or without the explain flag) to get the official documentation from Kubernetes.\n\nAnalyzers\nK8sGPT uses analyzers to triage and diagnose issues in your cluster. It has a set of analyzers that are built in, but\nyou will be able to write your own analyzers.\nBuilt in analyzers\nEnabled by default\n\n podAnalyzer\n pvcAnalyzer\n rsAnalyzer\n serviceAnalyzer\n eventAnalyzer\n ingressAnalyzer\n statefulSetAnalyzer\n deploymentAnalyzer\n cronJobAnalyzer\n nodeAnalyzer\n mutatingWebhookAnalyzer\n validatingWebhookAnalyzer\n\nOptional\n\n hpaAnalyzer\n pdbAnalyzer\n networkPolicyAnalyzer\n gatewayClass\n gateway\n httproute\n logAnalyzer\n\nExamples\nRun a scan with the default analyzers\nk8sgpt generate\nk8sgpt auth add\nk8sgpt analyze --explain\nk8sgpt analyze --explain --with-doc\n\nFilter on resource\nk8sgpt analyze --explain --filter=Service\n\nFilter by namespace\nk8sgpt analyze --explain --filter=Pod --namespace=default\n\nOutput to JSON\nk8sgpt analyze --explain --filter=Service --output=json\n\nAnonymize during explain\nk8sgpt analyze --explain --filter=Service --output=json --anonymize\n\n\n Using filters \nList filters\nk8sgpt filters list\n\nAdd default filters\nk8sgpt filters add [filter(s)]\n\nExamples :\n\nSimple filter : k8sgpt filters add Service\nMultiple filters : k8sgpt filters add Ingress,Pod\n\nRemove default filters\nk8sgpt filters remove [filter(s)]\n\nExamples :\n\nSimple filter : k8sgpt filters remove Service\nMultiple filters : k8sgpt filters remove Ingress,Pod\n\n\n\n Additional commands \nList configured backends\nk8sgpt auth list\n\nUpdate configured backends\nk8sgpt auth update $MY_BACKEND1,$MY_BACKEND2..\n\nRemove configured backends\nk8sgpt auth remove -b $MY_BACKEND1,$MY_BACKEND2..\n\nList integrations\nk8sgpt integrations list\n\nActivate integrations\nk8sgpt integrations activate [integration(s)]\n\nUse integration\nk8sgpt analyze --filter=[integration(s)]\n\nDeactivate integrations\nk8sgpt integrations deactivate [integration(s)]\n\nServe mode\nk8sgpt serve\n\nAnalysis with serve mode\ngrpcurl -plaintext -d '{\"namespace\": \"k8sgpt\", \"explain\" : \"true\"}' localhost:8080 schema.v1.ServerAnalyzerService/Analyze\n{\n  \"status\": \"OK\"\n}\n\nAnalysis with custom headers\nk8sgpt analyze --explain --custom-headers CustomHeaderKey:CustomHeaderValue\n\nPrint analysis stats\nk8sgpt analyze -s\nThe stats mode allows for debugging and understanding the time taken by an analysis by displaying the statistics of each analyzer.\n- Analyzer Ingress took 47.125583ms \n- Analyzer PersistentVolumeClaim took 53.009167ms \n- Analyzer CronJob took 57.517792ms \n- Analyzer Deployment took 156.6205ms \n- Analyzer Node took 160.109833ms \n- Analyzer ReplicaSet took 245.938333ms \n- Analyzer StatefulSet took 448.0455ms \n- Analyzer Pod took 5.662594708s \n- Analyzer Service took 38.583359166s\n\nDiagnostic information\nTo collect diagnostic information use the following command to create a dump_<timestamp>_json in your local directory.\nk8sgpt dump\n\n\nLLM AI Backends\nK8sGPT uses the chosen LLM, generative AI provider when you want to explain the analysis results using --explain flag e.g. k8sgpt analyze --explain. You can use --backend flag to specify a configured provider (it's openai by default).\nYou can list available providers using k8sgpt auth list:\nDefault:\n> openai\nActive:\nUnused:\n> openai\n> localai\n> ollama\n> azureopenai\n> cohere\n> amazonbedrock\n> amazonsagemaker\n> google\n> huggingface\n> noopai\n> googlevertexai\n> ibmwatsonxai\n\nFor detailed documentation on how to configure and use each provider see here.\nTo set a new default provider\nk8sgpt auth default -p azureopenai\nDefault provider set to azureopenai\n\nKey Features\n\nWith this option, the data is anonymized before being sent to the AI Backend. During the analysis execution, k8sgpt retrieves sensitive data (Kubernetes object names, labels, etc.). This data is masked when sent to the AI backend and replaced by a key that can be used to de-anonymize the data when the solution is returned to the user.\n Anonymization \n\nError reported during analysis:\n\nError: HorizontalPodAutoscaler uses StatefulSet/fake-deployment as ScaleTargetRef which does not exist.\n\nPayload sent to the AI backend:\n\nError: HorizontalPodAutoscaler uses StatefulSet/tGLcCRcHa1Ce5Rs as ScaleTargetRef which does not exist.\n\nPayload returned by the AI:\n\nThe Kubernetes system is trying to scale a StatefulSet named tGLcCRcHa1Ce5Rs using the HorizontalPodAutoscaler, but it cannot find the StatefulSet. The solution is to verify that the StatefulSet name is spelled correctly and exists in the same namespace as the HorizontalPodAutoscaler.\n\nPayload returned to the user:\n\nThe Kubernetes system is trying to scale a StatefulSet named fake-deployment using the HorizontalPodAutoscaler, but it cannot find the StatefulSet. The solution is to verify that the StatefulSet name is spelled correctly and exists in the same namespace as the HorizontalPodAutoscaler.\nNote: Anonymization does not currently apply to events.\nFurther Details\nAnonymization does not currently apply to events.\nIn a few analysers like Pod, we feed to the AI backend the event messages which are not known beforehand thus we are not masking them for the time being.\n\n\nThe following is the list of analysers in which data is being masked:-\n\nStatefulset\nService\nPodDisruptionBudget\nNode\nNetworkPolicy\nIngress\nHPA\nDeployment\nCronjob\n\n\n\nThe following is the list of analysers in which data is not being masked:-\n\nRepicaSet\nPersistentVolumeClaim\nPod\nLog\n*Events\n\n\n\n*Note:\n\n\nk8gpt will not mask the above analysers because they do not send any identifying information except Events analyser.\n\n\nMasking for Events analyzer is scheduled in the near future as seen in this issue. Further research has to be made to understand the patterns and be able to mask the sensitive parts of an event like pod name, namespace etc.\n\n\nThe following is the list of fields which are not being masked:-\n\nDescribe\nObjectStatus\nReplicas\nContainerStatus\n*Event Message\nReplicaStatus\nCount (Pod)\n\n\n\n*Note:\n\nIt is quite possible the payload of the event message might have something like \"super-secret-project-pod-X crashed\" which we don't currently redact (scheduled in the near future as seen in this issue).\n\nProceed with care\n\nThe K8gpt team recommends using an entirely different backend (a local model) in critical production environments. By using a local model, you can rest assured that everything stays within your DMZ, and nothing is leaked.\nIf there is any uncertainty about the possibility of sending data to a public LLM (open AI, Azure AI) and it poses a risk to business-critical operations, then, in such cases, the use of public LLM should be avoided based on personal assessment and the jurisdiction of risks involved.\n\n\n\n Configuration management\nk8sgpt stores config data in the $XDG_CONFIG_HOME/k8sgpt/k8sgpt.yaml file. The data is stored in plain text, including your OpenAI key.\nConfig file locations:\n\n\n\nOS\nPath\n\n\n\n\nMacOS\n~/Library/Application Support/k8sgpt/k8sgpt.yaml\n\n\nLinux\n~/.config/k8sgpt/k8sgpt.yaml\n\n\nWindows\n%LOCALAPPDATA%/k8sgpt/k8sgpt.yaml\n\n\n\n\n\nThere may be scenarios where caching remotely is preferred.\nIn these scenarios K8sGPT supports AWS S3 or Azure Blob storage Integration.\n Remote caching \nNote: You can only configure and use only one remote cache at a time\nAdding a remote cache\n\nAWS S3\n\nAs a prerequisite AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are required as environmental variables.\nConfiguration, k8sgpt cache add s3 --region <aws region> --bucket <name>\nMinio Configuration with HTTP endpoint  k8sgpt cache add s3 --bucket <name> --endpoint <http://localhost:9000>\nMinio Configuration with HTTPs endpoint, skipping TLS verification  k8sgpt cache add s3 --bucket <name> --endpoint <https://localhost:9000> --insecure\n\nK8sGPT will create the bucket if it does not exist\n\n\n\n\nAzure Storage\n\nWe support a number of techniques to authenticate against Azure\nConfiguration, k8sgpt cache add azure --storageacc <storage account name> --container <container name>\n\nK8sGPT assumes that the storage account already exist and it will create the container if it does not exist\nIt is the user responsibility have to grant specific permissions to their identity in order to be able to upload blob files and create SA containers (e.g Storage Blob Data Contributor)\n\n\n\n\nGoogle Cloud Storage\n\nAs a prerequisite GOOGLE_APPLICATION_CREDENTIALS are required as environmental variables.\nConfiguration,  k8sgpt cache add gcs --region <gcp region> --bucket <name> --projectid <project id>\n\nK8sGPT will create the bucket if it does not exist\n\n\n\n\n\nListing cache items\nk8sgpt cache list\n\nPurging an object from the cache\nNote: purging an object using this command will delete upstream files, so it requires appropriate permissions.\nk8sgpt cache purge $OBJECT_NAME\n\nRemoving the remote cache\nNote: this will not delete the upstream S3 bucket or Azure storage container\nk8sgpt cache remove\n\n\n\n Custom Analyzers\nThere may be scenarios where you wish to write your own analyzer in a language of your choice.\nK8sGPT now supports the ability to do so by abiding by the schema and serving the analyzer for consumption.\nTo do so, define the analyzer within the K8sGPT configuration and it will add it into the scanning process.\nIn addition to this you will need to enable the following flag on analysis:\nk8sgpt analyze --custom-analysis\n\nHere is an example local host analyzer in Rust\nWhen this is run on localhost:8080 the K8sGPT config can pick it up with the following additions:\ncustom_analyzers:\n  - name: host-analyzer\n    connection:\n      url: localhost\n      port: 8080\n\nThis now gives the ability to pass through hostOS information ( from this analyzer example ) to K8sGPT to use as context with normal analysis.\nSee the docs on how to write a custom analyzer\nListing custom analyzers configured\nk8sgpt custom-analyzer list\n\nAdding custom analyzer without install\nk8sgpt custom-analyzer add --name my-custom-analyzer --port 8085\n\nRemoving custom analyzer\nk8sgpt custom-analyzer remove --names \"my-custom-analyzer,my-custom-analyzer-2\"\n\n\nDocumentation\nFind our official documentation available here\nContributing\nPlease read our contributing guide.\nCommunity\nFind us on Slack\n\n\n\nLicense",
      "languages": {
        "rust": 1,
        "Rust": 1
      },
      "topics": [
        "Rust",
        "Azure",
        "AWS",
        "Kubernetes"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:51.009545"
    },
    {
      "owner": "gitleaks",
      "name": "gitleaks",
      "url": "https://github.com/gitleaks/gitleaks",
      "description": "Find secrets with Gitleaks üîë",
      "readme_content": "Gitleaks\n‚îå‚îÄ‚óã‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ ‚îÇ‚ï≤  ‚îÇ\n‚îÇ ‚îÇ ‚óã ‚îÇ\n‚îÇ ‚óã ‚ñë ‚îÇ\n‚îî‚îÄ‚ñë‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJoin our Discord! \nGitleaks is a tool for detecting secrets like passwords, API keys, and tokens in git repos, files, and whatever else you wanna throw at it via stdin.\n‚ûú  ~/code(master) gitleaks git -v\n\n    ‚óã\n    ‚îÇ‚ï≤\n    ‚îÇ ‚óã\n    ‚óã ‚ñë\n    ‚ñë    gitleaks\n\n\nFinding:     \"export BUNDLE_ENTERPRISE__CONTRIBSYS__COM=cafebabe:deadbeef\",\nSecret:      cafebabe:deadbeef\nRuleID:      sidekiq-secret\nEntropy:     2.609850\nFile:        cmd/generate/config/rules/sidekiq.go\nLine:        23\nCommit:      cd5226711335c68be1e720b318b7bc3135a30eb2\nAuthor:      John\nEmail:       john@users.noreply.github.com\nDate:        2022-08-03T12:31:40Z\nFingerprint: cd5226711335c68be1e720b318b7bc3135a30eb2:cmd/generate/config/rules/sidekiq.go:sidekiq-secret:23\n\nGetting Started\nGitleaks can be installed using Homebrew, Docker, or Go. Gitleaks is also available in binary form for many popular platforms and OS types on the releases page. In addition, Gitleaks can be implemented as a pre-commit hook directly in your repo or as a GitHub action using Gitleaks-Action.\nInstalling\n# MacOS\nbrew install gitleaks\n\n# Docker (DockerHub)\ndocker pull zricethezav/gitleaks:latest\ndocker run -v ${path_to_host_folder_to_scan}:/path zricethezav/gitleaks:latest [COMMAND] [OPTIONS] [SOURCE_PATH]\n\n# Docker (ghcr.io)\ndocker pull ghcr.io/gitleaks/gitleaks:latest\ndocker run -v ${path_to_host_folder_to_scan}:/path ghcr.io/gitleaks/gitleaks:latest [COMMAND] [OPTIONS] [SOURCE_PATH]\n\n# From Source (make sure `go` is installed)\ngit clone https://github.com/gitleaks/gitleaks.git\ncd gitleaks\nmake build\nGitHub Action\nCheck out the official Gitleaks GitHub Action\nname: gitleaks\non: [pull_request, push, workflow_dispatch]\njobs:\n  scan:\n    name: gitleaks\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n      - uses: gitleaks/gitleaks-action@v2\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          GITLEAKS_LICENSE: ${{ secrets.GITLEAKS_LICENSE}} # Only required for Organizations, not personal accounts.\n\nPre-Commit\n\n\nInstall pre-commit from https://pre-commit.com/#install\n\n\nCreate a .pre-commit-config.yaml file at the root of your repository with the following content:\nrepos:\n  - repo: https://github.com/gitleaks/gitleaks\n    rev: v8.23.1\n    hooks:\n      - id: gitleaks\n\nfor a native execution of GitLeaks or use the gitleaks-docker pre-commit ID for executing GitLeaks using the official Docker images\n\n\nAuto-update the config to the latest repos' versions by executing pre-commit autoupdate\n\n\nInstall with pre-commit install\n\n\nNow you're all set!\n\n\n‚ûú git commit -m \"this commit contains a secret\"\nDetect hardcoded secrets.................................................Failed\n\nNote: to disable the gitleaks pre-commit hook you can prepend SKIP=gitleaks to the commit command\nand it will skip running gitleaks\n‚ûú SKIP=gitleaks git commit -m \"skip gitleaks check\"\nDetect hardcoded secrets................................................Skipped\n\nUsage\nUsage:\n  gitleaks [command]\n\nAvailable Commands:\n  completion  generate the autocompletion script for the specified shell\n  dir         scan directories or files for secrets\n  git         scan git repositories for secrets\n  help        Help about any command\n  stdin       detect secrets from stdin\n  version     display gitleaks version\n\nFlags:\n  -b, --baseline-path string          path to baseline with issues that can be ignored\n  -c, --config string                 config file path\n                                      order of precedence:\n                                      1. --config/-c\n                                      2. env var GITLEAKS_CONFIG\n                                      3. (target path)/.gitleaks.toml\n                                      If none of the three options are used, then gitleaks will use the default config\n      --enable-rule strings           only enable specific rules by id\n      --exit-code int                 exit code when leaks have been encountered (default 1)\n  -i, --gitleaks-ignore-path string   path to .gitleaksignore file or folder containing one (default \".\")\n  -h, --help                          help for gitleaks\n      --ignore-gitleaks-allow         ignore gitleaks:allow comments\n  -l, --log-level string              log level (trace, debug, info, warn, error, fatal) (default \"info\")\n      --max-decode-depth int          allow recursive decoding up to this depth (default \"0\", no decoding is done)\n      --max-target-megabytes int      files larger than this will be skipped\n      --no-banner                     suppress banner\n      --no-color                      turn off color for verbose output\n      --redact uint[=100]             redact secrets from logs and stdout. To redact only parts of the secret just apply a percent value from 0..100. For example --redact=20 (default 100%)\n  -f, --report-format string          output format (json, csv, junit, sarif) (default \"json\")\n  -r, --report-path string            report file\n      --report-template string        template file used to generate the report (implies --report-format=template)\n  -v, --verbose                       show verbose output from scan\n      --version                       version for gitleaks\n\nUse \"gitleaks [command] --help\" for more information about a command.\n\nCommands\n‚ö†Ô∏è v8.19.0 introduced a change that deprecated detect and protect. Those commands are still available but\nare hidden in the --help menu. Take a look at this gist for easy command translations.\nIf you find v8.19.0 broke an existing command (detect/protect), please open an issue.\nThere are three scanning modes: git, dir, and stdin.\nGit\nThe git command lets you scan local git repos. Under the hood, gitleaks uses the git log -p command to scan patches.\nYou can configure the behavior of git log -p with the log-opts option.\nFor example, if you wanted to run gitleaks on a range of commits you could use the following\ncommand: gitleaks git -v --log-opts=\"--all commitA..commitB\" path_to_repo. See the git log documentation for more information.\nIf there is no target specified as a positional argument, then gitleaks will attempt to scan the current working directory as a git repo.\nDir\nThe dir (aliases include files, directory) command lets you scan directories and files. Example: gitleaks dir -v path_to_directory_or_file.\nIf there is no target specified as a positional argument, then gitleaks will scan the current working directory.\nStdin\nYou can also stream data to gitleaks with the stdin command. Example: cat some_file | gitleaks -v stdin\nCreating a baseline\nWhen scanning large repositories or repositories with a long history, it can be convenient to use a baseline. When using a baseline,\ngitleaks will ignore any old findings that are present in the baseline. A baseline can be any gitleaks report. To create a gitleaks report, run gitleaks with the --report-path parameter.\ngitleaks git --report-path gitleaks-report.json # This will save the report in a file called gitleaks-report.json\n\nOnce as baseline is created it can be applied when running the detect command again:\ngitleaks git --baseline-path gitleaks-report.json --report-path findings.json\n\nAfter running the detect command with the --baseline-path parameter, report output (findings.json) will only contain new issues.\nPre-Commit hook\nYou can run Gitleaks as a pre-commit hook by copying the example pre-commit.py script into\nyour .git/hooks/ directory.\nConfiguration\nGitleaks offers a configuration format you can follow to write your own secret detection rules:\n# Title for the gitleaks configuration file.\ntitle = \"Custom Gitleaks configuration\"\n\n# You have basically two options for your custom configuration:\n#\n# 1. define your own configuration, default rules do not apply\n#\n#    use e.g., the default configuration as starting point:\n#    https://github.com/gitleaks/gitleaks/blob/master/config/gitleaks.toml\n#\n# 2. extend a configuration, the rules are overwritten or extended\n#\n#    When you extend a configuration the extended rules take precedence over the\n#    default rules. I.e., if there are duplicate rules in both the extended\n#    configuration and the default configuration the extended rules or\n#    attributes of them will override the default rules.\n#    Another thing to know with extending configurations is you can chain\n#    together multiple configuration files to a depth of 2. Allowlist arrays are\n#    appended and can contain duplicates.\n\n# useDefault and path can NOT be used at the same time. Choose one.\n[extend]\n# useDefault will extend the default gitleaks config built in to the binary\n# the latest version is located at:\n# https://github.com/gitleaks/gitleaks/blob/master/config/gitleaks.toml\nuseDefault = true\n# or you can provide a path to a configuration to extend from.\n# The path is relative to where gitleaks was invoked,\n# not the location of the base config.\n# path = \"common_config.toml\"\n# If there are any rules you don't want to inherit, they can be specified here.\ndisabledRules = [ \"generic-api-key\"]\n\n# An array of tables that contain information that define instructions\n# on how to detect secrets\n[[rules]]\n\n# Unique identifier for this rule\nid = \"awesome-rule-1\"\n\n# Short human readable description of the rule.\ndescription = \"awesome rule 1\"\n\n# Golang regular expression used to detect secrets. Note Golang's regex engine\n# does not support lookaheads.\nregex = '''one-go-style-regex-for-this-rule'''\n\n# Int used to extract secret from regex match and used as the group that will have\n# its entropy checked if `entropy` is set.\nsecretGroup = 3\n\n# Float representing the minimum shannon entropy a regex group must have to be considered a secret.\nentropy = 3.5\n\n# Golang regular expression used to match paths. This can be used as a standalone rule or it can be used\n# in conjunction with a valid `regex` entry.\npath = '''a-file-path-regex'''\n\n# Keywords are used for pre-regex check filtering. Rules that contain\n# keywords will perform a quick string compare check to make sure the\n# keyword(s) are in the content being scanned. Ideally these values should\n# either be part of the identiifer or unique strings specific to the rule's regex\n# (introduced in v8.6.0)\nkeywords = [\n  \"auth\",\n  \"password\",\n  \"token\",\n]\n\n# Array of strings used for metadata and reporting purposes.\ntags = [\"tag\",\"another tag\"]\n\n    # ‚ö†Ô∏è In v8.21.0 `[rules.allowlist]` was replaced with `[[rules.allowlists]]`.\n    # This change was backwards-compatible: instances of `[rules.allowlist]` still  work.\n    #\n    # You can define multiple allowlists for a rule to reduce false positives.\n    # A finding will be ignored if _ANY_ `[[rules.allowlists]]` matches.\n    [[rules.allowlists]]\n    description = \"ignore commit A\"\n    # When multiple criteria are defined the default condition is \"OR\".\n    # e.g., this can match on |commits| OR |paths| OR |stopwords|.\n    condition = \"OR\"\n    commits = [ \"commit-A\", \"commit-B\"]\n    paths = [\n      '''go\\.mod''',\n      '''go\\.sum'''\n    ]\n    # note: stopwords targets the extracted secret, not the entire regex match\n    # like 'regexes' does. (stopwords introduced in 8.8.0)\n    stopwords = [\n      '''client''',\n      '''endpoint''',\n    ]\n\n    [[rules.allowlists]]\n    # The \"AND\" condition can be used to make sure all criteria match.\n    # e.g., this matches if |regexes| AND |paths| are satisfied.\n    condition = \"AND\"\n    # note: |regexes| defaults to check the _Secret_ in the finding.\n    # Acceptable values for |regexTarget| are \"secret\" (default), \"match\", and \"line\".\n    regexTarget = \"match\"\n    regexes = [ '''(?i)parseur[il]''' ]\n    paths = [ '''package-lock\\.json''' ]\n\n# You can extend a particular rule from the default config. e.g., gitlab-pat\n# if you have defined a custom token prefix on your GitLab instance\n[[rules]]\nid = \"gitlab-pat\"\n# all the other attributes from the default rule are inherited\n\n    [[rules.allowlists]]\n    regexTarget = \"line\"\n    regexes = [ '''MY-glpat-''' ]\n\n# This is a global allowlist which has a higher order of precedence than rule-specific allowlists.\n# If a commit listed in the `commits` field below is encountered then that commit will be skipped and no\n# secrets will be detected for said commit. The same logic applies for regexes and paths.\n[allowlist]\ndescription = \"global allow list\"\ncommits = [ \"commit-A\", \"commit-B\", \"commit-C\"]\npaths = [\n  '''gitleaks\\.toml''',\n  '''(.*?)(jpg|gif|doc)'''\n]\n\n# note: (global) regexTarget defaults to check the _Secret_ in the finding.\n# if regexTarget is not specified then _Secret_ will be used.\n# Acceptable values for regexTarget are \"match\" and \"line\"\nregexTarget = \"match\"\nregexes = [\n  '''219-09-9999''',\n  '''078-05-1120''',\n  '''(9[0-9]{2}|666)-\\d{2}-\\d{4}''',\n]\n# note: stopwords targets the extracted secret, not the entire regex match\n# like 'regexes' does. (stopwords introduced in 8.8.0)\nstopwords = [\n  '''client''',\n  '''endpoint''',\n]\nRefer to the default gitleaks config for examples or follow the contributing guidelines if you would like to contribute to the default configuration. Additionally, you can check out this gitleaks blog post which covers advanced configuration setups.\nAdditional Configuration\ngitleaks:allow\nIf you are knowingly committing a test secret that gitleaks will catch you can add a gitleaks:allow comment to that line which will instruct gitleaks\nto ignore that secret. Ex:\nclass CustomClass:\n    discord_client_secret = '8dyfuiRyq=vVc3RRr_edRk-fK__JItpZ'  #gitleaks:allow\n\n\n.gitleaksignore\nYou can ignore specific findings by creating a .gitleaksignore file at the root of your repo. In release v8.10.0 Gitleaks added a Fingerprint value to the Gitleaks report. Each leak, or finding, has a Fingerprint that uniquely identifies a secret. Add this fingerprint to the .gitleaksignore file to ignore that specific secret. See Gitleaks' .gitleaksignore for an example. Note: this feature is experimental and is subject to change in the future.\nDecoding\nSometimes secrets are encoded in a way that can make them difficult to find\nwith just regex. Now you can tell gitleaks to automatically find and decode\nencoded text. The flag --max-decode-depth enables this feature (the default\nvalue \"0\" means the feature is disabled by default).\nRecursive decoding is supported since decoded text can also contain encoded\ntext.  The flag --max-decode-depth sets the recursion limit. Recursion stops\nwhen there are no new segments of encoded text to decode, so setting a really\nhigh max depth doesn't mean it will make that many passes. It will only make as\nmany as it needs to decode the text. Overall, decoding only minimally increases\nscan times.\nThe findings for encoded text differ from normal findings in the following\nways:\n\nThe location points the bounds of the encoded text\n\nIf the rule matches outside the encoded text, the bounds are adjusted to\ninclude that as well\n\n\nThe match and secret contain the decoded value\nTwo tags are added decoded:<encoding> and decode-depth:<depth>\n\nCurrently supported encodings:\n\nbase64 (both standard and base64url)\n\nReporting\nGitleaks has built-in support for several report formats: json, csv, junit, and sarif.\nIf none of these formats fit your need, you can create your own report format with a Go text/template .tmpl file and the --report-template flag. The template can use extended functionality from the Masterminds/sprig template library.\nFor example, the following template provides a custom JSON output:\n# jsonextra.tmpl\n[{{ $lastFinding := (sub (len . ) 1) }}\n{{- range $i, $finding := . }}{{with $finding}}\n    {\n        \"Description\": {{ quote .Description }},\n        \"StartLine\": {{ .StartLine }},\n        \"EndLine\": {{ .EndLine }},\n        \"StartColumn\": {{ .StartColumn }},\n        \"EndColumn\": {{ .EndColumn }},\n        \"Line\": {{ quote .Line }},\n        \"Match\": {{ quote .Match }},\n        \"Secret\": {{ quote .Secret }},\n        \"File\": \"{{ .File }}\",\n        \"SymlinkFile\": {{ quote .SymlinkFile }},\n        \"Commit\": {{ quote .Commit }},\n        \"Entropy\": {{ .Entropy }},\n        \"Author\": {{ quote .Author }},\n        \"Email\": {{ quote .Email }},\n        \"Date\": {{ quote .Date }},\n        \"Message\": {{ quote .Message }},\n        \"Tags\": [{{ $lastTag := (sub (len .Tags ) 1) }}{{ range $j, $tag := .Tags }}{{ quote . }}{{ if ne $j $lastTag }},{{ end }}{{ end }}],\n        \"RuleID\": {{ quote .RuleID }},\n        \"Fingerprint\": {{ quote .Fingerprint }}\n    }{{ if ne $i $lastFinding }},{{ end }}\n{{- end}}{{ end }}\n]\n\nUsage:\n$ gitleaks dir ~/leaky-repo/ --report-path \"report.json\" --report-format template --report-template testdata/report/jsonextra.tmpl\nSponsorships\n\ncoderabbit.ai\n\n\n\n\nExit Codes\nYou can always set the exit code when leaks are encountered with the --exit-code flag. Default exit codes below:\n0 - no leaks present\n1 - leaks or error encountered\n126 - unknown flag",
      "languages": {
        "R": 1,
        "go": 1,
        "GO": 1,
        "Shell": 1,
        "Go": 1
      },
      "topics": [
        "Go",
        "Docker",
        "R",
        "Shell"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:51.899875"
    },
    {
      "owner": "danielmiessler",
      "name": "fabric",
      "url": "https://github.com/danielmiessler/fabric",
      "description": "fabric is an open-source framework for augmenting humans using AI. It provides a modular framework for solving specific problems using a crowdsourced set of AI prompts that can be used anywhere.",
      "readme_content": "fabric\n\n\n\n\n\n\nfabric is an open-source framework for augmenting humans using AI.\n\nUpdates ‚Ä¢\nWhat and Why ‚Ä¢\nPhilosophy ‚Ä¢\nInstallation ‚Ä¢\nUsage ‚Ä¢\nExamples ‚Ä¢\nJust Use the Patterns ‚Ä¢\nCustom Patterns ‚Ä¢\nHelper Apps ‚Ä¢\nMeta\n\n\nNavigation\n\nfabric\n\nNavigation\nUpdates\nIntro videos\nWhat and why\nPhilosophy\n\nBreaking problems into components\nToo many prompts\n\n\nInstallation\n\nGet Latest Release Binaries\nFrom Source\nEnvironment Variables\nSetup\nAdd aliases for all patterns\n\nSave your files in markdown using aliases\n\n\nMigration\nUpgrading\n\n\nUsage\nOur approach to prompting\nExamples\nJust use the Patterns\nCustom Patterns\nHelper Apps\n\nto_pdf\nto_pdf Installation\n\n\npbpaste\nWeb Interface\nMeta\n\nPrimary contributors\n\n\n\n\n\n\nUpdates\nNoteFebruary 5, 2025\n\nRemember that fabric supports o1 and o3 models, but you need to 1) not use -s, and 2) use the --raw flag because the o1 and o3 models don't support the --stream option or temperature settings.\n\n\nWhat and why\nSince the start of 2023 and GenAI we've seen a massive number of AI applications for accomplishing tasks. It's powerful, but it's not easy to integrate this functionality into our lives.\n\nIn other words, AI doesn't have a capabilities problem‚Äîit has an integration problem.\n\nFabric was created to address this by enabling everyone to granularly apply AI to everyday challenges.\nIntro videos\nKeep in mind that many of these were recorded when Fabric was Python-based, so remember to use the current install instructions below.\n\nNetwork Chuck\nDavid Bombal\nMy Own Intro to the Tool\nMore Fabric YouTube Videos\n\nPhilosophy\n\nAI isn't a thing; it's a magnifier of a thing. And that thing is human creativity.\n\nWe believe the purpose of technology is to help humans flourish, so when we talk about AI we start with the human problems we want to solve.\nBreaking problems into components\nOur approach is to break problems into individual pieces (see below) and then apply AI to them one at a time. See below for some examples.\n\nToo many prompts\nPrompts are good for this, but the biggest challenge I faced in 2023‚Äî‚Äîwhich still exists today‚Äîis the sheer number of AI prompts out there. We all have prompts that are useful, but it's hard to discover new ones, know if they are good or not, and manage different versions of the ones we like.\nOne of fabric's primary features is helping people collect and integrate prompts, which we call Patterns, into various parts of their lives.\nFabric has Patterns for all sorts of life and work activities, including:\n\nExtracting the most interesting parts of YouTube videos and podcasts\nWriting an essay in your own voice with just an idea as an input\nSummarizing opaque academic papers\nCreating perfectly matched AI art prompts for a piece of writing\nRating the quality of content to see if you want to read/watch the whole thing\nGetting summaries of long, boring content\nExplaining code to you\nTurning bad documentation into usable documentation\nCreating social media posts from any content input\nAnd a million more‚Ä¶\n\nInstallation\nTo install Fabric, you can use the latest release binaries or install it from the source.\nGet Latest Release Binaries\nWindows:\nhttps://github.com/danielmiessler/fabric/releases/latest/download/fabric-windows-amd64.exe\nMacOS (arm64):\ncurl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-darwin-arm64 > fabric && chmod +x fabric && ./fabric --version\nMacOS (amd64):\ncurl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-darwin-amd64 > fabric && chmod +x fabric && ./fabric --version\nLinux (amd64):\ncurl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-linux-amd64 > fabric && chmod +x fabric && ./fabric --version\nLinux (arm64):\ncurl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-linux-arm64 > fabric && chmod +x fabric && ./fabric --version\nFrom Source\nTo install Fabric, make sure Go is installed, and then run the following command.\n# Install Fabric directly from the repo\ngo install github.com/danielmiessler/fabric@latest\nEnvironment Variables\nYou may need to set some environment variables in your ~/.bashrc on linux or ~/.zshrc file on mac to be able to run the fabric command. Here is an example of what you can add:\nFor Intel based macs or linux\n# Golang environment variables\nexport GOROOT=/usr/local/go\nexport GOPATH=$HOME/go\n\n# Update PATH to include GOPATH and GOROOT binaries\nexport PATH=$GOPATH/bin:$GOROOT/bin:$HOME/.local/bin:$PATH\nfor Apple Silicon based macs\n# Golang environment variables\nexport GOROOT=$(brew --prefix go)/libexec\nexport GOPATH=$HOME/go\nexport PATH=$GOPATH/bin:$GOROOT/bin:$HOME/.local/bin:$PATH\nSetup\nNow run the following command\n# Run the setup to set up your directories and keys\nfabric --setup\nIf everything works you are good to go.\nAdd aliases for all patterns\nIn order to add aliases for all your patterns and use them directly as commands ie. summarize instead of fabric --pattern summarize\nYou can add the following to your .zshrc or .bashrc file.\n# Loop through all files in the ~/.config/fabric/patterns directory\nfor pattern_file in $HOME/.config/fabric/patterns/*; do\n    # Get the base name of the file (i.e., remove the directory path)\n    pattern_name=$(basename \"$pattern_file\")\n\n    # Create an alias in the form: alias pattern_name=\"fabric --pattern pattern_name\"\n    alias_command=\"alias $pattern_name='fabric --pattern $pattern_name'\"\n\n    # Evaluate the alias command to add it to the current shell\n    eval \"$alias_command\"\ndone\n\nyt() {\n    local video_link=\"$1\"\n    fabric -y \"$video_link\" --transcript\n}\nYou can add the below code for the equivalent aliases inside PowerShell by running notepad $PROFILE inside a PowerShell window:\n# Path to the patterns directory\n$patternsPath = Join-Path $HOME \".config/fabric/patterns\"\nforeach ($patternDir in Get-ChildItem -Path $patternsPath -Directory) {\n    $patternName = $patternDir.Name\n\n    # Dynamically define a function for each pattern\n    $functionDefinition = @\"\nfunction $patternName {\n    [CmdletBinding()]\n    param(\n        [Parameter(ValueFromPipeline = `$true)]\n        [string] `$InputObject,\n\n        [Parameter(ValueFromRemainingArguments = `$true)]\n        [String[]] `$patternArgs\n    )\n\n    begin {\n        # Initialize an array to collect pipeline input\n        `$collector = @()\n    }\n\n    process {\n        # Collect pipeline input objects\n        if (`$InputObject) {\n            `$collector += `$InputObject\n        }\n    }\n\n    end {\n        # Join all pipeline input into a single string, separated by newlines\n        `$pipelineContent = `$collector -join \"`n\"\n\n        # If there's pipeline input, include it in the call to fabric\n        if (`$pipelineContent) {\n            `$pipelineContent | fabric --pattern $patternName `$patternArgs\n        } else {\n            # No pipeline input; just call fabric with the additional args\n            fabric --pattern $patternName `$patternArgs\n        }\n    }\n}\n\"@\n    # Add the function to the current session\n    Invoke-Expression $functionDefinition\n}\n\n# Define the 'yt' function as well\nfunction yt {\n    [CmdletBinding()]\n    param(\n        [Parameter(Mandatory = $true)]\n        [string]$videoLink\n    )\n    fabric -y $videoLink --transcript\n}\nThis also creates a yt alias that allows you to use yt https://www.youtube.com/watch?v=4b0iet22VIk to get transcripts, comments, and metadata.\nSave your files in markdown using aliases\nIf in addition to the above aliases you would like to have the option to save the output to your favourite markdown note vault like Obsidian then instead of the above add the following to your .zshrc or .bashrc file:\n# Define the base directory for Obsidian notes\nobsidian_base=\"/path/to/obsidian\"\n\n# Loop through all files in the ~/.config/fabric/patterns directory\nfor pattern_file in ~/.config/fabric/patterns/*; do\n    # Get the base name of the file (i.e., remove the directory path)\n    pattern_name=$(basename \"$pattern_file\")\n\n    # Unalias any existing alias with the same name\n    unalias \"$pattern_name\" 2>/dev/null\n\n    # Define a function dynamically for each pattern\n    eval \"\n    $pattern_name() {\n        local title=\\$1\n        local date_stamp=\\$(date +'%Y-%m-%d')\n        local output_path=\\\"\\$obsidian_base/\\${date_stamp}-\\${title}.md\\\"\n\n        # Check if a title was provided\n        if [ -n \\\"\\$title\\\" ]; then\n            # If a title is provided, use the output path\n            fabric --pattern \\\"$pattern_name\\\" -o \\\"\\$output_path\\\"\n        else\n            # If no title is provided, use --stream\n            fabric --pattern \\\"$pattern_name\\\" --stream\n        fi\n    }\n    \"\ndone\n\nyt() {\n    local video_link=\"$1\"\n    fabric -y \"$video_link\" --transcript\n}\nThis will allow you to use the patterns as aliases like in the above for example summarize instead of fabric --pattern summarize --stream, however if you pass in an extra argument like this summarize \"my_article_title\" your output will be saved in the destination that you set in obsidian_base=\"/path/to/obsidian\" in the following format YYYY-MM-DD-my_article_title.md where the date gets autogenerated for you.\nYou can tweak the date format by tweaking the date_stamp format.\nMigration\nIf you have the Legacy (Python) version installed and want to migrate to the Go version, here's how you do it. It's basically two steps: 1) uninstall the Python version, and 2) install the Go version.\n# Uninstall Legacy Fabric\npipx uninstall fabric\n\n# Clear any old Fabric aliases\n(check your .bashrc, .zshrc, etc.)\n# Install the Go version\ngo install github.com/danielmiessler/fabric@latest\n# Run setup for the new version. Important because things have changed\nfabric --setup\nThen set your environmental variables as shown above.\nUpgrading\nThe great thing about Go is that it's super easy to upgrade. Just run the same command you used to install it in the first place and you'll always get the latest version.\ngo install github.com/danielmiessler/fabric@latest\nUsage\nOnce you have it all set up, here's how to use it.\nfabric -h\nUsage:\n  fabric [OPTIONS]\n\nApplication Options:\n  -p, --pattern=             Choose a pattern from the available patterns\n  -v, --variable=            Values for pattern variables, e.g. -v=#role:expert -v=#points:30\"\n  -C, --context=             Choose a context from the available contexts\n      --session=             Choose a session from the available sessions\n  -a, --attachment=          Attachment path or URL (e.g. for OpenAI image recognition messages)\n  -S, --setup                Run setup for all reconfigurable parts of fabric\n  -t, --temperature=         Set temperature (default: 0.7)\n  -T, --topp=                Set top P (default: 0.9)\n  -s, --stream               Stream\n  -P, --presencepenalty=     Set presence penalty (default: 0.0)\n  -r, --raw                  Use the defaults of the model without sending chat options (like temperature etc.) and use the user role instead of the system role for patterns.\n  -F, --frequencypenalty=    Set frequency penalty (default: 0.0)\n  -l, --listpatterns         List all patterns\n  -L, --listmodels           List all available models\n  -x, --listcontexts         List all contexts\n  -X, --listsessions         List all sessions\n  -U, --updatepatterns       Update patterns\n  -c, --copy                 Copy to clipboard\n  -m, --model=               Choose model\n  -o, --output=              Output to file\n      --output-session       Output the entire session (also a temporary one) to the output file\n  -n, --latest=              Number of latest patterns to list (default: 0)\n  -d, --changeDefaultModel   Change default model\n  -y, --youtube=             YouTube video \"URL\" to grab transcript, comments from it and send to chat\n      --transcript           Grab transcript from YouTube video and send to chat (it used per default).\n      --comments             Grab comments from YouTube video and send to chat\n      --metadata             Grab metadata from YouTube video and send to chat\n  -g, --language=            Specify the Language Code for the chat, e.g. -g=en -g=zh\n  -u, --scrape_url=          Scrape website URL to markdown using Jina AI\n  -q, --scrape_question=     Search question using Jina AI\n  -e, --seed=                Seed to be used for LMM generation\n  -w, --wipecontext=         Wipe context\n  -W, --wipesession=         Wipe session\n      --printcontext=        Print context\n      --printsession=        Print session\n      --readability          Convert HTML input into a clean, readable view\n      --serve                Initiate the API server\n      --dry-run              Show what would be sent to the model without actually sending it\n      --version              Print current version\n\nHelp Options:\n  -h, --help                 Show this help message\n\nOur approach to prompting\nFabric Patterns are different than most prompts you'll see.\n\nFirst, we use Markdown to help ensure maximum readability and editability. This not only helps the creator make a good one, but also anyone who wants to deeply understand what it does. Importantly, this also includes the AI you're sending it to!\n\nHere's an example of a Fabric Pattern.\nhttps://github.com/danielmiessler/fabric/blob/main/patterns/extract_wisdom/system.md\n\n\n\nNext, we are extremely clear in our instructions, and we use the Markdown structure to emphasize what we want the AI to do, and in what order.\n\n\nAnd finally, we tend to use the System section of the prompt almost exclusively. In over a year of being heads-down with this stuff, we've just seen more efficacy from doing that. If that changes, or we're shown data that says otherwise, we will adjust.\n\n\nExamples\n\nThe following examples use the macOS pbpaste to paste from the clipboard. See the pbpaste section below for Windows and Linux alternatives.\n\nNow let's look at some things you can do with Fabric.\n\nRun the summarize Pattern based on input from stdin. In this case, the body of an article.\n\npbpaste | fabric --pattern summarize\n\nRun the analyze_claims Pattern with the --stream option to get immediate and streaming results.\n\npbpaste | fabric --stream --pattern analyze_claims\n\nRun the extract_wisdom Pattern with the --stream option to get immediate and streaming results from any Youtube video (much like in the original introduction video).\n\nfabric -y \"https://youtube.com/watch?v=uXs-zPc63kM\" --stream --pattern extract_wisdom\n\n\nCreate patterns- you must create a .md file with the pattern and save it to ~/.config/fabric/patterns/[yourpatternname].\n\n\nRun a analyze_claims pattern on a website. Fabric uses Jina AI to scrape the URL into markdown format before sending it to the model.\n\n\nfabric -u https://github.com/danielmiessler/fabric/ -p analyze_claims\nJust use the Patterns\n\n\n\nIf you're not looking to do anything fancy, and you just want a lot of great prompts, you can navigate to the /patterns directory and start exploring!\nWe hope that if you used nothing else from Fabric, the Patterns by themselves will make the project useful.\nYou can use any of the Patterns you see there in any AI application that you have, whether that's ChatGPT or some other app or website. Our plan and prediction is that people will soon be sharing many more than those we've published, and they will be way better than ours.\nThe wisdom of crowds for the win.\nCustom Patterns\nYou may want to use Fabric to create your own custom Patterns‚Äîbut not share them with others. No problem!\nJust make a directory in ~/.config/custompatterns/ (or wherever) and put your .md files in there.\nWhen you're ready to use them, copy them into:\n~/.config/fabric/patterns/\n\nYou can then use them like any other Patterns, but they won't be public unless you explicitly submit them as Pull Requests to the Fabric project. So don't worry‚Äîthey're private to you.\nHelper Apps\nFabric also makes use of some core helper apps (tools) to make it easier to integrate with your various workflows. Here are some examples:\nto_pdf\nto_pdf is a helper command that converts LaTeX files to PDF format. You can use it like this:\nto_pdf input.tex\nThis will create a PDF file from the input LaTeX file in the same directory.\nYou can also use it with stdin which works perfectly with the write_latex pattern:\necho \"ai security primer\" | fabric --pattern write_latex | to_pdf\nThis will create a PDF file named output.pdf in the current directory.\nto_pdf Installation\nTo install to_pdf, install it the same way as you install Fabric, just with a different repo name.\ngo install github.com/danielmiessler/fabric/plugins/tools/to_pdf@latest\nMake sure you have a LaTeX distribution (like TeX Live or MiKTeX) installed on your system, as to_pdf requires pdflatex to be available in your system's PATH.\npbpaste\nThe examples use the macOS program pbpaste to paste content from the clipboard to pipe into fabric as the input. pbpaste is not available on Windows or Linux, but there are alternatives.\nOn Windows, you can use the PowerShell command Get-Clipboard from a PowerShell command prompt. If you like, you can also alias it to pbpaste. If you are using classic PowerShell, edit the file ~\\Documents\\WindowsPowerShell\\.profile.ps1, or if you are using PowerShell Core, edit ~\\Documents\\PowerShell\\.profile.ps1 and add the alias,\nSet-Alias pbpaste Get-Clipboard\nOn Linux, you can use xclip -selection clipboard -o to paste from the clipboard. You will likely need to install xclip with your package manager. For Debian based systems including Ubuntu,\nsudo apt update\nsudo apt install xclip -y\nYou can also create an alias by editing ~/.bashrc or ~/.zshrc and adding the alias,\nalias pbpaste='xclip -selection clipboard -o'\nWeb Interface\nFabric now includes a built-in web interface that provides a GUI alternative to the command-line interface and an out-of-the-box website for those who want to get started with web development or blogging.\nYou can use this app as a GUI interface for Fabric, a ready to go blog-site, or a website template for your own projects.\nThe web/src/lib/content directory includes starter .obsidian/ and templates/ directories, allowing you to open up the web/src/lib/content/ directory as an Obsidian.md vault. You can place your posts in the posts directory when you're ready to publish.\nInstalling\nThe GUI can be installed by navigating to the¬†web¬†directory and using¬†npm install,¬†pnpm install, or your favorite package manager. Then simply run¬†the development server to start the app.\nYou will need to run fabric in a separate terminal with the¬†fabric --serve¬†command.\nFrom the fabric project web/ directory:\nnpm run dev\n\n## or ##\n\npnpm run dev\n\n## or your equivalent\nStreamlit UI\nTo run the Streamlit user interface:\n# Install required dependencies\npip install streamlit pandas matplotlib seaborn numpy python-dotenv\n\n# Run the Streamlit app\nstreamlit run streamlit.py\nThe Streamlit UI provides a user-friendly interface for:\n\nRunning and chaining patterns\nManaging pattern outputs\nCreating and editing patterns\nAnalyzing pattern results\n\nMeta\nNoteSpecial thanks to the following people for their inspiration and contributions!\n\n\nJonathan Dunn for being the absolute MVP dev on the project, including spearheading the new Go version, as well as the GUI! All this while also being a full-time medical doctor!\nCaleb Sima for pushing me over the edge of whether to make this a public project or not.\nEugen Eisler and Frederick Ros for their invaluable contributions to the Go version\nDavid Peters for his work on the web interface.\nJoel Parish for super useful input on the project's Github directory structure..\nJoseph Thacker for the idea of a -c context flag that adds pre-created context in the ./config/fabric/ directory to all Pattern queries.\nJason Haddix for the idea of a stitch (chained Pattern) to filter content using a local model before sending on to a cloud model, i.e., cleaning customer data using llama2 before sending on to gpt-4 for analysis.\nAndre Guerra for assisting with numerous components to make things simpler and more maintainable.\n\nPrimary contributors\n\n\n\n\nfabric was created by Daniel Miessler in January of 2024.",
      "languages": {
        "python": 1,
        "Python": 1,
        "Shell": 1,
        "HTML": 1,
        "go": 1,
        "GO": 1,
        "R": 1,
        "Go": 1
      },
      "topics": [
        "Python",
        "Shell",
        "Web Development",
        "R",
        "Go"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:52.606790"
    },
    {
      "owner": "kubernetes",
      "name": "test-infra",
      "url": "https://github.com/kubernetes/test-infra",
      "description": "Test infrastructure for the Kubernetes project.",
      "readme_content": "test-infra\n\n\nThis repository contains tools and configuration files for the testing and\nautomation needs of the Kubernetes project.\nOur architecture diagram provides an (updated #13063)\noverview of how the different tools and services interact.\nCI Job Management\nKubernetes uses a prow instance at prow.k8s.io to handle CI and\nautomation for the entire project. Everyone can participate in a\nself-service PR-based workflow, where changes are automatically deployed\nafter they have been reviewed. All job configs are located in config/jobs\n\nAdd or update job configs\nDelete job configs\nTest job configs locally\nTrigger jobs on PRs using bot commands\n\nDashboards\nTest Result Dashboards\n\nTestgrid shows historical test results over time (testgrid)\nTriage shows clusters of similar test failures across all jobs (triage)\n\nJob and PR Dashboards\n\nDeck shows what jobs are running or have recently run in prow (prow/cmd/deck)\nGubernator's PR Dashboard shows which PRs need your review (gubernator)\nPR Status shows what needs to be done to get PRs matching a GitHub Query to merge (prow/cmd/tide)\nTide History shows what actions tide has taken over time to trigger tests and merge PRs (prow/cmd/tide)\nTide Status shows what PRs are in tide pools to be tested and merged (prow/cmd/tide)\n\nOther Tools\n\nboskos manages pools of resources; our CI leases GCP projects from these pools\nexperiment is a catchall directory for one-shot tools or scripts\ngcsweb is a UI we use to display test artifacts stored in public GCS buckets\nghproxy is a GitHub-aware reverse proxy cache to help keep our GitHub API token usage within rate limits\ngopherage is a tool for manipulating Go coverage files\ngreenhouse is a shared bazel cache we use to ensure faster build and test presubmit jobs\nlabel_sync creates, updates and migrates GitHub labels across orgs and repos based on labels.yaml file\nkettle extracts test results from GCS and puts them into bigquery\nkubetest is how our CI creates and e2e tests kubernetes clusters\nmaintenance/migratestatus is used to migrate or retire GitHub status contexts on PRs across orgs and repos\nmetrics runs queries against bigquery to generate metrics based on test results\nrobots/commenter is used by some of our jobs to comment on GitHub issues\n\nContributing\nPlease see CONTRIBUTING.MD",
      "languages": {
        "Go": 1,
        "go": 1,
        "GO": 1
      },
      "topics": [
        "Go",
        "Kubernetes"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:53.444681"
    },
    {
      "owner": "trufflesecurity",
      "name": "trufflehog",
      "url": "https://github.com/trufflesecurity/trufflehog",
      "description": "Find, verify, and analyze leaked credentials",
      "readme_content": "TruffleHog\nFind leaked credentials.\n\n\n\n\n\n\n\n\nüîé Now Scanning\n\n\n...and more\nTo learn more about about TruffleHog and its features and capabilities, visit our product page.\n\nüåê TruffleHog Enterprise\nAre you interested in continuously monitoring Git, Jira, Slack, Confluence, Microsoft Teams, Sharepoint, and more.. for credentials? We have an enterprise product that can help! Learn more at https://trufflesecurity.com/trufflehog-enterprise.\nWe take the revenue from the enterprise product to fund more awesome open source projects that the whole community can benefit from.\nWhat is TruffleHog üêΩ\nTruffleHog is the most powerful secrets Discovery, Classification, Validation, and Analysis tool. In this context secret refers to a credential a machine uses to authenticate itself to another machine. This includes API keys, database passwords, private encryption keys, and more...\nDiscovery üîç\nTruffleHog can look for secrets in many places including Git, chats, wikis, logs, API testing platforms, object stores, filesystems and more\nClassification üìÅ\nTruffleHog classifies over 800 secret types, mapping them back to the specific identity they belong to. Is it an AWS secret? Stripe secret? Cloudflare secret? Postgres password? SSL Private key? Sometimes its hard to tell looking at it, so TruffleHog classifies everything it finds.\nValidation ‚úÖ\nFor every secret TruffleHog can classify, it can also log in to confirm if that secret is live or not. This step is critical to know if there‚Äôs an active present danger or not.\nAnalysis üî¨\nFor the 20 some of the most commonly leaked out credential types, instead of sending one request to check if the secret can log in, TruffleHog can send many requests to learn everything there is to know about the secret. Who created it? What resources can it access? What permissions does it have on those resources?\nüì¢ Join Our Community\nHave questions? Feedback? Jump in slack or discord and hang out with us\nJoin our Slack Community\nJoin the Secret Scanning Discord\nüì∫ Demo\n\ndocker run --rm -it -v \"$PWD:/pwd\" trufflesecurity/trufflehog:latest github --org=trufflesecurity\nüíæ Installation\nSeveral options available for you:\nMacOS users\nbrew install trufflehog\nDocker:\nEnsure Docker engine is running before executing the following commands:\n¬†¬†¬†¬†Unix\ndocker run --rm -it -v \"$PWD:/pwd\" trufflesecurity/trufflehog:latest github --repo https://github.com/trufflesecurity/test_keys\n¬†¬†¬†¬†Windows Command Prompt\ndocker run --rm -it -v \"%cd:/=\\%:/pwd\" trufflesecurity/trufflehog:latest github --repo https://github.com/trufflesecurity/test_keys\n¬†¬†¬†¬†Windows PowerShell\ndocker run --rm -it -v \"${PWD}:/pwd\" trufflesecurity/trufflehog github --repo https://github.com/trufflesecurity/test_keys\n¬†¬†¬†¬†M1 and M2 Mac\ndocker run --platform linux/arm64 --rm -it -v \"$PWD:/pwd\" trufflesecurity/trufflehog:latest github --repo https://github.com/trufflesecurity/test_keys\nBinary releases\nDownload and unpack from https://github.com/trufflesecurity/trufflehog/releases\nCompile from source\ngit clone https://github.com/trufflesecurity/trufflehog.git\ncd trufflehog; go install\nUsing installation script\ncurl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -b /usr/local/bin\nUsing installation script, verify checksum signature (requires cosign to be installed)\ncurl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -v -b /usr/local/bin\nUsing installation script to install a specific version\ncurl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -b /usr/local/bin <ReleaseTag like v3.56.0>\nüîê Verifying the artifacts\nChecksums are applied to all artifacts, and the resulting checksum file is signed using cosign.\nYou need the following tool to verify signature:\n\nCosign\n\nVerification steps are as follow:\n\n\nDownload the artifact files you want, and the following files from the releases page.\n\ntrufflehog_{version}_checksums.txt\ntrufflehog_{version}_checksums.txt.pem\ntrufflehog_{version}_checksums.txt.sig\n\n\n\nVerify the signature:\ncosign verify-blob <path to trufflehog_{version}_checksums.txt> \\\n--certificate <path to trufflehog_{version}_checksums.txt.pem> \\\n--signature <path to trufflehog_{version}_checksums.txt.sig> \\\n--certificate-identity-regexp 'https://github\\.com/trufflesecurity/trufflehog/\\.github/workflows/.+' \\\n--certificate-oidc-issuer \"https://token.actions.githubusercontent.com\"\n\n\nOnce the signature is confirmed as valid, you can proceed to validate that the SHA256 sums align with the downloaded artifact:\nsha256sum --ignore-missing -c trufflehog_{version}_checksums.txt\n\n\nReplace {version} with the downloaded files version\nAlternatively, if you are using installation script, pass -v option to perform signature verification.\nThis required Cosign binary to be installed prior to running installation script.\nüöÄ Quick Start\n1: Scan a repo for only verified secrets\nCommand:\ntrufflehog git https://github.com/trufflesecurity/test_keys --results=verified,unknown\nExpected output:\nüê∑üîëüê∑  TruffleHog. Unearth your secrets. üê∑üîëüê∑\n\nFound verified result üê∑üîë\nDetector Type: AWS\nDecoder Type: PLAIN\nRaw result: AKIAYVP4CIPPERUVIFXG\nLine: 4\nCommit: fbc14303ffbf8fb1c2c1914e8dda7d0121633aca\nFile: keys\nEmail: counter <counter@counters-MacBook-Air.local>\nRepository: https://github.com/trufflesecurity/test_keys\nTimestamp: 2022-06-16 10:17:40 -0700 PDT\n...\n\n2: Scan a GitHub Org for only verified secrets\ntrufflehog github --org=trufflesecurity --results=verified,unknown\n3: Scan a GitHub Repo for only verified keys and get JSON output\nCommand:\ntrufflehog git https://github.com/trufflesecurity/test_keys --results=verified,unknown --json\nExpected output:\n{\"SourceMetadata\":{\"Data\":{\"Git\":{\"commit\":\"fbc14303ffbf8fb1c2c1914e8dda7d0121633aca\",\"file\":\"keys\",\"email\":\"counter \\u003ccounter@counters-MacBook-Air.local\\u003e\",\"repository\":\"https://github.com/trufflesecurity/test_keys\",\"timestamp\":\"2022-06-16 10:17:40 -0700 PDT\",\"line\":4}}},\"SourceID\":0,\"SourceType\":16,\"SourceName\":\"trufflehog - git\",\"DetectorType\":2,\"DetectorName\":\"AWS\",\"DecoderName\":\"PLAIN\",\"Verified\":true,\"Raw\":\"AKIAYVP4CIPPERUVIFXG\",\"Redacted\":\"AKIAYVP4CIPPERUVIFXG\",\"ExtraData\":{\"account\":\"595918472158\",\"arn\":\"arn:aws:iam::595918472158:user/canarytokens.com@@mirux23ppyky6hx3l6vclmhnj\",\"user_id\":\"AIDAYVP4CIPPJ5M54LRCY\"},\"StructuredData\":null}\n...\n\n4: Scan a GitHub Repo + its Issues and Pull Requests\ntrufflehog github --repo=https://github.com/trufflesecurity/test_keys --issue-comments --pr-comments\n5: Scan an S3 bucket for verified keys\ntrufflehog s3 --bucket=<bucket name> --results=verified,unknown\n6: Scan S3 buckets using IAM Roles\ntrufflehog s3 --role-arn=<iam role arn>\n7: Scan a Github Repo using SSH authentication in docker\ndocker run --rm -v \"$HOME/.ssh:/root/.ssh:ro\" trufflesecurity/trufflehog:latest git ssh://github.com/trufflesecurity/test_keys\n8: Scan individual files or directories\ntrufflehog filesystem path/to/file1.txt path/to/file2.txt path/to/dir\n9: Scan a local git repo\nClone the git repo. For example test keys repo.\n$ git clone git@github.com:trufflesecurity/test_keys.git\nRun trufflehog from the parent directory (outside the git repo).\n$ trufflehog git file://test_keys --results=verified,unknown\n10: Scan GCS buckets for verified secrets\ntrufflehog gcs --project-id=<project-ID> --cloud-environment --results=verified,unknown\n11: Scan a Docker image for verified secrets\nUse the --image flag multiple times to scan multiple images.\ntrufflehog docker --image trufflesecurity/secrets --results=verified,unknown\n12: Scan in CI\nSet the --since-commit flag to your default branch that people merge into (ex: \"main\"). Set the --branch flag to your PR's branch name (ex: \"feature-1\"). Depending on the CI/CD platform you use, this value can be pulled in dynamically (ex: CIRCLE_BRANCH in Circle CI and TRAVIS_PULL_REQUEST_BRANCH in Travis CI). If the repo is cloned and the target branch is already checked out during the CI/CD workflow, then --branch HEAD should be sufficient. The --fail flag will return an 183 error code if valid credentials are found.\ntrufflehog git file://. --since-commit main --branch feature-1 --results=verified,unknown --fail\n13: Scan a Postman workspace\nUse the --workspace-id, --collection-id, --environment flags multiple times to scan multiple targets.\ntrufflehog postman --token=<postman api token> --workspace-id=<workspace id>\n14: Scan a Jenkins server\ntrufflehog jenkins --url https://jenkins.example.com --username admin --password admin\n15: Scan an Elasticsearch server\nScan a Local Cluster\nThere are two ways to authenticate to a local cluster with TruffleHog: (1) username and password, (2) service token.\nConnect to a local cluster with username and password\ntrufflehog elasticsearch --nodes 192.168.14.3 192.168.14.4 --username truffle --password hog\nConnect to a local cluster with a service token\ntrufflehog elasticsearch --nodes 192.168.14.3 192.168.14.4 --service-token ‚ÄòAAEWVaWM...Rva2VuaSDZ‚Äô\nScan an Elastic Cloud Cluster\nTo scan a cluster on Elastic Cloud, you‚Äôll need a Cloud ID and API key.\ntrufflehog elasticsearch \\\n  --cloud-id 'search-prod:dXMtY2Vx...YjM1ODNlOWFiZGRlNjI0NA==' \\\n  --api-key 'MlVtVjBZ...ZSYlduYnF1djh3NG5FQQ=='\n16. Scan a GitHub Repository for Cross Fork Object References and Deleted Commits\nThe following command will enumerate deleted and hidden commits on a GitHub repository and then scan them for secrets. This is an alpha release feature.\ntrufflehog github-experimental --repo https://github.com/<USER>/<REPO>.git --object-discovery\nIn addition to the normal TruffleHog output, the --object-discovery flag creates two files in a new $HOME/.trufflehog directory: valid_hidden.txt and invalid.txt. These are used to track state during commit enumeration, as well as to provide users with a complete list of all hidden and deleted commits (valid_hidden.txt). If you'd like to automatically remove these files after scanning, please add the flag --delete-cached-data.\nNote: Enumerating all valid commits on a repository using this method takes between 20 minutes and a few hours, depending on the size of your repository. We added a progress bar to keep you updated on how long the enumeration will take. The actual secret scanning runs extremely fast.\nFor more information on Cross Fork Object References, please read our blog post.\n17. Scan Hugging Face\nScan a Hugging Face Model, Dataset or Space\ntrufflehog huggingface --model <model_id> --space <space_id> --dataset <dataset_id>\nScan all Models, Datasets and Spaces belonging to a Hugging Face Organization or User\ntrufflehog huggingface --org <orgname> --user <username>\n(Optionally) When scanning an organization or user, you can skip an entire class of resources with --skip-models, --skip-datasets, --skip-spaces OR a particular resource with --ignore-models <model_id>, --ignore-datasets <dataset_id>, --ignore-spaces <space_id>.\nScan Discussion and PR Comments\ntrufflehog huggingface --model <model_id> --include-discussions --include-prs\n‚ùì FAQ\n\nAll I see is üê∑üîëüê∑  TruffleHog. Unearth your secrets. üê∑üîëüê∑ and the program exits, what gives?\n\nThat means no secrets were detected\n\n\nWhy is the scan taking a long time when I scan a GitHub org\n\nUnauthenticated GitHub scans have rate limits. To improve your rate limits, include the --token flag with a personal access token\n\n\nIt says a private key was verified, what does that mean?\n\nCheck out our Driftwood blog post to learn how to do this, in short we've confirmed the key can be used live for SSH or SSL Blog post\n\n\nIs there an easy way to ignore specific secrets?\n\nIf the scanned source supports line numbers, then you can add a trufflehog:ignore comment on the line containing the secret to ignore that secrets.\n\n\n\nüì∞ What's new in v3?\nTruffleHog v3 is a complete rewrite in Go with many new powerful features.\n\nWe've added over 700 credential detectors that support active verification against their respective APIs.\nWe've also added native support for scanning GitHub, GitLab, Docker, filesystems, S3, GCS, Circle CI and Travis CI.\nInstantly verify private keys against millions of github users and billions of TLS certificates using our Driftwood technology.\nScan binaries, documents, and other file formats\nAvailable as a GitHub Action and a pre-commit hook\n\nWhat is credential verification?\nFor every potential credential that is detected, we've painstakingly implemented programmatic verification against the API that we think it belongs to. Verification eliminates false positives. For example, the AWS credential detector performs a GetCallerIdentity API call against the AWS API to verify if an AWS credential is active.\nüìù Usage\nTruffleHog has a sub-command for each source of data that you may want to scan:\n\ngit\ngithub\ngitlab\ndocker\ns3\nfilesystem (files and directories)\nsyslog\ncircleci\ntravisci\ngcs (Google Cloud Storage)\npostman\njenkins\nelasticsearch\n\nEach subcommand can have options that you can see with the --help flag provided to the sub command:\n$ trufflehog git --help\nusage: TruffleHog git [<flags>] <uri>\n\nFind credentials in git repositories.\n\nFlags:\n  -h, --help                Show context-sensitive help (also try --help-long and --help-man).\n      --log-level=0         Logging verbosity on a scale of 0 (info) to 5 (trace). Can be disabled with \"-1\".\n      --profile             Enables profiling and sets a pprof and fgprof server on :18066.\n  -j, --json                Output in JSON format.\n      --json-legacy         Use the pre-v3.0 JSON format. Only works with git, gitlab, and github sources.\n      --github-actions      Output in GitHub Actions format.\n      --concurrency=20           Number of concurrent workers.\n      --no-verification     Don't verify the results.\n      --results=RESULTS          Specifies which type(s) of results to output: verified, unknown, unverified, filtered_unverified. Defaults to all types.\n      --allow-verification-overlap\n                                 Allow verification of similar credentials across detectors\n      --filter-unverified   Only output first unverified result per chunk per detector if there are more than one results.\n      --filter-entropy=FILTER-ENTROPY\n                                 Filter unverified results with Shannon entropy. Start with 3.0.\n      --config=CONFIG            Path to configuration file.\n      --print-avg-detector-time\n                                 Print the average time spent on each detector.\n      --no-update           Don't check for updates.\n      --fail                Exit with code 183 if results are found.\n      --verifier=VERIFIER ...    Set custom verification endpoints.\n      --custom-verifiers-only   Only use custom verification endpoints.\n      --archive-max-size=ARCHIVE-MAX-SIZE\n                                 Maximum size of archive to scan. (Byte units eg. 512B, 2KB, 4MB)\n      --archive-max-depth=ARCHIVE-MAX-DEPTH\n                                 Maximum depth of archive to scan.\n      --archive-timeout=ARCHIVE-TIMEOUT\n                                 Maximum time to spend extracting an archive.\n      --include-detectors=\"all\"  Comma separated list of detector types to include. Protobuf name or IDs may be used, as well as ranges.\n      --exclude-detectors=EXCLUDE-DETECTORS\n                                 Comma separated list of detector types to exclude. Protobuf name or IDs may be used, as well as ranges. IDs defined here take precedence over the include list.\n      --version             Show application version.\n  -i, --include-paths=INCLUDE-PATHS\n                                 Path to file with newline separated regexes for files to include in scan.\n  -x, --exclude-paths=EXCLUDE-PATHS\n                                 Path to file with newline separated regexes for files to exclude in scan.\n      --exclude-globs=EXCLUDE-GLOBS\n                                 Comma separated list of globs to exclude in scan. This option filters at the `git log` level, resulting in faster scans.\n      --since-commit=SINCE-COMMIT\n                                 Commit to start scan from.\n      --branch=BRANCH            Branch to scan.\n      --max-depth=MAX-DEPTH      Maximum depth of commits to scan.\n      --bare                Scan bare repository (e.g. useful while using in pre-receive hooks)\n\nArgs:\n  <uri>  Git repository URL. https://, file://, or ssh:// schema expected.\n\nFor example, to scan a git repository, start with\ntrufflehog git https://github.com/trufflesecurity/trufflehog.git\n\nS3\nThe S3 source supports assuming IAM roles for scanning in addition to IAM users. This makes it easier for users to scan multiple AWS accounts without needing to rely on hardcoded credentials for each account.\nThe IAM identity that TruffleHog uses initially will need to have AssumeRole privileges as a principal in the trust policy of each IAM role to assume.\nTo scan a specific bucket using locally set credentials or instance metadata if on an EC2 instance:\ntrufflehog s3 --bucket=<bucket-name>\nTo scan a specific bucket using an assumed role:\ntrufflehog s3 --bucket=<bucket-name> --role-arn=<iam-role-arn>\nMultiple roles can be passed as separate arguments. The following command will attempt to scan every bucket each role has permissions to list in the S3 API:\ntrufflehog s3 --role-arn=<iam-role-arn-1> --role-arn=<iam-role-arn-2>\nExit Codes:\n\n0: No errors and no results were found.\n1: An error was encountered. Sources may not have completed scans.\n183: No errors were encountered, but results were found. Will only be returned if --fail flag is used.\n\n TruffleHog Github Action\nGeneral Usage\non:\n  push:\n    branches:\n      - main\n  pull_request:\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v4\n      with:\n        fetch-depth: 0\n    - name: Secret Scanning\n      uses: trufflesecurity/trufflehog@main\n      with:\n        extra_args: --results=verified,unknown\n\nIn the example config above, we're scanning for live secrets in all PRs and Pushes to main. Only code changes in the referenced commits are scanned. If you'd like to scan an entire branch, please see the \"Advanced Usage\" section below.\nShallow Cloning\nIf you're incorporating TruffleHog into a standalone workflow and aren't running any other CI/CD tooling alongside TruffleHog, then we recommend using Shallow Cloning to speed up your workflow. Here's an example for how to do it:\n...\n      - shell: bash\n        run: |\n          if [ \"${{ github.event_name }}\" == \"push\" ]; then\n            echo \"depth=$(($(jq length <<< '${{ toJson(github.event.commits) }}') + 2))\" >> $GITHUB_ENV\n            echo \"branch=${{ github.ref_name }}\" >> $GITHUB_ENV\n          fi\n          if [ \"${{ github.event_name }}\" == \"pull_request\" ]; then\n            echo \"depth=$((${{ github.event.pull_request.commits }}+2))\" >> $GITHUB_ENV\n            echo \"branch=${{ github.event.pull_request.head.ref }}\" >> $GITHUB_ENV\n          fi\n      - uses: actions/checkout@v3\n        with:\n          ref: ${{env.branch}}\n          fetch-depth: ${{env.depth}}\n      - uses: trufflesecurity/trufflehog@main\n        with:\n          extra_args: --results=verified,unknown\n...\n\nDepending on the event type (push or PR), we calculate the number of commits present. Then we add 2, so that we can reference a base commit before our code changes. We pass that integer value to the fetch-depth flag in the checkout action in addition to the relevant branch. Now our checkout process should be much shorter.\nCanary detection\nTruffleHog statically detects https://canarytokens.org/ and lets you know when they're present without setting them off. You can learn more here: https://trufflesecurity.com/canaries\n\nAdvanced Usage\n- name: TruffleHog\n  uses: trufflesecurity/trufflehog@main\n  with:\n    # Repository path\n    path:\n    # Start scanning from here (usually main branch).\n    base:\n    # Scan commits until here (usually dev branch).\n    head: # optional\n    # Extra args to be passed to the trufflehog cli.\n    extra_args: --log-level=2 --results=verified,unknown\nIf you'd like to specify specific base and head refs, you can use the base argument (--since-commit flag in TruffleHog CLI) and the head argument (--branch flag in the TruffleHog CLI). We only recommend using these arguments for very specific use cases, where the default behavior does not work.\nAdvanced Usage: Scan entire branch\n- name: scan-push\n        uses: trufflesecurity/trufflehog@main\n        with:\n          base: \"\"\n          head: ${{ github.ref_name }}\n          extra_args: --results=verified,unknown\n\nTruffleHog GitLab CI\nExample Usage\nstages:\n  - security\n\nsecurity-secrets:\n  stage: security\n  allow_failure: false\n  image: alpine:latest\n  variables:\n    SCAN_PATH: \".\" # Set the relative path in the repo to scan\n  before_script:\n    - apk add --no-cache git curl jq\n    - curl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -b /usr/local/bin\n  script:\n    - trufflehog filesystem \"$SCAN_PATH\" --results=verified,unknown --fail --json | jq\n  rules:\n    - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'\nIn the example pipeline above, we're scanning for live secrets in all repository directories and files. This job runs only when the pipeline source is a merge request event, meaning it's triggered when a new merge request is created.\nPre-commit Hook\nTruffleHog can be used in a pre-commit hook to prevent credentials from leaking before they ever leave your computer.\nKey Usage Note:\n\nFor optimal hook efficacy, execute git add followed by git commit separately. This ensures TruffleHog analyzes all intended changes.\nAvoid using git commit -am, as it might bypass pre-commit hook execution for unstaged modifications.\n\nAn example .pre-commit-config.yaml is provided (see pre-commit.com for installation).\nrepos:\n  - repo: local\n    hooks:\n      - id: trufflehog\n        name: TruffleHog\n        description: Detect secrets in your data.\n        entry: bash -c 'trufflehog git file://. --since-commit HEAD --results=verified,unknown --fail'\n        # For running trufflehog in docker, use the following entry instead:\n        # entry: bash -c 'docker run --rm -v \"$(pwd):/workdir\" -i --rm trufflesecurity/trufflehog:latest git file:///workdir --since-commit HEAD --results=verified,unknown --fail'\n        language: system\n        stages: [\"commit\", \"push\"]\nRegex Detector (alpha)\nTruffleHog supports detection and verification of custom regular expressions.\nFor detection, at least one regular expression and keyword is required.\nA keyword is a fixed literal string identifier that appears in or around\nthe regex to be detected. To allow maximum flexibility for verification, a\nwebhook is used containing the regular expression matches.\nTruffleHog will send a JSON POST request containing the regex matches to a\nconfigured webhook endpoint. If the endpoint responds with a 200 OK response\nstatus code, the secret is considered verified.\nCustom Detectors support a few different filtering mechanisms: entropy, regex targeting the entire match, regex targeting the captured secret,\nand excluded word lists checked against the secret (captured group if present, entire match if capture group is not present). Note that if\nyour custom detector has multiple regex set (in this example hogID, and hogToken), then the filters get applied to each regex. Here is an example of a custom detector using these filters.\nNB: This feature is alpha and subject to change.\nRegex Detector Example\n# config.yaml\ndetectors:\n  - name: HogTokenDetector\n    keywords:\n      - hog\n    regex:\n      hogID: '\\b(HOG[0-9A-Z]{17})\\b'\n      hogToken: '[^A-Za-z0-9+\\/]{0,1}([A-Za-z0-9+\\/]{40})[^A-Za-z0-9+\\/]{0,1}'\n    verify:\n      - endpoint: http://localhost:8000/\n        # unsafe must be set if the endpoint is HTTP\n        unsafe: true\n        headers:\n          - \"Authorization: super secret authorization header\"\n$ trufflehog filesystem /tmp --config config.yaml --results=verified,unknown\nüê∑üîëüê∑  TruffleHog. Unearth your secrets. üê∑üîëüê∑\n\nFound verified result üê∑üîë\nDetector Type: CustomRegex\nDecoder Type: PLAIN\nRaw result: HOGAAIUNNWHAHJJWUQYR\nFile: /tmp/hog-facts.txt\n\nData structure sent to the custom verification server:\n{\n    \"HogTokenDetector\": {\n        \"HogID\": [\"HOGAAIUNNWHAHJJWUQYR\"],\n        \"HogSecret\": [\"sD9vzqdSsAOxntjAJ/qZ9sw+8PvEYg0r7D1Hhh0C\"],\n    }\n}\n\nVerification Server Example (Python)\nUnless you run a verification server, secrets found by the custom regex\ndetector will be unverified. Here is an example Python implementation of a\nverification server for the above config.yaml file.\nimport json\nfrom http.server import BaseHTTPRequestHandler, HTTPServer\n\nAUTH_HEADER = 'super secret authorization header'\n\n\nclass Verifier(BaseHTTPRequestHandler):\n    def do_GET(self):\n        self.send_response(405)\n        self.end_headers()\n\n    def do_POST(self):\n        try:\n            if self.headers['Authorization'] != AUTH_HEADER:\n                self.send_response(401)\n                self.end_headers()\n                return\n\n            # read the body\n            length = int(self.headers['Content-Length'])\n            request = json.loads(self.rfile.read(length))\n            self.log_message(\"%s\", request)\n\n            # check the match, you'll need to implement validateToken, which takes an array of ID's and Secrets\n            if not validateTokens(request['HogTokenDetector']['hogID'], request['HogTokenDetector']['hogSecret']):\n                self.send_response(200)\n                self.end_headers()\n            else:\n                # any other response besides 200\n                self.send_response(406)\n                self.end_headers()\n        except Exception:\n            self.send_response(400)\n            self.end_headers()\n\n\nwith HTTPServer(('', 8000), Verifier) as server:\n    try:\n        server.serve_forever()\n    except KeyboardInterrupt:\n        pass\nüîç Analyze\nTruffleHog supports running a deeper analysis of a credential to view its permissions and the resources it has access to.\ntrufflehog analyze\n‚ù§Ô∏è Contributors\nThis project exists thanks to all the people who contribute. [Contribute].\n\n\n\nüíª Contributing\nContributions are very welcome! Please see our contribution guidelines first.\nWe no longer accept contributions to TruffleHog v2, but that code is available in the v2 branch.\nAdding new secret detectors\nWe have published some documentation and tooling to get started on adding new secret detectors. Let's improve detection together!\nUse as a library\nCurrently, trufflehog is in heavy development and no guarantees can be made on\nthe stability of the public APIs at this time.\nLicense Change\nSince v3.0, TruffleHog is released under a AGPL 3 license, included in LICENSE. TruffleHog v3.0 uses none of the previous codebase, but care was taken to preserve backwards compatibility on the command line interface. The work previous to this release is still available licensed under GPL 2.0 in the history of this repository and the previous package releases and tags. A completed CLA is required for us to accept contributions going forward.",
      "languages": {
        "python": 1,
        "Python": 1,
        "go": 1,
        "GO": 1,
        "Shell": 1,
        "Go": 1
      },
      "topics": [
        "Python",
        "AWS",
        "Docker",
        "Shell",
        "Go"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:54.549258"
    },
    {
      "owner": "gruntwork-io",
      "name": "terragrunt",
      "url": "https://github.com/gruntwork-io/terragrunt",
      "description": "Terragrunt is a flexible orchestration tool that allows Infrastructure as Code written in OpenTofu/Terraform to scale.",
      "readme_content": "Terragrunt\n\n\n\n\n\nTerragrunt is a flexible orchestration tool that allows Infrastructure as Code written in OpenTofu/Terraform to scale.\nPlease see the following for more info, including install instructions and complete documentation:\n\nTerragrunt Website\nGetting started with Terragrunt\nTerragrunt Documentation\nContributing to Terragrunt\nCommercial Support\n\nJoin the Discord!\nJoin our community for discussions, support, and contributions:\n\nLicense\nThis code is released under the MIT License. See LICENSE.txt.",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T12:46:55.285918"
    },
    {
      "owner": "jaseemuddinn",
      "name": "onnoff_revamped",
      "url": "https://github.com/jaseemuddinn/onnoff_revamped",
      "description": "",
      "readme_content": "This is a [Next.js](https://nextjs.org) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).\n\n## Getting Started\n\nFirst, run the development server:\n\n```bash\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\n```\n\nOpen [http://localhost:3000](http://localhost:3000) with your browser to see the result.\n\nYou can start editing the page by modifying `app/page.js`. The page auto-updates as you edit the file.\n\nThis project uses [`next/font`](https://nextjs.org/docs/app/building-your-application/optimizing/fonts) to automatically optimize and load [Geist](https://vercel.com/font), a new font family for Vercel.\n\n## Learn More\n\nTo learn more about Next.js, take a look at the following resources:\n\n- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.\n- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.\n\nYou can check out [the Next.js GitHub repository](https://github.com/vercel/next.js) - your feedback and contributions are welcome!\n\n## Deploy on Vercel\n\nThe easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.\n\nCheck out our [Next.js deployment documentation](https://nextjs.org/docs/app/building-your-application/deploying) for more details.\n# onnoff_revamped",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T13:02:52.095105"
    },
    {
      "owner": "jaseemuddinn",
      "name": "TheAce",
      "url": "https://github.com/jaseemuddinn/TheAce",
      "description": "",
      "readme_content": "React + Vite\nThis template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.\nCurrently, two official plugins are available:\n\n@vitejs/plugin-react uses Babel for Fast Refresh\n@vitejs/plugin-react-swc uses SWC for Fast Refresh",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T13:02:52.871128"
    },
    {
      "owner": "jaseemuddinn",
      "name": "calenderApp",
      "url": "https://github.com/jaseemuddinn/calenderApp",
      "description": "",
      "readme_content": "This is a [Next.js](https://nextjs.org) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).\n\n## Getting Started\n\nFirst, run the development server:\n\n```bash\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\n```\n\nOpen [http://localhost:3000](http://localhost:3000) with your browser to see the result.\n\nYou can start editing the page by modifying `app/page.js`. The page auto-updates as you edit the file.\n\nThis project uses [`next/font`](https://nextjs.org/docs/app/building-your-application/optimizing/fonts) to automatically optimize and load [Geist](https://vercel.com/font), a new font family for Vercel.\n\n## Learn More\n\nTo learn more about Next.js, take a look at the following resources:\n\n- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.\n- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.\n\nYou can check out [the Next.js GitHub repository](https://github.com/vercel/next.js) - your feedback and contributions are welcome!\n\n## Deploy on Vercel\n\nThe easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.\n\nCheck out our [Next.js deployment documentation](https://nextjs.org/docs/app/building-your-application/deploying) for more details.\n# calenderApp",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T13:02:53.547052"
    },
    {
      "owner": "jaseemuddinn",
      "name": "onnoff",
      "url": "https://github.com/jaseemuddinn/onnoff",
      "description": "",
      "readme_content": "# React + Vite\n\nThis template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.\n\nCurrently, two official plugins are available:\n\n- [@vitejs/plugin-react](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react/README.md) uses [Babel](https://babeljs.io/) for Fast Refresh\n- [@vitejs/plugin-react-swc](https://github.com/vitejs/vite-plugin-react-swc) uses [SWC](https://swc.rs/) for Fast Refresh\n# onnoff",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T13:02:54.265153"
    },
    {
      "owner": "jaseemuddinn",
      "name": "face_recog",
      "url": "https://github.com/jaseemuddinn/face_recog",
      "description": "",
      "readme_content": "This is a [Next.js](https://nextjs.org/) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).\n\n## Getting Started\n\nFirst, run the development server:\n\n```bash\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\n```\n\nOpen [http://localhost:3000](http://localhost:3000) with your browser to see the result.\n\nYou can start editing the page by modifying `app/page.js`. The page auto-updates as you edit the file.\n\nThis project uses [`next/font`](https://nextjs.org/docs/basic-features/font-optimization) to automatically optimize and load Inter, a custom Google Font.\n\n## Learn More\n\nTo learn more about Next.js, take a look at the following resources:\n\n- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.\n- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.\n\nYou can check out [the Next.js GitHub repository](https://github.com/vercel/next.js/) - your feedback and contributions are welcome!\n\n## Deploy on Vercel\n\nThe easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.\n\nCheck out our [Next.js deployment documentation](https://nextjs.org/docs/deployment) for more details.\n# face_recog",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T13:02:54.942298"
    },
    {
      "owner": "jaseemuddinn",
      "name": "theace_v2",
      "url": "https://github.com/jaseemuddinn/theace_v2",
      "description": "Under Development",
      "readme_content": "This is a Next.js project bootstrapped with create-next-app.\nGetting Started\nFirst, run the development server:\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\nOpen http://localhost:3000 with your browser to see the result.\nYou can start editing the page by modifying app/page.js. The page auto-updates as you edit the file.\nThis project uses next/font to automatically optimize and load Inter, a custom Google Font.\nLearn More\nTo learn more about Next.js, take a look at the following resources:\n\nNext.js Documentation - learn about Next.js features and API.\nLearn Next.js - an interactive Next.js tutorial.\n\nYou can check out the Next.js GitHub repository - your feedback and contributions are welcome!\nDeploy on Vercel\nThe easiest way to deploy your Next.js app is to use the Vercel Platform from the creators of Next.js.\nCheck out our Next.js deployment documentation for more details.",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T13:02:55.611905"
    },
    {
      "owner": "jaseemuddinn",
      "name": "Edulnnova",
      "url": "https://github.com/jaseemuddinn/Edulnnova",
      "description": "",
      "readme_content": "React + Vite\nThis template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.\nCurrently, two official plugins are available:\n\n@vitejs/plugin-react uses Babel for Fast Refresh\n@vitejs/plugin-react-swc uses SWC for Fast Refresh",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T13:02:56.309913"
    },
    {
      "owner": "jaseemuddinn",
      "name": "VeraLink",
      "url": "https://github.com/jaseemuddinn/VeraLink",
      "description": "",
      "readme_content": "",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T13:02:57.066188"
    },
    {
      "owner": "jaseemuddinn",
      "name": "pdsalon",
      "url": "https://github.com/jaseemuddinn/pdsalon",
      "description": "",
      "readme_content": "React + Vite\nThis template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.\nCurrently, two official plugins are available:\n\n@vitejs/plugin-react uses Babel for Fast Refresh\n@vitejs/plugin-react-swc uses SWC for Fast Refresh",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T13:02:57.749018"
    },
    {
      "owner": "jaseemuddinn",
      "name": "irada_revamped",
      "url": "https://github.com/jaseemuddinn/irada_revamped",
      "description": "",
      "readme_content": "React + Vite\nThis template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.\nCurrently, two official plugins are available:\n\n@vitejs/plugin-react uses Babel for Fast Refresh\n@vitejs/plugin-react-swc uses SWC for Fast Refresh",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T13:02:58.523216"
    },
    {
      "owner": "404avinotfound",
      "name": "Coding-and-Decoding",
      "url": "https://github.com/404avinotfound/Coding-and-Decoding",
      "description": "Project on Coding Decoding",
      "readme_content": "Coding-and-Decoding\nProject on Coding Decoding",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T13:05:15.130099"
    },
    {
      "owner": "dagger",
      "name": "dagger",
      "url": "https://github.com/dagger/dagger",
      "description": "An engine to run your pipelines in containers",
      "readme_content": "What is Dagger?\nDagger is a tool that lets you replace your software project's artisanal scripts with a modern API and cross-language scripting engine.\n\nEncapsulate all your project's tasks and workflows into simple functions, written in your programming language of choice\nDagger packages your functions into a custom GraphQL API\nRun your functions from the CLI, your language interpreter, or a custom HTTP client\nPackage your functions into a module, to reuse in your next project or share with the community\nSearch the Daggerverse for existing modules, and import them into yours. All Dagger modules can reuse each other's functions - across language.\n\nBenefits to app teams\n\nReduce complexity: even complex builds can be expressed as a few simple functions\nNo more \"push and pray\": everything CI can do, your dev environment can do too\nUse the same language to develop your app and its scripts\nEasy onboarding of new developers: if you can build, test and deploy - they can too.\nEverything is cached by default: expect 2x to 10x speedups\nParity between dev and CI environments\nCross-team collaboration: reuse another team's workflows without learning their stack\n\nBenefits to platform teams\n\nReduce CI lock-in. Dagger functions run on all major CI platforms - no proprietary DSL needed.\nDon't be a bottleneck. Let app teams write their own functions. Enable standardization by providing them a library of reusable components.\nFaster CI runs. CI pipelines that are \"Daggerized\" typically run 2x to 10x faster, thanks to caching and concurrency. This means developers waste less time waiting for CI, and you spend less money on CI compute.\nA viable platform strategy. App teams need flexibility, and you need control. Dagger gives you a way to reconcile the two, in an incremental way that leverages the stack you already have.\n\nLearn more\n\nHow does it work?\nQuickstart\nCookbook\n\nJoin the community\n\nJoin the Dagger community server\nFollow us on Twitter\nCheck out our community activities\nRead more in our documentation\n\nContributing\nInterested in contributing or building dagger from scratch? See\nCONTRIBUTING.md.",
      "languages": {},
      "topics": [
        "GraphQL"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T16:24:23.800232"
    },
    {
      "owner": "evcc-io",
      "name": "evcc",
      "url": "https://github.com/evcc-io/evcc",
      "description": "Solar Charging ‚òÄÔ∏èüöò",
      "readme_content": "evcc üöò‚òÄÔ∏è\n\n\n\n\n\nevcc is an extensible EV Charge Controller and home energy management system. Featured in PV magazine.\n\nFeatures\n\nsimple and clean user interface\nwide range of supported chargers:\n\nABL eMH1, Alfen (Eve), Bender (CC612/613), cFos (PowerBrain), Daheimladen, Ebee (Wallbox), Ensto (Chago Wallbox), EVSEWifi/ smartWB, Garo (GLB, GLB+, LS4), go-eCharger, HardyBarth (eCB1, cPH1, cPH2), Heidelberg (Energy Control), Innogy (eBox), Juice (Charger Me), KEBA/BMW, Mennekes (Amedio, Amtron Premium/Xtra, Amtron ChargeConrol), older NRGkicks (before 2022/2023), NRGKick Gen2,openWB (includes Pro), Optec (Mobility One), PC Electric (includes Garo), Siemens, TechniSat (Technivolt), Tinkerforge Warp Charger, Ubitricity (Heinz), Vestel, Wallbe, Webasto (Live), Mobile Charger Connect and many more\nEEBus support (Elli, PMCC)\nexperimental OCPP support\nBuild-your-own: Phoenix Contact (includes ESL Walli), EVSE DIN\nSmart-Home outlets: FritzDECT, Shelly, Tasmota, TP-Link\n\n\nwide range of supported meters for grid, pv, battery and charger:\n\nModBus: Eastron SDM, MPM3PM, ORNO WE, SBC ALE3 and many more, see https://github.com/volkszaehler/mbmd#supported-devices for a complete list\nIntegrated systems: SMA Sunny Home Manager and Energy Meter, KOSTAL Smart Energy Meter (KSEM, EMxx)\nSunspec-compatible inverter or home battery devices: Fronius, SMA, SolarEdge, KOSTAL, STECA, E3DC, ...\nand various others: Discovergy, Tesla PowerWall, LG ESS HOME, OpenEMS (FENECON)\n\n\nvehicle integration (state of charge, remote charge, battery and preconditioning status):\n\nAudi, BMW, Citro√´n, Dacia, Fiat, Ford, Hyundai, Jaguar, Kia, Landrover, Mercedes-Benz, Mini, Nissan, Opel, Peugeot, Porsche, Renault, Seat, Smart, Skoda, Tesla, Volkswagen, Volvo, ...\nServices: OVMS, Tronity\nScooters: Niu, Silence\n\n\nplugins for integrating with any charger/ meter/ vehicle:\n\nModbus, HTTP, MQTT, Javascript, WebSockets and shell scripts\n\n\nstatus notifications using Telegram, PushOver and many more\nlogging using InfluxDB and Grafana\ngranular charge power control down to mA steps with supported chargers (labeled by e.g. smartWB as OLC)\nREST and MQTT APIs for integration with home automation systems\nAdd-ons for Home Assistant and OpenHAB (not maintained by the evcc core team)\n\nGetting Started\nYou'll find everything you need in our documentation.\nContributing\nTechnical details on how to contribute, how to add translations and how to build evcc from source can be found here.\n\nSponsorship\n\nevcc believes in open source software. We're committed to provide best in class EV charging experience.\nMaintaining evcc consumes time and effort. With the vast amount of different devices to support, we depend on community and vendor support to keep evcc alive.\nWhile evcc is open source, we would also like to encourage vendors to provide open source hardware devices, public documentation and support open source projects like ours that provide additional value to otherwise closed hardware. Where this is not the case, evcc requires \"sponsor token\" to finance ongoing development and support of evcc.\nLearn more about our sponsorship model.",
      "languages": {
        "javascript": 1,
        "JavaScript": 1,
        "go": 1,
        "Shell": 1,
        "Go": 1,
        "GO": 1
      },
      "topics": [
        "Go",
        "JavaScript",
        "Shell"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T16:24:24.662348"
    },
    {
      "owner": "cilium",
      "name": "cilium",
      "url": "https://github.com/cilium/cilium",
      "description": "eBPF-based Networking, Security, and Observability",
      "readme_content": "Cilium is a networking, observability, and security solution with an eBPF-based\ndataplane. It provides a simple flat Layer 3 network with the ability to span\nmultiple clusters in either a native routing or overlay mode. It is L7-protocol\naware and can enforce network policies on L3-L7 using an identity based security\nmodel that is decoupled from network addressing.\nCilium implements distributed load balancing for traffic between pods and to\nexternal services, and is able to fully replace kube-proxy, using efficient\nhash tables in eBPF allowing for almost unlimited scale. It also supports\nadvanced functionality like integrated ingress and egress gateway, bandwidth\nmanagement and service mesh, and provides deep network and security visibility and monitoring.\nA new Linux kernel technology called eBPF is at the foundation of Cilium. It\nsupports dynamic insertion of eBPF bytecode into the Linux kernel at various\nintegration points such as: network IO, application sockets, and tracepoints to\nimplement security, networking and visibility logic. eBPF is highly efficient\nand flexible. To learn more about eBPF, visit eBPF.io.\n\n\n\n\n\n\n\n\n\n\n\n\n\nStable Releases\nThe Cilium community maintains minor stable releases for the last three minor\nCilium versions. Older Cilium stable versions from minor releases prior to that\nare considered EOL.\nFor upgrades to new minor releases please consult the Cilium Upgrade Guide.\nListed below are the actively maintained release branches along with their latest\npatch release, corresponding image pull tags and their release notes:\n\n\nv1.17\n2025-02-12\nquay.io/cilium/cilium:v1.17.1\nRelease Notes\n\nv1.16\n2025-02-13\nquay.io/cilium/cilium:v1.16.7\nRelease Notes\n\nv1.15\n2025-02-18\nquay.io/cilium/cilium:v1.15.14\nRelease Notes\n\n\n\n\nArchitectures\nCilium images are distributed for AMD64 and AArch64 architectures.\n\nSoftware Bill of Materials\nStarting with Cilium version 1.13.0, all images include a Software Bill of\nMaterials (SBOM). The SBOM is generated in SPDX format. More information\non this is available on Cilium SBOM.\n\nDevelopment\nFor development and testing purpose, the Cilium community publishes snapshots,\nearly release candidates (RC) and CI container images build from the main\nbranch. These images are\nnot for use in production.\nFor testing upgrades to new development releases please consult the latest\ndevelopment build of the Cilium Upgrade Guide.\nListed below are branches for testing along with their snapshots or RC releases,\ncorresponding image pull tags and their release notes where applicable:\n\n\nmain\ndaily\nquay.io/cilium/cilium-ci:latest\nN/A\n\nv1.17.0-rc.2\n2025-01-24\nquay.io/cilium/cilium:v1.17.0-rc.2\nPre Release Candidate Notes\n\n\n\n\nFunctionality Overview\n\nProtect and secure APIs transparently\nAbility to secure modern application protocols such as REST/HTTP, gRPC and\nKafka. Traditional firewalls operate at Layer 3 and 4. A protocol running on a\nparticular port is either completely trusted or blocked entirely. Cilium\nprovides the ability to filter on individual application protocol requests such\nas:\n\nAllow all HTTP requests with method GET and path /public/.*. Deny all\nother requests.\nAllow service1 to produce on Kafka topic topic1 and service2 to\nconsume on topic1. Reject all other Kafka messages.\nRequire the HTTP header X-Token: [0-9]+ to be present in all REST calls.\n\nSee the section Layer 7 Policy in our documentation for the latest list of\nsupported protocols and examples on how to use it.\n\nSecure service to service communication based on identities\nModern distributed applications rely on technologies such as application\ncontainers to facilitate agility in deployment and scale out on demand. This\nresults in a large number of application containers being started in a short\nperiod of time. Typical container firewalls secure workloads by filtering on\nsource IP addresses and destination ports. This concept requires the firewalls\non all servers to be manipulated whenever a container is started anywhere in\nthe cluster.\nIn order to avoid this situation which limits scale, Cilium assigns a security\nidentity to groups of application containers which share identical security\npolicies. The identity is then associated with all network packets emitted by\nthe application containers, allowing to validate the identity at the receiving\nnode. Security identity management is performed using a key-value store.\n\nSecure access to and from external services\nLabel based security is the tool of choice for cluster internal access control.\nIn order to secure access to and from external services, traditional CIDR based\nsecurity policies for both ingress and egress are supported. This allows to\nlimit access to and from application containers to particular IP ranges.\n\nSimple Networking\nA simple flat Layer 3 network with the ability to span multiple clusters\nconnects all application containers. IP allocation is kept simple by using host\nscope allocators. This means that each host can allocate IPs without any\ncoordination between hosts.\nThe following multi node networking models are supported:\n\nOverlay: Encapsulation-based virtual network spanning all hosts.\nCurrently, VXLAN and Geneve are baked in but all encapsulation formats\nsupported by Linux can be enabled.\nWhen to use this mode: This mode has minimal infrastructure and integration\nrequirements. It works on almost any network infrastructure as the only\nrequirement is IP connectivity between hosts which is typically already\ngiven.\n\nNative Routing: Use of the regular routing table of the Linux host.\nThe network is required to be capable to route the IP addresses of the\napplication containers.\nWhen to use this mode: This mode is for advanced users and requires some\nawareness of the underlying networking infrastructure. This mode works well\nwith:\n\nNative IPv6 networks\nIn conjunction with cloud network routers\nIf you are already running routing daemons\n\n\n\n\nLoad Balancing\nCilium implements distributed load balancing for traffic between application\ncontainers and to external services and is able to fully replace components\nsuch as kube-proxy. The load balancing is implemented in eBPF using efficient\nhashtables allowing for almost unlimited scale.\nFor north-south type load balancing, Cilium's eBPF implementation is optimized\nfor maximum performance, can be attached to XDP (eXpress Data Path), and supports\ndirect server return (DSR) as well as Maglev consistent hashing if the load\nbalancing operation is not performed on the source host.\nFor east-west type load balancing, Cilium performs efficient service-to-backend\ntranslation right in the Linux kernel's socket layer (e.g. at TCP connect time)\nsuch that per-packet NAT operations overhead can be avoided in lower layers.\n\nBandwidth Management\nCilium implements bandwidth management through efficient EDT-based (Earliest Departure\nTime) rate-limiting with eBPF for container traffic that is egressing a node. This\nallows to significantly reduce transmission tail latencies for applications and to\navoid locking under multi-queue NICs compared to traditional approaches such as HTB\n(Hierarchy Token Bucket) or TBF (Token Bucket Filter) as used in the bandwidth CNI\nplugin, for example.\n\nMonitoring and Troubleshooting\nThe ability to gain visibility and troubleshoot issues is fundamental to the\noperation of any distributed system. While we learned to love tools like\ntcpdump and ping and while they will always find a special place in our\nhearts, we strive to provide better tooling for troubleshooting. This includes\ntooling to provide:\n\nEvent monitoring with metadata: When a packet is dropped, the tool doesn't\njust report the source and destination IP of the packet, the tool provides\nthe full label information of both the sender and receiver among a lot of\nother information.\nMetrics export via Prometheus: Key metrics are exported via Prometheus for\nintegration with your existing dashboards.\nHubble: An observability platform specifically written for Cilium. It\nprovides service dependency maps, operational monitoring and alerting,\nand application and security visibility based on flow logs.\n\n\nGetting Started\n\nWhy Cilium?\nGetting Started\nArchitecture and Concepts\nInstalling Cilium\nFrequently Asked Questions\nContributing\n\n\nCommunity\n\nSlack\nJoin the Cilium Slack channel to chat with\nCilium developers and other Cilium users. This is a good place to learn about\nCilium, ask questions, and share your experiences.\n\nSpecial Interest Groups (SIG)\nSee Special Interest groups for a list of all SIGs and their meeting times.\n\nDeveloper meetings\nThe Cilium developer community hangs out on Zoom to chat. Everybody is welcome.\n\nWeekly, Wednesday,\n5:00 pm Europe/Zurich time (CET/CEST),\nusually equivalent to 8:00 am PT, or 11:00 am ET. Meeting Notes and Zoom Info\nThird Wednesday of each month, 9:00 am Japan time (JST). APAC Meeting Notes and Zoom Info\n\n\neBPF & Cilium Office Hours livestream\nWe host a weekly community YouTube livestream called eCHO which (very loosely!) stands for eBPF & Cilium Office Hours. Join us live, catch up with past episodes, or head over to the eCHO repo and let us know your ideas for topics we should cover.\n\nGovernance\nThe Cilium project is governed by a group of Maintainers and Committers.\nHow they are selected and govern is outlined in our governance document.\n\nAdopters\nA list of adopters of the Cilium project who are deploying it in production, and of their use cases,\ncan be found in file USERS.md.\n\nLicense\nThe Cilium user space components are licensed under the\nApache License, Version 2.0.\nThe BPF code templates are dual-licensed under the\nGeneral Public License, Version 2.0 (only)\nand the 2-Clause BSD License\n(you can use the terms of either license, at your option).",
      "languages": {},
      "topics": [
        "gRPC"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T16:24:25.555111"
    },
    {
      "owner": "veops",
      "name": "oneterm",
      "url": "https://github.com/veops/oneterm",
      "description": "Provide secure access and control over all infrastructure",
      "readme_content": "A Simple, Lightweight, Flexible Bastion Host.\n\n\n\n\n\n  English ¬∑ ‰∏≠Êñá(ÁÆÄ‰Ωì)\n\nWhat is OneTerm\nOneTerm is a simple, lightweight and flexible enterprise-class bastion host, designed and developed based on 4A compliant, i.e. Authen, Authorize, Account, and Audit, which ensures the security and compliance of the system through strict access control and monitoring features.\n\nProduct documentÔºöhttps://veops.cn/docs/docs/oneterm/onterm_design\nPreview onlineÔºöOneTerm\n\nusername: demo or admin\npassword: 123456\n\n\nATTENTION: branch main may be unstable as the result of continued development, Please use releases to get the latest stable version\n\nCore Feature\n\n\nAccess control: Acting as an intermediary, OneTerm restricts direct access to critical systems. Users must authenticate through OneTerm before accessing other servers or systems.\n\n\nSecurity audit: OneTerm can record user logins and activities, providing audit logs for investigation in case of security incidents. This ensures that every user's actions are traceable and auditable.\n\n\nJump access to: OneTerm offers a jump host mechanism, allowing users to connect to other internal servers through OneTerm. This helps reduce the risk of exposing internal servers directly to the outside, as only OneTerm needs to be accessible externally.\n\n\nPassword management: OneTerm can enforce robust password policies and centrally manage passwords through a single entry point. This helps improve the overall system's password security.\n\n\nSession recording: OneTerm can record user sessions with servers, which is valuable for monitoring and investigating privileged user activities. In case of security incidents, session recordings can be replayed to understand detailed operations.\n\n\nPrevent direct attacks: Since OneTerm is the sole entry point for systems and resources, it can serve as a primary obstacle for attackers. This helps reduce the risk of direct attacks on internal systems.\n\n\nUnified access: OneTerm provides a single entry point through which users can access different systems without needing to remember multiple login credentials. This enhances user convenience and work efficiency.\n\n\nProduct Advantage\n\nAuthentication and Authorization: Authentication and Authorization: OneTerm should have a robust and flexible identity authentication and authorization mechanism. This includes supporting multi-factor authentication to ensure that only authorized users can access internal network resources and enabling fine-grained management of user permissions.\nSecure communication: OneTerm supports secure communication protocols and encryption technologies to protect data transmission between users and internal servers. This helps prevent man-in-the-middle attacks and data leakage.\nAudit and monitoring: OneTerm features powerful audit and monitoring capabilities, recording user activities and generating audit logs. This helps trace security incidents, identify potential threats, and meet compliance requirements.\nRemote Management and Session Isolation: OneTerm supports remote management, allowing administrators to securely manage internal servers. Additionally, it should have session isolation functionality to ensure that access between users is isolated from each other, preventing lateral movement attacks.\nCombination with open source CMDB: Oneterm is combined with VE CMDB (which has been open source), users can import assets in CMDB with one click, ensuring easy operation and smooth process.\n\nTech Stack\n\nBack-end: Go\nFront-end: Vue.js\nUI component library: Ant Design Vue\n\nGetting started & staying tuned with us\nStar us, and you will receive all releases notifications from GitHub without any delay!\n\nOverview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuick Start\n\ndocker-compose install\ngit clone https://github.com/veops/oneterm.git\ncd oneterm/deploy\ndocker compose up -d\n\nvisit\n\nOpen your browser and visit: http://127.0.0.1:8666\nUsername: admin\nPassword: 123456\n\n\n\nContributing\nWe welcome all developers to contribute code to improve and extend this project. Please read our contribution guidelines first. Additionally, you can support Veops open source through social media, events, and sharing.\n\n\n\nMore Open Source\n\nCMDB: Simple, lightweight, and versatile operational CMDB\nACL: A general permission control management system.\nmessenger: A simple and lightweight message sending service.\n\nCommunity\n\nEmail: bd@veops.cn\nWeChat official account: Welcome to follow our WeChat official account and join our group channels",
      "languages": {
        "Go": 1,
        "go": 1,
        "GO": 1
      },
      "topics": [
        "Go",
        "Docker"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T16:24:26.360520"
    },
    {
      "owner": "moby",
      "name": "moby",
      "url": "https://github.com/moby/moby",
      "description": "The Moby Project - a collaborative project for the container ecosystem to assemble container-based systems",
      "readme_content": "The Moby Project\n\n\n\n\nMoby is an open-source project created by Docker to enable and accelerate software containerization.\nIt provides a \"Lego set\" of toolkit components, the framework for assembling them into custom container-based systems, and a place for all container enthusiasts and professionals to experiment and exchange ideas.\nComponents include container build tools, a container registry, orchestration tools, a runtime and more, and these can be used as building blocks in conjunction with other tools and projects.\nPrinciples\nMoby is an open project guided by strong principles, aiming to be modular, flexible and without too strong an opinion on user experience.\nIt is open to the community to help set its direction.\n\nModular: the project includes lots of components that have well-defined functions and APIs that work together.\nBatteries included but swappable: Moby includes enough components to build fully featured container systems, but its modular architecture ensures that most of the components can be swapped by different implementations.\nUsable security: Moby provides secure defaults without compromising usability.\nDeveloper focused: The APIs are intended to be functional and useful to build powerful tools.\nThey are not necessarily intended as end user tools but as components aimed at developers.\nDocumentation and UX is aimed at developers not end users.\n\nAudience\nThe Moby Project is intended for engineers, integrators and enthusiasts looking to modify, hack, fix, experiment, invent and build systems based on containers.\nIt is not for people looking for a commercially supported system, but for people who want to work and learn with open source code.\nRelationship with Docker\nThe components and tools in the Moby Project are initially the open source components that Docker and the community have built for the Docker Project.\nNew projects can be added if they fit with the community goals. Docker is committed to using Moby as the upstream for the Docker Product.\nHowever, other projects are also encouraged to use Moby as an upstream, and to reuse the components in diverse ways, and all these uses will be treated in the same way. External maintainers and contributors are welcomed.\nThe Moby project is not intended as a location for support or feature requests for Docker products, but as a place for contributors to work on open source code, fix bugs, and make the code more useful.\nThe releases are supported by the maintainers, community and users, on a best efforts basis only. For customers who want enterprise or commercial support, Docker Desktop and Mirantis Container Runtime are the appropriate products for these use cases.\n\nLegal\nBrought to you courtesy of our legal counsel. For more context,\nplease see the NOTICE document in this repo.\nUse and transfer of Moby may be subject to certain restrictions by the\nUnited States and other governments.\nIt is your responsibility to ensure that your use and/or transfer does not\nviolate applicable laws.\nFor more information, please see https://www.bis.doc.gov\nLicensing\nMoby is licensed under the Apache License, Version 2.0. See\nLICENSE for the full\nlicense text.",
      "languages": {},
      "topics": [
        "Docker"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T16:24:31.866482"
    },
    {
      "owner": "beego",
      "name": "beego",
      "url": "https://github.com/beego/beego",
      "description": "beego is an open-source, high-performance web framework for the Go programming language.",
      "readme_content": "Beego   \nBeego is used for rapid development of enterprise application in Go, including RESTful APIs, web apps and backend services.\nIt is inspired by Tornado, Sinatra and Flask. beego has some Go-specific features such as interfaces and struct embedding.\nQuick Start\n\nNew Doc Website - unavailable\nNew Doc Website Backup @flycash\nNew Doc Website source code\nOld Doc - github\nExample\n\n\nKindly remind that sometimes the HTTPS certificate is expired, you may get some NOT SECURE warning\n\nWeb Application\nCreate hello directory, cd hello directory\nmkdir hello\ncd hello\n\nInit module\ngo mod init\n\nDownload and install\ngo get github.com/beego/beego/v2@latest\n\nCreate file hello.go\npackage main\n\nimport \"github.com/beego/beego/v2/server/web\"\n\nfunc main() {\n\tweb.Run()\n}\nDownload required dependencies\ngo mod tidy\n\nBuild and run\ngo build hello.go\n./hello\n\nGo to http://localhost:8080\nCongratulations! You've just built your first beego app.\nFeatures\n\nRESTful support\nMVC architecture\nModularity\nAuto API documents\nAnnotation router\nNamespace\nPowerful development tools\nFull stack for Web & API\n\nModules\n\norm\nsession\nlogs\nconfig\ncache\ncontext\nadmin\nhttplib\ntask\ni18n\n\nCommunity\n\nWelcome to join us in Slack: https://beego.slack.com invite,\nQQ Group ID:523992905\nContribution Guide.\n\nLicense\nbeego source code is licensed under the Apache Licence, Version 2.0\n(https://www.apache.org/licenses/LICENSE-2.0.html).",
      "languages": {
        "HTML": 1,
        "Go": 1,
        "go": 1,
        "GO": 1
      },
      "topics": [
        "Go",
        "RESTful APIs"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T16:24:32.817351"
    },
    {
      "owner": "junegunn",
      "name": "fzf",
      "url": "https://github.com/junegunn/fzf",
      "description": "üå∏ A command-line fuzzy finder",
      "readme_content": "Special thanks to:\n\n\n\n\n\nWarp, the intelligent terminal for developers\nAvailable for MacOS and Linux\n\n\n \nfzf is a general-purpose command-line fuzzy finder.\n\nIt's an interactive filter program for any kind of list; files, command\nhistory, processes, hostnames, bookmarks, git commits, etc. It implements\na \"fuzzy\" matching algorithm, so you can quickly type in patterns with omitted\ncharacters and still get the results you want.\nHighlights\n\nüì¶ Portable ‚Äî Distributed as a single binary for easy installation\n‚ö° Blazingly fast ‚Äî Highly optimized code instantly processes millions of items\nüõ†Ô∏è Extremely versatile ‚Äî Fully customizable via an event-action binding mechanism\nüîã Batteries included ‚Äî Includes integration with bash, zsh, fish, Vim, and Neovim\n\nSponsors ‚ù§Ô∏è\nI would like to thank all the sponsors of this project who make it possible for me to continue to improve fzf.\nIf you'd like to sponsor this project, please visit https://github.com/sponsors/junegunn.\n\nTable of Contents\n\nInstallation\n\nUsing Homebrew\nLinux packages\nWindows packages\nUsing git\nBinary releases\nSetting up shell integration\nVim/Neovim plugin\n\n\nUpgrading fzf\nBuilding fzf\nUsage\n\nUsing the finder\nDisplay modes\n\n--height mode\n--tmux mode\n\n\nSearch syntax\nEnvironment variables\nCustomizing the look\nOptions\nDemo\n\n\nExamples\nKey bindings for command-line\nFuzzy completion for bash and zsh\n\nFiles and directories\nProcess IDs\nHost names\nEnvironment variables / Aliases\nCustomizing fzf options for completion\nCustomizing completion source for paths and directories\nSupported commands\nCustom fuzzy completion\n\n\nVim plugin\nAdvanced topics\n\nCustomizing for different types of input\nPerformance\nExecuting external programs\nTurning into a different process\nReloading the candidate list\n\n1. Update the list of processes by pressing CTRL-R\n2. Switch between sources by pressing CTRL-D or CTRL-F\n3. Interactive ripgrep integration\n\n\nPreview window\nPreviewing an image\n\n\nTips\n\nRespecting .gitignore\nFish shell\nfzf Theme Playground\n\n\nRelated projects\nLicense\n\nInstallation\nUsing Homebrew\nYou can use Homebrew (on macOS or Linux) to install fzf.\nbrew install fzf\nImportantTo set up shell integration (key bindings and fuzzy completion),\nsee the instructions below.\n\nfzf is also available via MacPorts: sudo port install fzf\nLinux packages\n\n\n\nPackage Manager\nLinux Distribution\nCommand\n\n\n\n\nAPK\nAlpine Linux\nsudo apk add fzf\n\n\nAPT\nDebian 9+/Ubuntu 19.10+\nsudo apt install fzf\n\n\nConda\n\nconda install -c conda-forge fzf\n\n\nDNF\nFedora\nsudo dnf install fzf\n\n\nNix\nNixOS, etc.\nnix-env -iA nixpkgs.fzf\n\n\nPacman\nArch Linux\nsudo pacman -S fzf\n\n\npkg\nFreeBSD\npkg install fzf\n\n\npkgin\nNetBSD\npkgin install fzf\n\n\npkg_add\nOpenBSD\npkg_add fzf\n\n\nPortage\nGentoo\nemerge --ask app-shells/fzf\n\n\nSpack\n\nspack install fzf\n\n\nXBPS\nVoid Linux\nsudo xbps-install -S fzf\n\n\nZypper\nopenSUSE\nsudo zypper install fzf\n\n\n\nImportantTo set up shell integration (key bindings and fuzzy completion),\nsee the instructions below.\n\n\nWindows packages\nOn Windows, fzf is available via Chocolatey, Scoop,\nWinget, and MSYS2:\n\n\n\nPackage manager\nCommand\n\n\n\n\nChocolatey\nchoco install fzf\n\n\nScoop\nscoop install fzf\n\n\nWinget\nwinget install fzf\n\n\nMSYS2 (pacman)\npacman -S $MINGW_PACKAGE_PREFIX-fzf\n\n\n\nUsing git\nAlternatively, you can \"git clone\" this repository to any directory and run\ninstall script.\ngit clone --depth 1 https://github.com/junegunn/fzf.git ~/.fzf\n~/.fzf/install\nThe install script will add lines to your shell configuration file to modify\n$PATH and set up shell integration.\nBinary releases\nYou can download the official fzf binaries from the releases page.\n\nhttps://github.com/junegunn/fzf/releases\n\nSetting up shell integration\nAdd the following line to your shell configuration file.\n\nbash\n# Set up fzf key bindings and fuzzy completion\neval \"$(fzf --bash)\"\n\nzsh\n# Set up fzf key bindings and fuzzy completion\nsource <(fzf --zsh)\n\nfish\n# Set up fzf key bindings\nfzf --fish | source\n\n\nNote--bash, --zsh, and --fish options are only available in fzf 0.48.0 or\nlater. If you have an older version of fzf, or want finer control, you can\nsource individual script files in the /shell directory. The\nlocation of the files may vary depending on the package manager you use.\nPlease refer to the package documentation for more information.\n(e.g. apt show fzf)\n\nTipYou can disable CTRL-T or ALT-C binding by setting FZF_CTRL_T_COMMAND or\nFZF_ALT_C_COMMAND to an empty string when sourcing the script.\nFor example, to disable ALT-C binding:\n\nbash: FZF_ALT_C_COMMAND= eval \"$(fzf --bash)\"\nzsh: FZF_ALT_C_COMMAND= source <(fzf --zsh)\nfish: fzf --fish | FZF_ALT_C_COMMAND= source\n\nSetting the variables after sourcing the script will have no effect.\n\nVim/Neovim plugin\nIf you use vim-plug, add this to\nyour Vim configuration file:\nPlug 'junegunn/fzf', { 'do': { -> fzf#install() } }\nPlug 'junegunn/fzf.vim'\n\njunegunn/fzf provides the basic library functions\n\nfzf#install() makes sure that you have the latest binary\n\n\njunegunn/fzf.vim is a separate project\nthat provides a variety of useful commands\n\nTo learn more about the Vim integration, see README-VIM.md.\nTipIf you use Neovim and prefer Lua-based plugins, check out\nfzf-lua.\n\nUpgrading fzf\nfzf is being actively developed, and you might want to upgrade it once in a\nwhile. Please follow the instruction below depending on the installation\nmethod used.\n\ngit: cd ~/.fzf && git pull && ./install\nbrew: brew update; brew upgrade fzf\nmacports: sudo port upgrade fzf\nchocolatey: choco upgrade fzf\nvim-plug: :PlugUpdate fzf\n\nBuilding fzf\nSee BUILD.md.\nUsage\nfzf will launch interactive finder, read the list from STDIN, and write the\nselected item to STDOUT.\nfind * -type f | fzf > selected\nWithout STDIN pipe, fzf will traverse the file system under the current\ndirectory to get the list of files.\nvim $(fzf)\nNoteYou can override the default behavior\n\nEither by setting $FZF_DEFAULT_COMMAND to a command that generates the desired list\nOr by setting --walker, --walker-root, and --walker-skip options in $FZF_DEFAULT_OPTS\n\n\nWarningA more robust solution would be to use xargs but we've presented\nthe above as it's easier to grasp\nfzf --print0 | xargs -0 -o vim\n\nTipfzf also has the ability to turn itself into a different process.\nfzf --bind 'enter:become(vim {})'\nSee Turning into a different process\nfor more information.\n\nUsing the finder\n\nCTRL-K / CTRL-J (or CTRL-P / CTRL-N) to move cursor up and down\nEnter key to select the item, CTRL-C / CTRL-G / ESC to exit\nOn multi-select mode (-m), TAB and Shift-TAB to mark multiple items\nEmacs style key bindings\nMouse: scroll, click, double-click; shift-click and shift-scroll on\nmulti-select mode\n\nDisplay modes\nfzf by default runs in fullscreen mode, but there are other display modes.\n--height mode\nWith --height HEIGHT[%], fzf will start below the cursor with the given height.\nfzf --height 40%\nreverse layout and --border goes well with this option.\nfzf --height 40% --layout reverse --border\nBy prepending ~ to the height, you're setting the maximum height.\n# Will take as few lines as possible to display the list\nseq 3 | fzf --height ~100%\nseq 3000 | fzf --height ~100%\nHeight value can be a negative number.\n# Screen height - 3\nfzf --height -3\n--tmux mode\nWith --tmux option, fzf will start in a tmux popup.\n# --tmux [center|top|bottom|left|right][,SIZE[%]][,SIZE[%][,border-native]]\n\nfzf --tmux center         # Center, 50% width and height\nfzf --tmux 80%            # Center, 80% width and height\nfzf --tmux 100%,50%       # Center, 100% width and 50% height\nfzf --tmux left,40%       # Left, 40% width\nfzf --tmux left,40%,90%   # Left, 40% width, 90% height\nfzf --tmux top,40%        # Top, 40% height\nfzf --tmux bottom,80%,40% # Bottom, 80% height, 40% height\n--tmux is silently ignored when you're not on tmux.\nNoteIf you're stuck with an old version of tmux that doesn't support popup,\nor if you want to open fzf in a regular tmux pane, check out\nfzf-tmux script.\n\nTipYou can add these options to $FZF_DEFAULT_OPTS so that they're applied by\ndefault. For example,\n# Open in tmux popup if on tmux, otherwise use --height mode\nexport FZF_DEFAULT_OPTS='--height 40% --tmux bottom,40% --layout reverse --border top'\n\nSearch syntax\nUnless otherwise specified, fzf starts in \"extended-search mode\" where you can\ntype in multiple search terms delimited by spaces. e.g. ^music .mp3$ sbtrkt !fire\n\n\n\nToken\nMatch type\nDescription\n\n\n\n\nsbtrkt\nfuzzy-match\nItems that match sbtrkt\n\n\n'wild\nexact-match (quoted)\nItems that include wild\n\n\n'wild'\nexact-boundary-match (quoted both ends)\nItems that include wild at word boundaries\n\n\n^music\nprefix-exact-match\nItems that start with music\n\n\n.mp3$\nsuffix-exact-match\nItems that end with .mp3\n\n\n!fire\ninverse-exact-match\nItems that do not include fire\n\n\n!^music\ninverse-prefix-exact-match\nItems that do not start with music\n\n\n!.mp3$\ninverse-suffix-exact-match\nItems that do not end with .mp3\n\n\n\nIf you don't prefer fuzzy matching and do not wish to \"quote\" every word,\nstart fzf with -e or --exact option. Note that when  --exact is set,\n'-prefix \"unquotes\" the term.\nA single bar character term acts as an OR operator. For example, the following\nquery matches entries that start with core and end with either go, rb,\nor py.\n^core go$ | rb$ | py$\n\nEnvironment variables\n\nFZF_DEFAULT_COMMAND\n\nDefault command to use when input is tty\ne.g. export FZF_DEFAULT_COMMAND='fd --type f'\n\n\nFZF_DEFAULT_OPTS\n\nDefault options\ne.g. export FZF_DEFAULT_OPTS=\"--layout=reverse --inline-info\"\n\n\nFZF_DEFAULT_OPTS_FILE\n\nIf you prefer to manage default options in a file, set this variable to\npoint to the location of the file\ne.g. export FZF_DEFAULT_OPTS_FILE=~/.fzfrc\n\n\n\nWarningFZF_DEFAULT_COMMAND is not used by shell integration due to the\nslight difference in requirements.\n\nCTRL-T runs $FZF_CTRL_T_COMMAND to get a list of files and directories\nALT-C runs $FZF_ALT_C_COMMAND to get a list of directories\nvim ~/**<tab> runs fzf_compgen_path() with the prefix (~/) as the first argument\ncd foo**<tab> runs fzf_compgen_dir() with the prefix (foo) as the first argument\n\nThe available options are described later in this document.\n\nCustomizing the look\nThe user interface of fzf is fully customizable with a large number of\nconfiguration options. For a quick setup, you can start with one of the style\npresets ‚Äî default, full, or minimal ‚Äî using the --style option.\nfzf --style full \\\n    --preview 'fzf-preview.sh {}' --bind 'focus:transform-header:file --brief {}'\n\n\n\nPreset\nScreenshot\n\n\n\n\ndefault\n\n\n\nfull\n\n\n\nminimal\n\n\n\n\nHere's an example based on the full preset:\n\n\ngit ls-files | fzf --style full \\\n    --border --padding 1,2 \\\n    --border-label ' Demo ' --input-label ' Input ' --header-label ' File Type ' \\\n    --preview 'fzf-preview.sh {}' \\\n    --bind 'result:transform-list-label:\n        if [[ -z $FZF_QUERY ]]; then\n          echo \" $FZF_MATCH_COUNT items \"\n        else\n          echo \" $FZF_MATCH_COUNT matches for [$FZF_QUERY] \"\n        fi\n        ' \\\n    --bind 'focus:transform-preview-label:[[ -n {} ]] && printf \" Previewing [%s] \" {}' \\\n    --bind 'focus:+transform-header:file --brief {} || echo \"No file selected\"' \\\n    --bind 'ctrl-r:change-list-label( Reloading the list )+reload(sleep 2; git ls-files)' \\\n    --color 'border:#aaaaaa,label:#cccccc' \\\n    --color 'preview-border:#9999cc,preview-label:#ccccff' \\\n    --color 'list-border:#669966,list-label:#99cc99' \\\n    --color 'input-border:#996666,input-label:#ffcccc' \\\n    --color 'header-border:#6699cc,header-label:#99ccff'\n\nOptions\nSee the man page (fzf --man or man fzf) for the full list of options.\nDemo\nIf you learn by watching videos, check out this screencast by @samoshkin to explore fzf features.\n\n\n\nExamples\n\nWiki page of examples\n\nDisclaimer: The examples on this page are maintained by the community\nand are not thoroughly tested\n\n\nAdvanced fzf examples\n\nKey bindings for command-line\nBy setting up shell integration, you can use\nthe following key bindings in bash, zsh, and fish.\n\nCTRL-T - Paste the selected files and directories onto the command-line\n\nThe list is generated using --walker file,dir,follow,hidden option\n\nYou can override the behavior by setting FZF_CTRL_T_COMMAND to a custom command that generates the desired list\nOr you can set --walker* options in FZF_CTRL_T_OPTS\n\n\nSet FZF_CTRL_T_OPTS to pass additional options to fzf\n# Preview file content using bat (https://github.com/sharkdp/bat)\nexport FZF_CTRL_T_OPTS=\"\n  --walker-skip .git,node_modules,target\n  --preview 'bat -n --color=always {}'\n  --bind 'ctrl-/:change-preview-window(down|hidden|)'\"\n\nCan be disabled by setting FZF_CTRL_T_COMMAND to an empty string when\nsourcing the script\n\n\nCTRL-R - Paste the selected command from history onto the command-line\n\nIf you want to see the commands in chronological order, press CTRL-R\nagain which toggles sorting by relevance\nPress CTRL-/ or ALT-/ to toggle line wrapping\nSet FZF_CTRL_R_OPTS to pass additional options to fzf\n# CTRL-Y to copy the command into clipboard using pbcopy\nexport FZF_CTRL_R_OPTS=\"\n  --bind 'ctrl-y:execute-silent(echo -n {2..} | pbcopy)+abort'\n  --color header:italic\n  --header 'Press CTRL-Y to copy command into clipboard'\"\n\n\n\nALT-C - cd into the selected directory\n\nThe list is generated using --walker dir,follow,hidden option\nSet FZF_ALT_C_COMMAND to override the default command\n\nOr you can set --walker-* options in FZF_ALT_C_OPTS\n\n\nSet FZF_ALT_C_OPTS to pass additional options to fzf\n# Print tree structure in the preview window\nexport FZF_ALT_C_OPTS=\"\n  --walker-skip .git,node_modules,target\n  --preview 'tree -C {}'\"\n\nCan be disabled by setting FZF_ALT_C_COMMAND to an empty string when\nsourcing the script\n\n\n\nDisplay modes for these bindings can be separately configured via\nFZF_{CTRL_T,CTRL_R,ALT_C}_OPTS or globally via FZF_DEFAULT_OPTS.\n(e.g. FZF_CTRL_R_OPTS='--tmux bottom,60% --height 60% --border top')\nMore tips can be found on the wiki page.\nFuzzy completion for bash and zsh\nFiles and directories\nFuzzy completion for files and directories can be triggered if the word before\nthe cursor ends with the trigger sequence, which is by default **.\n\nCOMMAND [DIRECTORY/][FUZZY_PATTERN]**<TAB>\n\n# Files under the current directory\n# - You can select multiple items with TAB key\nvim **<TAB>\n\n# Files under parent directory\nvim ../**<TAB>\n\n# Files under parent directory that match `fzf`\nvim ../fzf**<TAB>\n\n# Files under your home directory\nvim ~/**<TAB>\n\n\n# Directories under current directory (single-selection)\ncd **<TAB>\n\n# Directories under ~/github that match `fzf`\ncd ~/github/fzf**<TAB>\nProcess IDs\nFuzzy completion for PIDs is provided for kill command.\n# Can select multiple processes with <TAB> or <Shift-TAB> keys\nkill -9 **<TAB>\nHost names\nFor ssh and telnet commands, fuzzy completion for hostnames is provided. The\nnames are extracted from /etc/hosts and ~/.ssh/config.\nssh **<TAB>\ntelnet **<TAB>\nEnvironment variables / Aliases\nunset **<TAB>\nexport **<TAB>\nunalias **<TAB>\nCustomizing fzf options for completion\n# Use ~~ as the trigger sequence instead of the default **\nexport FZF_COMPLETION_TRIGGER='~~'\n\n# Options to fzf command\nexport FZF_COMPLETION_OPTS='--border --info=inline'\n\n# Options for path completion (e.g. vim **<TAB>)\nexport FZF_COMPLETION_PATH_OPTS='--walker file,dir,follow,hidden'\n\n# Options for directory completion (e.g. cd **<TAB>)\nexport FZF_COMPLETION_DIR_OPTS='--walker dir,follow'\n\n# Advanced customization of fzf options via _fzf_comprun function\n# - The first argument to the function is the name of the command.\n# - You should make sure to pass the rest of the arguments ($@) to fzf.\n_fzf_comprun() {\n  local command=$1\n  shift\n\n  case \"$command\" in\n    cd)           fzf --preview 'tree -C {} | head -200'   \"$@\" ;;\n    export|unset) fzf --preview \"eval 'echo \\$'{}\"         \"$@\" ;;\n    ssh)          fzf --preview 'dig {}'                   \"$@\" ;;\n    *)            fzf --preview 'bat -n --color=always {}' \"$@\" ;;\n  esac\n}\nCustomizing completion source for paths and directories\n# Use fd (https://github.com/sharkdp/fd) for listing path candidates.\n# - The first argument to the function ($1) is the base path to start traversal\n# - See the source code (completion.{bash,zsh}) for the details.\n_fzf_compgen_path() {\n  fd --hidden --follow --exclude \".git\" . \"$1\"\n}\n\n# Use fd to generate the list for directory completion\n_fzf_compgen_dir() {\n  fd --type d --hidden --follow --exclude \".git\" . \"$1\"\n}\nSupported commands\nOn bash, fuzzy completion is enabled only for a predefined set of commands\n(complete | grep _fzf to see the list). But you can enable it for other\ncommands as well by using _fzf_setup_completion helper function.\n# usage: _fzf_setup_completion path|dir|var|alias|host COMMANDS...\n_fzf_setup_completion path ag git kubectl\n_fzf_setup_completion dir tree\nCustom fuzzy completion\n(Custom completion API is experimental and subject to change)\nFor a command named \"COMMAND\", define _fzf_complete_COMMAND function using\n_fzf_complete helper.\n# Custom fuzzy completion for \"doge\" command\n#   e.g. doge **<TAB>\n_fzf_complete_doge() {\n  _fzf_complete --multi --reverse --prompt=\"doge> \" -- \"$@\" < <(\n    echo very\n    echo wow\n    echo such\n    echo doge\n  )\n}\n\nThe arguments before -- are the options to fzf.\nAfter --, simply pass the original completion arguments unchanged (\"$@\").\nThen, write a set of commands that generates the completion candidates and\nfeed its output to the function using process substitution (< <(...)).\n\nzsh will automatically pick up the function using the naming convention but in\nbash you have to manually associate the function with the command using the\ncomplete command.\n[ -n \"$BASH\" ] && complete -F _fzf_complete_doge -o default -o bashdefault doge\nIf you need to post-process the output from fzf, define\n_fzf_complete_COMMAND_post as follows.\n_fzf_complete_foo() {\n  _fzf_complete --multi --reverse --header-lines=3 -- \"$@\" < <(\n    ls -al\n  )\n}\n\n_fzf_complete_foo_post() {\n  awk '{print $NF}'\n}\n\n[ -n \"$BASH\" ] && complete -F _fzf_complete_foo -o default -o bashdefault foo\nVim plugin\nSee README-VIM.md.\nAdvanced topics\nCustomizing for different types of input\nSince fzf is a general-purpose text filter, its algorithm was designed to\n\"generally\" work well with any kind of input. However, admittedly, there is no\ntrue one-size-fits-all solution, and you may want to tweak the algorithm and\nsome of the settings depending on the type of the input. To make this process\neasier, fzf provides a set of \"scheme\"s for some common input types.\n\n\n\nScheme\nDescription\n\n\n\n\n--scheme=default\nGeneric scheme designed to work well with any kind of input\n\n\n--scheme=path\nSuitable for file paths\n\n\n--scheme=history\nSuitable for command history or any input where chronological ordering is important\n\n\n\n(See fzf --man for the details)\nPerformance\nfzf is fast. Performance should not be a problem in most use cases. However,\nyou might want to be aware of the options that can affect performance.\n\n--ansi tells fzf to extract and parse ANSI color codes in the input, and it\nmakes the initial scanning slower. So it's not recommended that you add it\nto your $FZF_DEFAULT_OPTS.\n--nth makes fzf slower because it has to tokenize each line.\nA plain string --delimiter should be preferred over a regular expression\ndelimiter.\n--with-nth makes fzf slower as fzf has to tokenize and reassemble each\nline.\n\nExecuting external programs\nYou can set up key bindings for starting external processes without leaving\nfzf (execute, execute-silent).\n# Press F1 to open the file with less without leaving fzf\n# Press CTRL-Y to copy the line to clipboard and aborts fzf (requires pbcopy)\nfzf --bind 'f1:execute(less -f {}),ctrl-y:execute-silent(echo {} | pbcopy)+abort'\nSee KEY BINDINGS section of the man page for details.\nTurning into a different process\nbecome(...) is similar to execute(...)/execute-silent(...) described\nabove, but instead of executing the command and coming back to fzf on\ncomplete, it turns fzf into a new process for the command.\nfzf --bind 'enter:become(vim {})'\nCompared to the seemingly equivalent command substitution vim \"$(fzf)\", this\napproach has several advantages:\n\nVim will not open an empty file when you terminate fzf with\nCTRL-C\nVim will not open an empty file when you press ENTER on an empty\nresult\nCan handle multiple selections even when they have whitespaces\nfzf --multi --bind 'enter:become(vim {+})'\n\n\nTo be fair, running fzf --print0 | xargs -0 -o vim instead of vim \"$(fzf)\"\nresolves all of the issues mentioned. Nonetheless, become(...) still offers\nadditional benefits in different scenarios.\n\nYou can set up multiple bindings to handle the result in different ways\nwithout any wrapping script\nfzf --bind 'enter:become(vim {}),ctrl-e:become(emacs {})'\n\nPreviously, you would have to use --expect=ctrl-e and check the first\nline of the output of fzf\n\n\nYou can easily build the subsequent command using the field index\nexpressions of fzf\n# Open the file in Vim and go to the line\ngit grep --line-number . |\n    fzf --delimiter : --nth 3.. --bind 'enter:become(vim {1} +{2})'\n\n\nReloading the candidate list\nBy binding reload action to a key or an event, you can make fzf dynamically\nreload the candidate list. See #1750 for\nmore details.\n1. Update the list of processes by pressing CTRL-R\nps -ef |\n  fzf --bind 'ctrl-r:reload(ps -ef)' \\\n      --header 'Press CTRL-R to reload' --header-lines=1 \\\n      --height=50% --layout=reverse\n2. Switch between sources by pressing CTRL-D or CTRL-F\nFZF_DEFAULT_COMMAND='find . -type f' \\\n  fzf --bind 'ctrl-d:reload(find . -type d),ctrl-f:reload(eval \"$FZF_DEFAULT_COMMAND\")' \\\n      --height=50% --layout=reverse\n3. Interactive ripgrep integration\nThe following example uses fzf as the selector interface for ripgrep. We bound\nreload action to change event, so every time you type on fzf, the ripgrep\nprocess will restart with the updated query string denoted by the placeholder\nexpression {q}. Also, note that we used --disabled option so that fzf\ndoesn't perform any secondary filtering.\n: | rg_prefix='rg --column --line-number --no-heading --color=always --smart-case' \\\n    fzf --bind 'start:reload:$rg_prefix \"\"' \\\n        --bind 'change:reload:$rg_prefix {q} || true' \\\n        --bind 'enter:become(vim {1} +{2})' \\\n        --ansi --disabled \\\n        --height=50% --layout=reverse\nIf ripgrep doesn't find any matches, it will exit with a non-zero exit status,\nand fzf will warn you about it. To suppress the warning message, we added\n|| true to the command, so that it always exits with 0.\nSee \"Using fzf as interactive Ripgrep launcher\"\nfor more sophisticated examples.\nPreview window\nWhen the --preview option is set, fzf automatically starts an external process\nwith the current line as the argument and shows the result in the split window.\nYour $SHELL is used to execute the command with $SHELL -c COMMAND.\nThe window can be scrolled using the mouse or custom key bindings.\n# {} is replaced with the single-quoted string of the focused line\nfzf --preview 'cat {}'\nPreview window supports ANSI colors, so you can use any program that\nsyntax-highlights the content of a file, such as\nBat or\nHighlight:\nfzf --preview 'bat --color=always {}' --preview-window '~3'\nYou can customize the size, position, and border of the preview window using\n--preview-window option, and the foreground and background color of it with\n--color option. For example,\nfzf --height 40% --layout reverse --info inline --border \\\n    --preview 'file {}' --preview-window up,1,border-horizontal \\\n    --bind 'ctrl-/:change-preview-window(50%|hidden|)' \\\n    --color 'fg:#bbccdd,fg+:#ddeeff,bg:#334455,preview-bg:#223344,border:#778899'\nSee the man page (man fzf) for the full list of options.\nMore advanced examples can be found here.\nWarningSince fzf is a general-purpose text filter rather than a file finder, it is\nnot a good idea to add --preview option to your $FZF_DEFAULT_OPTS.\n# *********************\n# ** DO NOT DO THIS! **\n# *********************\nexport FZF_DEFAULT_OPTS='--preview \"bat --style=numbers --color=always --line-range :500 {}\"'\n\n# bat doesn't work with any input other than the list of files\nps -ef | fzf\nseq 100 | fzf\nhistory | fzf\n\nPreviewing an image\nfzf can display images in the preview window using one of the following protocols:\n\nKitty graphics protocol\niTerm2 inline images protocol\nSixel\n\nSee bin/fzf-preview.sh script for more information.\nfzf --preview 'fzf-preview.sh {}'\nTips\nRespecting .gitignore\nYou can use fd,\nripgrep, or the silver\nsearcher to traverse the file\nsystem while respecting .gitignore.\n# Feed the output of fd into fzf\nfd --type f --strip-cwd-prefix | fzf\n\n# Setting fd as the default source for fzf\nexport FZF_DEFAULT_COMMAND='fd --type f --strip-cwd-prefix'\n\n# Now fzf (w/o pipe) will use the fd command to generate the list\nfzf\n\n# To apply the command to CTRL-T as well\nexport FZF_CTRL_T_COMMAND=\"$FZF_DEFAULT_COMMAND\"\nIf you want the command to follow symbolic links and don't want it to exclude\nhidden files, use the following command:\nexport FZF_DEFAULT_COMMAND='fd --type f --strip-cwd-prefix --hidden --follow --exclude .git'\nFish shell\nCTRL-T key binding of fish, unlike those of bash and zsh, will use the last\ntoken on the command-line as the root directory for the recursive search. For\ninstance, hitting CTRL-T at the end of the following command-line\nls /var/\nwill list all files and directories under /var/.\nWhen using a custom FZF_CTRL_T_COMMAND, use the unexpanded $dir variable to\nmake use of this feature. $dir defaults to . when the last token is not a\nvalid directory. Example:\nset -g FZF_CTRL_T_COMMAND \"command find -L \\$dir -type f 2> /dev/null | sed '1d; s#^\\./##'\"\nfzf Theme Playground\nfzf Theme Playground created by\nVitor Mello is a webpage where you can\ninteractively create fzf themes.\nRelated projects\nhttps://github.com/junegunn/fzf/wiki/Related-projects\nLicense\nThe MIT License (MIT)\nCopyright (c) 2013-2024 Junegunn Choi",
      "languages": {
        "R": 1,
        "go": 1,
        "Shell": 1,
        "Go": 1,
        "GO": 1
      },
      "topics": [
        "Go",
        "Lua",
        "Shell",
        "R"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T16:24:33.851445"
    },
    {
      "owner": "grpc",
      "name": "grpc-go",
      "url": "https://github.com/grpc/grpc-go",
      "description": "The Go language implementation of gRPC. HTTP/2 based RPC",
      "readme_content": "gRPC-Go\n\n\n\nThe Go implementation of gRPC: A high performance, open source, general\nRPC framework that puts mobile and HTTP/2 first. For more information see the\nGo gRPC docs, or jump directly into the quick start.\nPrerequisites\n\nGo: any one of the two latest major releases.\n\nInstallation\nSimply add the following import to your code, and then go [build|run|test]\nwill automatically fetch the necessary dependencies:\nimport \"google.golang.org/grpc\"\n\nNote: If you are trying to access grpc-go from China, see the\nFAQ below.\n\nLearn more\n\nGo gRPC docs, which include a quick start and API\nreference among other resources\nLow-level technical docs from this repository\nPerformance benchmark\nExamples\n\nFAQ\nI/O Timeout Errors\nThe golang.org domain may be blocked from some countries. go get usually\nproduces an error like the following when this happens:\n$ go get -u google.golang.org/grpc\npackage google.golang.org/grpc: unrecognized import path \"google.golang.org/grpc\" (https fetch: Get https://google.golang.org/grpc?go-get=1: dial tcp 216.239.37.1:443: i/o timeout)\nTo build Go code, there are several options:\n\n\nSet up a VPN and access google.golang.org through that.\n\n\nWith Go module support: it is possible to use the replace feature of go mod to create aliases for golang.org packages.  In your project's directory:\ngo mod edit -replace=google.golang.org/grpc=github.com/grpc/grpc-go@latest\ngo mod tidy\ngo mod vendor\ngo build -mod=vendor\nAgain, this will need to be done for all transitive dependencies hosted on\ngolang.org as well. For details, refer to golang/go issue\n#28652.\n\n\nCompiling error, undefined: grpc.SupportPackageIsVersion\nPlease update to the latest version of gRPC-Go using\ngo get google.golang.org/grpc.\nHow to turn on logging\nThe default logger is controlled by environment variables. Turn everything on\nlike this:\n$ export GRPC_GO_LOG_VERBOSITY_LEVEL=99\n$ export GRPC_GO_LOG_SEVERITY_LEVEL=info\nThe RPC failed with error \"code = Unavailable desc = transport is closing\"\nThis error means the connection the RPC is using was closed, and there are many\npossible reasons, including:\n\nmis-configured transport credentials, connection failed on handshaking\nbytes disrupted, possibly by a proxy in between\nserver shutdown\nKeepalive parameters caused connection shutdown, for example if you have\nconfigured your server to terminate connections regularly to trigger DNS\nlookups.\nIf this is the case, you may want to increase your\nMaxConnectionAgeGrace,\nto allow longer RPC calls to finish.\n\nIt can be tricky to debug this because the error happens on the client side but\nthe root cause of the connection being closed is on the server side. Turn on\nlogging on both client and server, and see if there are any transport\nerrors.",
      "languages": {
        "Go": 1,
        "go": 1,
        "GO": 1
      },
      "topics": [
        "Go",
        "gRPC"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T16:24:34.863160"
    },
    {
      "owner": "1",
      "name": "discourse_docker",
      "url": "https://github.com/1/discourse_docker",
      "description": "A Docker image for Discourse",
      "readme_content": "About\n\n\nDocker is an open source project to pack, ship and run any Linux application in a lighter weight, faster container than a traditional virtual machine.\n\n\nDocker makes it much easier to deploy a Discourse forum on your servers and keep it updated. For background, see Sam's blog post.\n\n\nThe templates and base image configure Discourse with the Discourse team's recommended optimal defaults.\n\n\nGetting Started\nThe simplest way to get started is via the standalone template, which can be installed in 30 minutes or less. For detailed install instructions, see\nhttps://github.com/discourse/discourse/blob/master/docs/INSTALL-cloud.md\nDirectory Structure\n/cids\nContains container ids for currently running Docker containers. cids are Docker's \"equivalent\" of pids. Each container will have a unique git like hash.\n/containers\nThis directory is for container definitions for your various Discourse containers. You are in charge of this directory, it ships empty.\n/samples\nSample container definitions you may use to bootstrap your environment. You can copy templates from here into the containers directory.\n/shared\nPlaceholder spot for shared volumes with various Discourse containers. You may elect to store certain persistent information outside of a container, in our case we keep various logfiles and upload directory outside. This allows you to rebuild containers easily without losing important information. Keeping uploads outside of the container allows you to share them between multiple web instances.\n/templates\npups-managed templates you may use to bootstrap your environment.\n/image\nDockerfiles for Discourse; see the README for further details.\nThe Docker repository will always contain the latest built version at: https://hub.docker.com/r/discourse/discourse/, you should not need to build the base image.\nLauncher\nThe base directory contains a single bash script which is used to manage containers. You can use it to \"bootstrap\" a new container, enter, start, stop and destroy a container.\nUsage: launcher COMMAND CONFIG [--skip-prereqs]\nCommands:\n    start:      Start/initialize a container\n    stop:       Stop a running container\n    restart:    Restart a container\n    destroy:    Stop and remove a container\n    enter:      Use docker exec to enter a container\n    logs:       Docker logs for container\n\tmemconfig:  Configure sane defaults for available RAM\n    bootstrap:  Bootstrap a container for the config based on a template\n    rebuild:    Rebuild a container (destroy old, bootstrap, start new)\n\nIf the environment variable \"SUPERVISED\" is set to true, the container won't be detached, allowing a process monitoring tool to manage the restart behaviour of the container.\nContainer Configuration\nThe beginning of the container definition can contain the following \"special\" sections:\ntemplates:\ntemplates:\n  - \"templates/cron.template.yml\"\n  - \"templates/postgres.template.yml\"\n\nThis template is \"composed\" out of all these child templates, this allows for a very flexible configuration structure. Furthermore you may add specific hooks that extend the templates you reference.\nexpose:\nexpose:\n  - \"2222:22\"\n  - \"127.0.0.1:20080:80\"\n\nExpose port 22 inside the container on port 2222 on ALL local host interfaces. In order to bind to only one interface, you may specify the host's IP address as ([<host_interface>:[host_port]])|(<host_port>):<container_port>[/udp] as defined in the docker port binding documentation\nvolumes:\nvolumes:\n  - volume:\n      host: /var/discourse/shared\n      guest: /shared\n\n\nExpose a directory inside the host to the container.\nlinks:\nlinks:\n  - link:\n      name: postgres\n      alias: postgres\n\nLinks another container to the current container. This will add --link postgres:postgres\nto the options when running the container.\nUpgrading Discourse\nThe Docker setup gives you multiple upgrade options:\n\n\nUse the front end at http://yoursite.com/admin/upgrade to upgrade an already running image.\n\n\nCreate a new base image manually by running:\n\n\n\n./launcher rebuild my_image\n\nSingle Container vs. Multiple Container\nThe samples directory contains a standalone template. This template bundles all of the software required to run Discourse into a single container. The advantage is that it is easy.\nThe multiple container configuration setup is far more flexible and robust, however it is also more complicated to set up. A multiple container setup allows you to:\n\nMinimize downtime when upgrading to new versions of Discourse. You can bootstrap new web processes while your site is running and only after it is built, switch the new image in.\nScale your forum to multiple servers.\nAdd servers for redundancy.\nHave some required services (e.g. the database) run on beefier hardware.\n\nIf you want a multiple container setup, see the data.yml and web_only.yml templates in the samples directory. To ease this process, launcher will inject an env var called DISCOURSE_HOST_IP which will be available inside the image.\nWARNING: In a multiple container configuration, make sure you setup iptables or some other firewall to protect various ports (for postgres/redis).\nOn Ubuntu, install the ufw or iptables-persistent package to manage firewall rules.\nEmail\nFor a Discourse instance to function properly Email must be set up. Use the SMTP_URL env var to set your SMTP address, see sample templates for an example. The Docker image does not contain postfix, exim or another MTA, it was omitted because it is very tricky to set up correctly.\nTroubleshooting\nView the container logs: ./launcher logs my_container\nSpawn a shell inside your container using ./launcher enter my_container. This is the most foolproof method if you have host root access.\nIf you see network errors trying to retrieve code from github.com or rubygems.org try again - sometimes there are temporary interruptions and a retry is all it takes.\nBehind a proxy network with no direct access to the Internet? Add proxy information to the container environment by adding to the existing env block in the container.yml file:\nenv:\n    ‚Ä¶existing entries‚Ä¶\n    HTTP_PROXY: http://proxyserver:port/\n    http_proxy: http://proxyserver:port/\n    HTTPS_PROXY: http://proxyserver:port/\n    https_proxy: http://proxyserver:port/\nSecurity\nDirectory permissions in Linux are UID/GID based, if your numeric IDs on the\nhost do not match the IDs in the guest, permissions will mismatch. On clean\ninstalls you can ensure they are in sync by looking at /etc/passwd and\n/etc/group, the Discourse account will have UID 1000.\nAdvanced topics\n\nSetting up SSL with Discourse Docker\nMultisite configuration with Docker\nLinking containers for a multiple container setup\nUsing Rubygems mirror to improve connection problem in China\n\nDeveloping with Vagrant\nIf you are looking to make modifications to this repository, you can easily test\nout your changes before committing, using the magic of\nVagrant.  Install Vagrant as per the default\ninstructions, and\nthen run:\nvagrant up\n\nThis will spawn a new Ubuntu VM, install Docker, and then await your\ninstructions.  You can then SSH into the VM with vagrant ssh, become\nroot with sudo -i, and then you're right to go.  Your live git repo is\nalready available at /var/discourse, so you can just cd /var/discourse\nand then start running launcher.\nLicense\nMIT",
      "languages": {
        "R": 1,
        "go": 1,
        "Shell": 1,
        "Go": 1,
        "GO": 1
      },
      "topics": [
        "Docker",
        "Redis",
        "R",
        "Shell",
        "Go"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T17:23:38.673570"
    },
    {
      "owner": "ruby",
      "name": "ruby",
      "url": "https://github.com/ruby/ruby",
      "description": "The Ruby Programming Language",
      "readme_content": "What is Ruby?\nRuby is an interpreted object-oriented programming language often\nused for web development. It also offers many scripting features\nto process plain text and serialized files, or manage system tasks.\nIt is simple, straightforward, and extensible.\nFeatures of Ruby\n\nSimple Syntax\nNormal Object-oriented Features (e.g. class, method calls)\nAdvanced Object-oriented Features (e.g. mix-in, singleton-method)\nOperator Overloading\nException Handling\nIterators and Closures\nGarbage Collection\nDynamic Loading of Object Files (on some architectures)\nHighly Portable (works on many Unix-like/POSIX compatible platforms as\nwell as Windows, macOS, etc.) cf.\nhttps://docs.ruby-lang.org/en/master/maintainers_md.html#label-Platform+Maintainers\n\nHow to get Ruby\nFor a complete list of ways to install Ruby, including using third-party tools\nlike rvm, see:\nhttps://www.ruby-lang.org/en/downloads/\nYou can download release packages and the snapshot of the repository. If you want to\ndownload whole versions of Ruby, please visit https://www.ruby-lang.org/en/downloads/releases/.\nDownload with Git\nThe mirror of the Ruby source tree can be checked out with the following command:\n$ git clone https://github.com/ruby/ruby.git\n\nThere are some other branches under development. Try the following command\nto see the list of branches:\n$ git ls-remote https://github.com/ruby/ruby.git\n\nYou may also want to use https://git.ruby-lang.org/ruby.git (actual master of Ruby source)\nif you are a committer.\nHow to build\nSee Building Ruby\nRuby home page\nhttps://www.ruby-lang.org/\nDocumentation\n\nEnglish\nJapanese\n\nMailing list\nThere is a mailing list to discuss Ruby. To subscribe to this list, please\nsend the following phrase:\njoin\n\nin the mail subject (not body) to the address ruby-talk-request@ml.ruby-lang.org.\nCopying\nSee the file COPYING.\nFeedback\nQuestions about the Ruby language can be asked on the Ruby-Talk mailing list\nor on websites like https://stackoverflow.com.\nBugs should be reported at https://bugs.ruby-lang.org. Read \"Reporting Issues\" for more information.\nContributing\nSee \"Contributing to Ruby\", which includes setup and build instructions.\nThe Author\nRuby was originally designed and developed by Yukihiro Matsumoto (Matz) in 1995.\nmatz@ruby-lang.org",
      "languages": {
        "HTML": 1,
        "ruby": 1,
        "Ruby": 1
      },
      "topics": [
        "Web Development",
        "Ruby"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T17:23:56.509375"
    },
    {
      "owner": "rapid7",
      "name": "metasploit-framework",
      "url": "https://github.com/rapid7/metasploit-framework",
      "description": "Metasploit Framework",
      "readme_content": "Metasploit Framework\nThe Metasploit Framework is an open-source tool released under a BSD-style license. For detailed licensing information, refer to the COPYING file.\nLatest Version\nAccess the latest version of Metasploit from the Nightly Installers page.\nDocumentation\nComprehensive documentation, including usage guides, is available at Metasploit Docs.\nDevelopment Environment\nTo set up a development environment, visit the Development Setup Guide.\nBug and Feature Requests\nSubmit bugs and feature requests via the GitHub Issues tracker. New submissions can be made through the MSF-BUGv1 form.\nAPI Documentation\nFor information on writing modules, refer to the API Documentation.\nSupport and Communication\nFor questions and suggestions, join the Freenode IRC channel or contact the metasploit-hackers mailing list.\nInstalling Metasploit\nRecommended Installation\nWe recommend installation with the official Metasploit installers on Linux or macOS. Metasploit is also pre-installed with Kali.\nFor a manual setup, consult the Dev Environment Setup guide.\nUsing Metasploit\nTo get started with Metasploit:\n\nStart msfconsole: This is the primary interface for interacting with Metasploit.\nExplore Resources:\n\nVisit the Using Metasploit section of the documentation.\n\n\n\nContributing\nTo contribute to Metasploit:\n\nSetup Development Environment: Follow the instructions in the Development Setup Guide on GitHub.\nClone the Repository: Obtain the source code from the official repository.\nSubmit a Pull Request: After making changes, submit a pull request for review. Additional details can be found in the Contributing Guide.",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T17:23:57.604665"
    },
    {
      "owner": "basecamp",
      "name": "kamal",
      "url": "https://github.com/basecamp/kamal",
      "description": "Deploy web apps anywhere.",
      "readme_content": "Kamal: Deploy web apps anywhere\nFrom bare metal to cloud VMs, deploy web apps anywhere with zero downtime. Kamal uses kamal-proxy to seamlessly switch requests between containers. Works seamlessly across multiple servers, using SSHKit to execute commands. Originally built for Rails apps, Kamal will work with any type of web app that can be containerized with Docker.\n‚û°Ô∏è See kamal-deploy.org for documentation on installation, configuration, and commands.\nContributing to the documentation\nPlease help us improve Kamal's documentation on the the basecamp/kamal-site repository.\nLicense\nKamal is released under the MIT License.",
      "languages": {},
      "topics": [
        "Docker"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T17:23:58.496668"
    },
    {
      "owner": "maybe-finance",
      "name": "maybe",
      "url": "https://github.com/maybe-finance/maybe",
      "description": "The OS for your personal finances",
      "readme_content": "(Note: The image above is a mockup of what we're working towards. We're rapidly approaching the functionality shown, but not all of the parts are ready just yet.)\nMaybe: The OS for your personal finances\nGet\ninvolved: Discord ‚Ä¢ Website ‚Ä¢ Issues\nIf you're looking for the previous React codebase, you can find it\nat maybe-finance/maybe-archive.\nBackstory\nWe spent the better part of 2021/2022 building a personal finance + wealth\nmanagement app called, Maybe. Very full-featured, including an \"Ask an Advisor\"\nfeature which connected users with an actual CFP/CFA to help them with their\nfinances (all included in your subscription).\nThe business end of things didn't work out, and so we shut things down mid-2023.\nWe spent the better part of $1,000,000 building the app (employees +\ncontractors, data providers/services, infrastructure, etc.).\nWe're now reviving the product as a fully open-source project. The goal is to\nlet you run the app yourself, for free, and use it to manage your own finances\nand eventually offer a hosted version of the app for a small monthly fee.\nMaybe Hosting\nThere are 3 primary ways to use the Maybe app:\n\nManaged (easiest) - coming soon...\nOne-click deploy\nSelf-host with Docker\n\nContributing\nBefore contributing, you'll likely find it helpful\nto understand context and general vision/direction.\nOnce you've done that, please visit\nour contributing guide\nto get started!\nLocal Development Setup\nIf you are trying to self-host the Maybe app, stop here. You\nshould read this guide to get started.\nThe instructions below are for developers to get started with contributing to the app.\nRequirements\n\nSee .ruby-version file for required Ruby version\nPostgreSQL >9.3 (ideally, latest stable version)\n\nAfter cloning the repo, the basic setup commands are:\ncd maybe\ncp .env.local.example .env.local\nbin/setup\nbin/dev\n\n# Optionally, load demo data\nrake demo_data:reset\nAnd visit http://localhost:3000 to see the app. You can use the following\ncredentials to log in (generated by DB seed):\n\nEmail: user@maybe.local\nPassword: password\n\nFor further instructions, see guides below.\nMulti-currency support\nIf you'd like multi-currency support, there are a few extra steps to follow.\n\nSign up for an API key at Synth. It's a Maybe\nproduct and the free plan is sufficient for basic multi-currency support.\nAdd your API key to your .env file.\n\nSetup Guides\nDev Container (optional)\nThis is 100% optional and meant for devs who don't want to worry about\ninstalling requirements manually for their platform. You can\nfollow this guide\nto learn more about Dev Containers.\nIf you run into could not connect to server errors, you may need to change\nyour .env's DB_HOST environment variable value to db to point to the\nPostgres container.\nMac\nPlease visit\nour Mac dev setup guide.\nLinux\nPlease visit\nour Linux dev setup guide.\nWindows\nPlease visit\nour Windows dev setup guide.\nTesting Emails\nIn development, we use letter_opener to automatically open emails in your\nbrowser. When an email sends locally, a new browser tab will open with a\npreview.\nRepo Activity\n\nCopyright & license\nMaybe is distributed under\nan AGPLv3 license. \"\nMaybe\" is a trademark of Maybe Finance, Inc.",
      "languages": {
        "ruby": 1,
        "Ruby": 1
      },
      "topics": [
        "PostgreSQL",
        "Ruby",
        "Docker"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T17:23:59.664471"
    },
    {
      "owner": "spree",
      "name": "spree",
      "url": "https://github.com/spree/spree",
      "description": "An open source eCommerce platform giving you full control and customizability. Modular and API-first. Multi-vendor, multi-tenant, multi-store, multi-currency, multi-language. Built using Ruby on Rails. Developed by @vendo-dev",
      "readme_content": "Spree Commerce\n\n    An open-source eCommerce platform giving you full control and customizability.\n    \n    Build any eCommerce solution that your business requires.\n    \n\nSlack\n    ¬∑\n    Documentation\n    ¬∑\n    Website\n    ¬∑\n    Demo\n    ¬∑\n    Roadmap\n\n\n\n\n\n\n\n\n\n\nGetting Started\nVisit the Quickstart Guide to set up Spree in 5 minutes.\nIf you like what you see, consider giving Spree a GitHub star ‚≠ê\nThank you for supporting Spree open-source ‚ù§Ô∏è\nEnterprise support\nContact us for enterprise support and custom development services. We offer:\n\nmigrations and upgrades,\ndelivering your Spree application,\noptimizing your Spree stack.\n\nEnterprise Edition - Vendo\nBesides Enterprise support we also offer an enterprise Spree version called Vendo - which is a commercial offering from the Spree team that gives you all the tools you need to launch your store or marketplace and provides you with ready-to-use integrations that will reduce your project's development time and cost.\nTo get access to Spree Enterprise (Vendo), contact our Sales team\nFeatures\nCustomizable and modular\n\npick and choose parts you want to use\ncustomize everything else (storefront, order processing, API, etc)\n\n\nComposable and API-first\n\nconnect with your existing ecosystem, build custom workflows with ease\nStorefront and Admin API\nWebhooks\n\n\nCart and Checkout for any use case\n\nadvanced cart functionality\nover 30 payment provider integrations out of the box\nAPI to integrate any other payment gateway\nrobust discounts system\nstore credits, gift cards\n\n\n\nGlobal Commerce ready\n\nmulti-currency\nmulti-language\nfull translation support for products, categories, and more\ndifferent shipping methods / costs for different regions\nadvanced tax calculation\n\n\n\nMulti-Store ready\n\nhost multiple brands / stores on a single Spree instance\neach with different branding, configuration, payment methods, shipping options, product catalogs etc\n\n\nResponsive Admin Panel\nManage and curate products, users, orders, returns, shipments & more\n\nOrders & Post-purchase management\nManage orders, shipments, returns and refunds\n\n\nWhy developers love it\n\nRuns anywhere - cloud, VPS, Docker, Kubernetes\nBattle Tested - used by thousands of merchants around the globe in all categories since 2007!\nExtensions available adding new features to Spree\n\nWhat you can build with Spree\nUse Spree for any use case. Innovate beyond what's out there.\nA headless eCommerce micro-service\nCapture orders and payments for whatever you're selling - physical or digital, products or services\n\nA Multi-vendor marketplace\nRun your own marketplace with multiple suppliers, each with a dedicated supplier dashboard\n\nA B2B eCommerce\nStart capturing 6+ figure orders from resellers with safe payments and a checkout process that fits your business model\n\nWholesale eCommerce\nRun your wholesale operation the way your retail partners expect\n\nA white-label SaaS or multi-tenant eCommerce platform\nLaunch a multi-tenant eCommerce platform for your customers, resellers, affiliates in any configuration, eg. B2B2B, B2B2C, B2B2E\n\nCommunity & Contributing\nSpree is an open source project and we love contributions in any form - pull requests, issues, feature ideas!\nFollow our Contributing Guide\nJoin our Slack to meet other community members.\nContact\nContact us and let's go!\nDeveloped by\nSpree is developed and maintained by\n\n\n\n\nVendo is an eCommerce platform based on Spree which you can customize to your exact needs (source available).\nGo global on day 1. DTC x B2B x Marketplace. API-first. One or multiple storefronts.\n\nLicense\nSpree Commerce is a free, open-source eCommerce framework giving you full control and customizability.\nFor Spree Commerce versions 4.10 and later in the spree/spree repository two licenses apply simultaneously and users are required to comply with the terms of these two licenses at the same time:\n\n\nAGPL-3.0 - for all contributions from version 4.10 onwards\n\n\nBSD-3-Clause - for all other contributions predating version 4.10\n\n\nEffectively, for versions 4.10 and upwards AGPL-3.0 license applies.\nSpree Commerce versions 4.9 and earlier in the spree/spree repository are available under the BSD-3-Clause license and users are required to comply with its terms.\nIf you‚Äôd like to use Spree Commerce without the AGPL-3.0 restrictions e.g. for a SaaS business, please talk to us about obtaining a Commercial License.\nAll third party components incorporated into this software are licensed under the original license provided by the owner of the applicable component.\nPlease refer to our Licensing FAQ in case of questions",
      "languages": {
        "Go": 1,
        "go": 1,
        "GO": 1
      },
      "topics": [
        "Kubernetes",
        "Go",
        "Docker"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T17:24:00.576123"
    },
    {
      "owner": "ytti",
      "name": "oxidized",
      "url": "https://github.com/ytti/oxidized",
      "description": "Oxidized is a network device configuration backup tool. It's a RANCID replacement!",
      "readme_content": "Oxidized\n\n\n\n\n\n\nOxidized is a network device configuration backup tool. It's a RANCID replacement!\nIt is light and extensible and supports over 130 operating system types.\nFeature highlights:\n\nAutomatically adds/removes threads to meet configured retrieval interval\nRestful API to a move node immediately to head-of-queue (GET/POST /node/next/[NODE])\nSyslog udp+file example to catch config change events (IOS/JunOS) and trigger a config fetch\n\nWill signal which IOS/JunOS user made the change, can then be used by output modules (via POST)\nThe git output module uses this info - 'git blame' will show who changed each line\n\n\nRestful API to reload list of nodes (GET /reload)\nRestful API to fetch configurations (/node/fetch/[NODE] or /node/fetch/group/[NODE])\nRestful API to show list of nodes (GET /nodes)\nRestful API to show list of version for a node (/node/version[NODE]) and diffs\n\nCheck out the Oxidized TREX 2014 presentation video on YouTube!\n\n‚ö†Ô∏è Maintainer Wanted! ‚ö†Ô∏è\nIs your company using Oxidized and has Ruby developers on staff? I'd love help from an extra maintainer!\n\nIndex\n\nSupported OS Types\nInstallation\n\nDebian and Ubuntu\nCentOS, Oracle Linux, Red Hat Linux\nFreeBSD\nBuild from Git\nDocker\nPodman-Compose\nInstalling Ruby 2.3 using RVM\n\n\nInitial Configuration\nConfiguration\n\nDebugging\nPrivileged mode\nDisabling SSH exec channels\nSources\n\nSource: CSV\nSource: SQL\nSource: SQLite\nSource: Mysql\nSource: HTTP\n\n\nOutputs\n\nOutput: GIT\nOutput: GIT-Crypt\nOutput: HTTP\nOutput: File\nOutput types\n\n\nAdvanced Configuration\nAdvanced Group Configuration\nHooks\n\nHook: exec\nHook: githubrepo\nHook: awssns\nHook: slackdiff\nHook: xmppdiff\nHook: ciscosparkdiff\n\n\n\n\nCreating and Extending Models\nHelp\nHelp Needed\nRuby API\n\nInput\nOutput\nSource\nModel\n\n\n\nInstallation\nDebian and Ubuntu\nDebian \"buster\" or newer and Ubuntu 17.10 (artful) or newer are recommended. On Ubuntu, begin by enabling the universe\nrepository (required for libssh2-1-dev):\nadd-apt-repository universe\nInstall the dependencies:\napt-get install ruby ruby-dev libsqlite3-dev libssl-dev pkg-config cmake libssh2-1-dev libicu-dev zlib1g-dev g++ libyaml-dev\nFinally, install the gems:\ngem install oxidized\ngem install oxidized-script oxidized-web # If you don't install oxidized-web, ensure \"rest\" is removed from your Oxidized config.\nCentOS, Oracle Linux, Red Hat Linux\nOn CentOS 6 and 7 / RHEL 6 and 7, begin by installing Ruby 3.1 via RVM by following the instructions:\nMake sure you dont have any leftover ruby:\nyum erase ruby\nThen, install gpg key and rvm\nsudo gpg --keyserver hkp://keys.gnupg.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDB\ncurl -sSL https://get.rvm.io | bash -s stable\nsource /etc/profile.d/rvm.sh\nrvm requirements run\nrvm install 3.1\nrvm use 3.1\nInstall oxidized requirements:\nyum install make cmake which sqlite-devel openssl-devel libssh2-devel gcc libicu-devel gcc-c++\nInstall the gems:\ngem install oxidized oxidized-web\nYou need to wrap the gem and reference the wrap in the systemctl service file:\nrvm wrapper oxidized\nYou can see where the wrapped gem is via\nrvm wrapper show oxidized\nUse that path in the oxidized.service file, restart the systemctl daemon, run oxidized by hand once, edit config file, start service.\nFreeBSD\nUse RVM to install Ruby v2.3, then install all required packages and gems:\npkg install cmake pkgconf\ngem install oxidized\ngem install oxidized-script oxidized-web\nOxidized is also available via FreeBSD ports:\npkg install rubygem-oxidized rubygem-oxidized-script rubygem-oxidized-web\nBuild from Git\ngit clone https://github.com/ytti/oxidized.git\ncd oxidized/\ngem install bundler\nrake install\nRunning with Docker\nCurrently, Docker Hub automatically builds the master branch for linux/amd64\nand linux/arm64 platforms as\noxidized/oxidized. Each official\nrelease also gets its own tag.\nYou can make use of this container or build your own.\nTo build your own, clone git repo:\ngit clone https://github.com/ytti/oxidized\nThen, build the container locally (requires docker 17.05.0-ce or higher):\ndocker build -q -t oxidized/oxidized:latest oxidized/\nOnce you've built the container (or chosen to make use of the automatically built container in Docker Hub, which will be downloaded for you by docker on the first run command had you not built it), proceed as follows:\nCreate a configuration directory in the host system:\nmkdir /etc/oxidized\nRun the container for the first time to initialize the config:\nNote: this step in only required for creating the Oxidized configuration file and can be skipped if you already have one.\ndocker run --rm -v /etc/oxidized:/home/oxidized/.config/oxidized -p 8888:8888/tcp --user oxidized -t oxidized/oxidized:latest oxidized\nIf the RESTful API and Web Interface are enabled, on the docker host running the container\nedit /etc/oxidized/config and modify rest: 127.0.0.1:8888 to rest: 0.0.0.0:8888. This will bind port 8888 to all interfaces, and expose the port so that it could be accessed externally. (Issue #445)\nAlternatively, you can use docker-compose to launch the oxidized container:\n# docker-compose.yml\n# docker-compose file example for oxidized that will start along with docker daemon\n---\nversion: \"3\"\nservices:\n  oxidized:\n    restart: always\n    image: oxidized/oxidized:latest\n    ports:\n      - 8888:8888/tcp\n    environment:\n      CONFIG_RELOAD_INTERVAL: 600\n    volumes:\n       - config:/home/oxidized/.config/oxidized/\nvolumes:\n  config:\nCreate the /etc/oxidized/router.db (see CSV Source for further info):\nvim /etc/oxidized/router.db\nRun container again to start oxidized with your configuration:\ndocker run -v /etc/oxidized:/home/oxidized/.config/oxidized -p 8888:8888/tcp -t oxidized/oxidized:latest\noxidized[1]: Oxidized starting, running as pid 1\noxidized[1]: Loaded 1 nodes\nPuma 2.13.4 starting...\n* Min threads: 0, max threads: 16\n* Environment: development\n* Listening on tcp://0.0.0.0:8888\nIf you want to have the config automatically reloaded (e.g. when using a http source that changes):\ndocker run -v /etc/oxidized:/home/oxidized/.config/oxidized -p 8888:8888/tcp -e CONFIG_RELOAD_INTERVAL=3600 -t oxidized/oxidized:latest\nIf you need to use an internal CA (e.g. to connect to an private github instance):\ndocker run -v /etc/oxidized:/home/oxidized/.config/oxidized -v /path/to/MY-CA.crt:/usr/local/share/ca-certificates/MY-CA.crt -p 8888:8888/tcp -e UPDATE_CA_CERTIFICATES=true -t oxidized/oxidized:latest\nRunning with podman-compose\nUnder examples/podman-compose, you will find a complete\nexample of how to integrate the container into a docker-compose.yml file.\nInstalling Ruby 2.3 using RVM\nInstall Ruby 2.3 build dependencies\nyum install curl gcc-c++ patch readline readline-devel zlib zlib-devel\nyum install libyaml-devel libffi-devel openssl-devel make cmake\nyum install bzip2 autoconf automake libtool bison iconv-devel libssh2-devel libicu-devel\nInstall RVM\ncurl -L get.rvm.io | bash -s stable\nSetup RVM environment and compile and install Ruby 2.3 and set it as default\nsource /etc/profile.d/rvm.sh\nrvm install 2.3\nrvm use --default 2.3\nConfiguration\nOxidized configuration is in YAML format. Configuration files are subsequently sourced from /etc/oxidized/config then ~/.config/oxidized/config. The hashes will be merged, this might be useful for storing source information in a system wide file and  user specific configuration in the home directory (to only include a staff specific username and password). Eg. if many users are using oxs, see Oxidized::Script.\nIt is recommended practice to run Oxidized using its own username.  This username can be added using standard command-line tools:\nuseradd -s /bin/bash -m oxidized\n\nIt is recommended not to run Oxidized as root. After creating a dedicated user, switch to the oxidized user using su oxidized to ensure that Oxidized is run under the correct user context.\n\nTo initialize a default configuration in your home directory ~/.config/oxidized/config, simply run oxidized once. If you don't further configure anything from the output and source sections, it'll extend the examples on a subsequent oxidized execution. This is useful to see what options for a specific source or output backend are available.\nYou can set the env variable OXIDIZED_HOME to change its home directory.\nOXIDIZED_HOME=/etc/oxidized\n\n$ tree -L 1 /etc/oxidized\n/etc/oxidized/\n‚îú‚îÄ‚îÄ config\n‚îú‚îÄ‚îÄ log-router-ssh\n‚îú‚îÄ‚îÄ log-router-telnet\n‚îú‚îÄ‚îÄ pid\n‚îú‚îÄ‚îÄ router.db\n‚îî‚îÄ‚îÄ repository.git\nSource\nOxidized supports CSV,  SQLite, MySQL and HTTP as source backends. The CSV backend reads nodes from a rancid compatible router.db file. The SQLite and MySQL backends will fire queries against a database and map certain fields to model items. The HTTP backend will fire queries against a http/https url. Take a look at the Configuration for more details.\nOutputs\nPossible outputs are either File, GIT, GIT-Crypt and HTTP. The file backend takes a destination directory as argument and will keep a file per device, with most recent running version of a device. The GIT backend (recommended) will initialize an empty GIT repository in the specified path and create a new commit on every configuration change. The GIT-Crypt backend will also initialize a GIT repository but every configuration push to it will be encrypted on the fly by using git-crypt tool. Take a look at the Configuration for more details.\nMaps define how to map a model's fields to model model fields. Most of the settings should be self explanatory, log is ignored if use_syslog is set to true.\nFirst create the directory where the CSV output is going to store device configs and start Oxidized once.\nmkdir -p ~/.config/oxidized/configs\noxidized\nNow tell Oxidized where it finds a list of network devices to backup configuration from. You can either use CSV or SQLite as source. To create a CSV source add the following snippet:\nsource:\n  default: csv\n  csv:\n    file: ~/.config/oxidized/router.db\n    delimiter: !ruby/regexp /:/\n    map:\n      name: 0\n      model: 1\nNow lets create a file based device database (you might want to switch to SQLite later on). Put your routers in ~/.config/oxidized/router.db (file format is compatible with rancid). Simply add an item per line:\nrouter01.example.com:ios\nswitch01.example.com:procurve\nrouter02.example.com:ios\n\nRun oxidized again to take the first backups.\nExtra\nUbuntu init setup\nThe systemd service assumes that you have a user named 'oxidized' and that oxidized is in one of the following paths:\n/sbin\n/bin\n/usr/sbin\n/usr/bin\n/usr/local/bin\n\n\nCopy systemd service file from extra/ folder to /etc/systemd/system\n\nsudo cp extra/oxidized.service /etc/systemd/system\n\nSetup /var/run/\n\nmkdir /run/oxidized\nchown oxidized:oxidized /run/oxidized\n\nMake oxidized start on boot\n\nsudo systemctl enable oxidized.service\nHelp\nIf you need help with Oxidized then we have a few methods you can use to get in touch.\n\nGitter - You can join the Lobby on gitter to chat to other Oxidized users.\nGitHub - For help and requests for code changes / updates.\nForum - A user forum run by LibreNMS where you can ask for help and support.\n\nHelp Needed\nAs things stand right now, oxidized is maintained by very few people.\nWe would appreciate more individuals and companies getting involved in Oxidized.\nBeyond software development, documentation or maintenance of Oxidized, you could\nbecome a model maintainer, which can be done with little burden and would be a\nbig help to the community.\nInterested? Have a look at CONTRIBUTING.md.\nLicense and Copyright\n      Copyright\n      2013-2015 Saku Ytti <saku@ytti.fi>\n      2013-2015 Samer Abdel-Hafez <sam@arahant.net>\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.",
      "languages": {
        "ruby": 1,
        "SQL": 1,
        "Ruby": 1
      },
      "topics": [
        "SQL",
        "MySQL",
        "Ruby",
        "Docker"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T17:24:01.939156"
    },
    {
      "owner": "DataDog",
      "name": "dd-trace-rb",
      "url": "https://github.com/DataDog/dd-trace-rb",
      "description": "Datadog Tracing Ruby Client",
      "readme_content": "Datadog Trace Client\n\n\n\ndatadog is Datadog's client library for Ruby. It includes a suite of tools which provide visibility into the performance and security of Ruby applications, to enable Ruby developers to identify bottlenecks and other issues.\nGetting started\nIf you're upgrading from a 1.x version, check out the upgrade guide.\nFor a product overview, installation, and configuration check out our documentation.\nFor the gem API, check out our API documentation.\nFor descriptions of terminology used in APM, take a look at the APM Terms and Concepts.\nFor contributing, checkout the contribution guidelines and development guide.",
      "languages": {
        "ruby": 1,
        "Ruby": 1
      },
      "topics": [
        "Ruby"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T17:24:02.684764"
    },
    {
      "owner": "Homebrew",
      "name": "brew",
      "url": "https://github.com/Homebrew/brew",
      "description": "üç∫ The missing package manager for macOS (or Linux)",
      "readme_content": "Homebrew\n\nFeatures, usage and installation instructions are summarised on the homepage. Terminology (e.g. the difference between a Cellar, Tap, Cask and so forth) is explained here.\nWhat Packages Are Available?\n\nType brew formulae for a list.\nOr visit formulae.brew.sh to browse packages online.\n\nMore Documentation\nbrew help, man brew or check our documentation.\nTroubleshooting\nFirst, please run brew update and brew doctor.\nSecond, read the Troubleshooting Checklist.\nIf you don't read these it will take us far longer to help you with your problem.\nDonations\nHomebrew is a non-profit project run entirely by unpaid volunteers. We need your funds to pay for software, hardware and hosting around continuous integration and future improvements to the project. Every donation will be spent on making Homebrew better for our users.\nPlease consider a regular donation through GitHub Sponsors, Open Collective or Patreon. Homebrew is fiscally hosted by the Open Source Collective.\nFor questions about donations, including corporate giving, please email the Homebrew PLC at plc@brew.sh.\nCommunity\n\nHomebrew/discussions (forum)\n@homebrew@fosstodon.org (Mastodon)\n@MacHomebrew (ùïè (formerly known as Twitter))\n\nContributing\nWe'd love you to contribute to Homebrew. First, please read our Contribution Guide and Code of Conduct.\nWe explicitly welcome contributions from people who have never contributed to open-source before: we were all beginners once! We can help build on a partially working pull request with the aim of getting it merged. We are also actively seeking to diversify our contributors and especially welcome contributions from women from all backgrounds and people of colour.\nA good starting point for contributing is to first tap homebrew/core, then run brew audit --strict with some of the packages you use (e.g. brew audit --strict wget if you use wget) and read through the warnings. Try to fix them until brew audit --strict shows no results and submit a pull request. If no formulae you use have warnings you can run brew audit --strict without arguments to have it run on all packages and pick one.\nAlternatively, for something more substantial, check out one of the issues labelled help wanted in Homebrew/brew or Homebrew/homebrew-core.\nGood luck!\nSecurity\nPlease report security issues by filling in the security advisory form.\nWho We Are\nHomebrew's Project Leader is Mike McQuaid.\nHomebrew's Project Leadership Committee is Colin Dean, Michka Popoff, Mike McQuaid, Patrick Linnane and Vanessa Gennarelli.\nHomebrew's Technical Steering Committee is Bo Anderson, FX Coudert, Mike McQuaid and Rylan Polster.\nHomebrew's maintainers are Alexander Bayandin, Bevan Kay, Bo Anderson, Branch Vincent, Caleb Xu, Carlo Cabrera, Daeho Ro, Douglas Eichelberger, Dustin Rodrigues, Eric Knibbe, FX Coudert, Issy Long, Justin Krehel, Klaus Hipp, Markus Reiter, Michael Cho, Michka Popoff, Mike McQuaid, Nanda H Krishna, Patrick Linnane, Rui Chen, Ruoyu Zhong, Rylan Polster, Sam Ford, Sean Molenaar, ≈†tefan Baebler, Thierry Moisan, Timothy Sutton and William Woodruff.\nFormer maintainers with significant contributions include Miccal Matthews, Misty De M√©o, Shaun Jackman, V√≠tor Galv√£o, Claudia Pellegrino, Seeker, Jan Viljanen, JCount, commitay, Dominyk Tiller, Tim Smith, Baptiste Fontaine, Xu Cheng, Martin Afanasjew, Brett Koonce, Charlie Sharpsteen, Jack Nagel, Adam Vandenberg, Andrew Janke, Alex Dunn, neutric, Tomasz Pajor, Uladzislau Shablinski, Alyssa Ross, ilovezfs, Chongyu Zhu and Homebrew's creator: Max Howell.\nLicense\nCode is under the BSD 2-clause \"Simplified\" License.\nDocumentation is under the Creative Commons Attribution license.\nSponsors\nOur macOS continuous integration infrastructure is hosted by MacStadium's Orka.\n\nSecure password storage and syncing is provided by 1Password for Teams.\n\nhttps://brew.sh's DNS is resolving with DNSimple.\n\n\nHomebrew is generously supported by GitHub, Custom Ink, Randy Reddig, Codecademy, MacPaw Inc., mikadelbert, Workbrew and many other users and organisations via GitHub Sponsors.",
      "languages": {},
      "topics": [],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T17:24:03.500721"
    },
    {
      "owner": "decidim",
      "name": "decidim",
      "url": "https://github.com/decidim/decidim",
      "description": "The participatory democracy framework. A generator and multiple gems made with Ruby on Rails",
      "readme_content": "The participatory democracy framework\nFree Open-Source participatory democracy, citizen participation and open government for cities and organizations. Explore the docs ¬ª\nJoin our Matrix.org chat rooms.\n\nFeatures ¬∑\n    Roadmap ¬∑\n    Report Bug ¬∑\n    Propose New Features ¬∑\n    Read Blog\n\nüí° What is Decidim?\n\nDecidim is a participatory democracy framework, written in Ruby on Rails, originally developed for the Barcelona City government online and offline participation website.\nInstalling these libraries will provide you a generator and gems to help you develop web applications like the ones found on example applications or like our demo application.\n\n\nAll members of the Decidim community agree with Decidim Social Contract or Code of Democratic Guarantees.\n\n\nTable of Contents\n\n\n\nüöÄ Getting started\n\n\nüôå Contribute\n\n\nüß© Modules\n\n\n\nü™™ Identity verifications and authorizations\n\n\nüö™ Authentication options\n\n\n\n\n\nüìò License\n\n\nüîé Example applications\n\n\nüîí Security\n\n\nü´∂ Financial contributions\n\n\n\nüßë Members\n\n\nüíª Partners\n\n\n\n\n\nüìñ Learn More\n\n\nüé© Credits\n\n\n\n\n\nüöÄ Getting started\n\n\nTLDR: install gem, generate a Ruby on Rails app, enjoy.\n\n\n\ngem install decidim\ndecidim decidim_application\n\n\n\nWe have set up a guide on how to install, set up and upgrade Decidim.\nSee the Getting started guide.\n\n\n\n\nüôå Contribute\n\n\nAnyone can participate in Metadecidim, our own distance of Decidim for improving Decidim. The community is formed by people with different profiles and backgrounds.\n\n\nHow can you contribute? There are many ways to do it, some more specific to the software -improving documentation or translations, reporting bugs or proposing improvements-, but you can also participate in discussions about the governance of the community (find out how we organize ourselves).\n\n\nHaving a lively community is crucial to this project, so we encourage you to find out what is the best way for you to contribute to the commons! üå±\n\n\nRead more about contributions in our contribution guidelines.\n\n\n\n\nüß© Modules\n\n\nIf you need to have some features that we do not have yet, we recommend that you make a module.\nThis is a Ruby on Rails engine with some APIs specific to Decidim (for registering with the menus, integration with spaces like Participatory Processes or Assemblies, with /admin or /api, etc).\n\n\nAs a base you can use these modules, although check first that the version is compatible with your current Decidim version.\nAlso, you should know that until v1.0.0 we are under development, and these internal APIs can change.\n\n\nWe recommend that you extensively test your module.\n\n\nSee Modules page on Decidim.org.\n\n\nü™™ Identity verifications and authorizations\n\nOne specific thing regarding these kind of applications is how you manage the permissions that the participants will have in the platform (aka the authorization or verification logic). This tries to solve the problem of how to verify that the user is who they say they are and that they have the right to participate in this city or organization. Read more about Authorizations in our documentation.\n\n\n\nüö™ Authentication options\n\nYou can easily add any authentication provider to Decidim that is provided by OmniAuth. Also you have a list of modules related to authorization already developed by the community.\n\n\n\n\n\nüìò License\n\n\nIf you plan to put your application in production, you will need to publish it using the same license: GPL Affero 3.\n\n\nWe recommend doing that on GitHub (or any other code hosting platform) before publishing.\n\n\nYou can read more on \"Being Open Source From Day One is Especially Important for Government Projects\".\n\n\nIf you have any trouble you can contact us on our Matrix.org chat room for developers.\n\n\n\n\nüîé Example applications\n\n\nSince Decidim is a ruby gem, you can check out the dependent repositories to see how many applications are on the wild or tests that other developers have made. You can see a highlight of example applications in our documentation.\n\n\n\n\nüîí Security\n\n\nSecurity is very important to us.\nIf you have any issue regarding security, please disclose the information responsibly by sending an email to security [at] decidim [dot] org and not by creating a github/metadecidim issue.\nWe appreciate your effort to make Decidim more secure.\nSee full security policy.\n\n\n\n\nü´∂ Financial contributions\n\n\nDecidim helps citizens, organizations and public institutions to democratically self-organize at every scale. Thanks to Decidim, any organization is able to configure spaces for participation (initiatives, assemblies, or processes) and enrich them through the multiple available components (meetings, surveys, proposals, participatory budgets, accountability for results, comments, and many other).\n\n\nYou can contribute financially to the sustainability of this project through OpenCollective.\n\n\nThe funds will enable the maintainers to:\n\n\n\n\nreview community contributions\n\n\ntriage issues\n\n\nfix bugs related to performance\n\n\nimprove the design of the platform\n\n\nwrite better documentation\n\n\nimprove performance of the platform security\n\n\n\n\nüßë Members\n\nMembers have the right to participate in all the participation spaces of the Metadecidim platform with voice and vote, exercise their vote in strategic and internal decisions, elect or be elected in representative bodies, request and obtain explanations about the management of the positions of the Association, receive information about the activities and make common uses that are established. Read more about becoming a Decidim association member.\n\n\n\n\n\n\n\n\nüíª Partners\n\nAny organization offering services on Decidim can contribute back to the commons by becoming a Partner. Each Partner commits to include a clause in each new service contract around Decidim, explicitly stating that a small percentage is allocated to the maintenance of the source code. For a company, the percentage is 3%, and for a nonprofit organization, it is 1.5%.  Read more about becoming a Decidim association partner.\n\n\n\n\n\n\n\n\n\n\nüìñ Learn More\n\n\n\n\nDecidim Resource\nDescription\n\n\n\n\nüöÄ Our latest releases\nNew features and bug fixes.\n\n\nüß© Modules\nFind out new ways of enhancing Decidim.\n\n\nüó≥ Propose new Features\nIs there any missing feature? Propose a new one!\n\n\nüìì Docs\nFull documentation for creating and customizing your own Decidim application.\n\n\nüìí API Reference\nDetailed reference on Decidim‚Äôs API.\n\n\nüîé Examples\nSee some ways where Decidim is used, with code examples.\n\n\nüì¨ Blog\nAll the latest news and releases from Decidim.\n\n\nüí¨ Join Matrix.org\nNeed help with your specific use case? Say hi on Matrix!\n\n\nüó∫ Roadmap\nSee where Decidim is working to build new features.\n\n\nüôå Contribute\nHow to contribute to the Decidim project and code base.\n\n\n\n\n\n\nüé© Credits\n\n\n\n\nThis project is tested with BrowserStack.",
      "languages": {
        "ruby": 1,
        "Ruby": 1
      },
      "topics": [
        "Ruby"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T17:24:04.489905"
    },
    {
      "owner": "chatwoot",
      "name": "chatwoot",
      "url": "https://github.com/chatwoot/chatwoot",
      "description": "Open-source live-chat, email support, omni-channel desk. An alternative to Intercom, Zendesk, Salesforce Service Cloud etc. üî•üí¨",
      "readme_content": "üö® Note: This branch is unstable. For the stable branch's source code, please use the branch 3.x\n\n\n\nChatwoot\nCustomer engagement suite, an open-source alternative to Intercom, Zendesk, Salesforce Service Cloud etc.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChatwoot is an open-source, self-hosted customer engagement suite. Chatwoot lets you view and manage your customer data, communicate with them irrespective of which medium they use, and re-engage them based on their profile.\nFeatures\nChatwoot supports the following conversation channels:\n\nWebsite: Talk to your customers using our live chat widget and make use of our SDK to identify a user and provide contextual support.\nFacebook: Connect your Facebook pages and start replying to the direct messages to your page.\nInstagram: Connect your Instagram profile and start replying to the direct messages.\nTwitter: Connect your Twitter profiles and reply to direct messages or the tweets where you are mentioned.\nTelegram: Connect your Telegram bot and reply to your customers right from a single dashboard.\nWhatsApp: Connect your WhatsApp business account and manage the conversation in Chatwoot.\nLine: Connect your Line account and manage the conversations in Chatwoot.\nSMS: Connect your Twilio SMS account and reply to the SMS queries in Chatwoot.\nAPI Channel: Build custom communication channels using our API channel.\nEmail: Forward all your email queries to Chatwoot and view it in our integrated dashboard.\n\nAnd more.\nOther features include:\n\nCRM: Save all your customer information right inside Chatwoot, use contact notes to log emails, phone calls, or meeting notes.\nCustom Attributes: Define custom attribute attributes to store information about a contact or a conversation and extend the product to match your workflow.\nShared multi-brand inboxes: Manage multiple brands or pages using a shared inbox.\nPrivate notes: Use @mentions and private notes to communicate internally about a conversation.\nCanned responses (Saved replies): Improve the response rate by adding saved replies for frequently asked questions.\nConversation Labels: Use conversation labels to create custom workflows.\nAuto assignment: Chatwoot intelligently assigns a ticket to the agents who have access to the inbox depending on their availability and load.\nConversation continuity: If the user has provided an email address through the chat widget, Chatwoot will send an email to the customer under the agent name so that the user can continue the conversation over the email.\nMulti-lingual support: Chatwoot supports 10+ languages.\nPowerful API & Webhooks: Extend the capability of the software using Chatwoot‚Äôs webhooks and APIs.\nIntegrations: Chatwoot natively integrates with Slack right now. Manage your conversations in Slack without logging into the dashboard.\n\nDocumentation\nDetailed documentation is available at chatwoot.com/help-center.\nTranslation process\nThe translation process for Chatwoot web and mobile app is managed at https://translate.chatwoot.com using Crowdin. Please read the translation guide for contributing to Chatwoot.\nBranching model\nWe use the git-flow branching model. The base branch is develop.\nIf you are looking for a stable version, please use the master or tags labelled as v1.x.x.\nDeployment\nHeroku one-click deploy\nDeploying Chatwoot to Heroku is a breeze. It's as simple as clicking this button:\n\nFollow this link to understand setting the correct environment variables for the app to work with all the features. There might be breakages if you do not set the relevant environment variables.\nDigitalOcean 1-Click Kubernetes deployment\nChatwoot now supports 1-Click deployment to DigitalOcean as a kubernetes app.\n\n\n\nOther deployment options\nFor other supported options, checkout our deployment page.\nSecurity\nLooking to report a vulnerability? Please refer our SECURITY.md file.\nCommunity? Questions? Support ?\nIf you need help or just want to hang out, come, say hi on our Discord server.\nContributors ‚ú®\nThanks goes to all these wonderful people:\n\nChatwoot ¬© 2017-2025, Chatwoot Inc - Released under the MIT License.",
      "languages": {},
      "topics": [
        "Kubernetes"
      ],
      "stars": 0,
      "forks": 0,
      "last_updated": "2025-02-22T17:24:05.588292"
    }
  ],
  "user_profiles": {
    "chanakya2006": {
      "username": "chanakya2006",
      "bio": "",
      "readme_content": "",
      "repositories": [
        "github-repo-recommendation-on-basis-of-profile",
        "fitness_api",
        "pdf_chatbot",
        "python"
      ],
      "top_languages": {
        "Go": 1,
        "go": 1,
        "GO": 1,
        "python": 1,
        "Python": 1
      },
      "top_topics": {
        "Go": 1,
        "Python": 1
      },
      "last_updated": "2025-02-22T12:46:46.092174"
    },
    "jaseemuddinn": {
      "username": "jaseemuddinn",
      "bio": "",
      "readme_content": "Hi there, I am Jaseemuddin Naseem üëã\n\n\n\n  \n\n\nüéØ Portfolio website: Portfolio\n‚ö° Fun fact: The first rule of programming- if it works, don‚Äôt touch it.ü§ì\n\nüíª Thing(s) I love\n\nWeb Development \n(Figuring out) ü§ì\n  \n\n\nüõ†Tech Stack\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚≠ê : Show some ¬†‚ù§Ô∏è¬† to my repositories!",
      "repositories": [
        "onnoff_revamped",
        "TheAce",
        "calenderApp",
        "onnoff",
        "face_recog",
        "theace_v2",
        "Edulnnova",
        "VeraLink",
        "pdsalon",
        "irada_revamped"
      ],
      "top_languages": {},
      "top_topics": {
        "Web Development": 1
      },
      "last_updated": "2025-02-22T13:02:58.527129"
    },
    "404avinotfound": {
      "username": "404avinotfound",
      "bio": "//Greatings\n#include <stdio.h>\n\nvoid main() {\n\n     printf(\"Hi\");   \n     \n     return 0;\n}",
      "readme_content": "",
      "repositories": [
        "Coding-and-Decoding"
      ],
      "top_languages": {},
      "top_topics": {},
      "last_updated": "2025-02-22T13:05:15.135008"
    },
    "hello": {
      "username": "hello",
      "bio": "",
      "readme_content": "",
      "repositories": [],
      "top_languages": {},
      "top_topics": {},
      "last_updated": "2025-02-22T14:48:45.565776"
    },
    "1": {
      "username": "1",
      "bio": "",
      "readme_content": "Grit\nGrit is no longer maintained. Check out rugged.\nGrit gives you object oriented read/write access to Git repositories via Ruby.\nThe main goals are stability and performance. To this end, some of the\ninteractions with Git repositories are done by shelling out to the system's\ngit command, and other interactions are done with pure Ruby\nreimplementations of core Git functionality. This choice, however, is\ntransparent to end users, and you need not know which method is being used.\nThis software was developed to power GitHub, and should be considered\nproduction ready. An extensive test suite is provided to verify its\ncorrectness.\nGrit is maintained by Tom Preston-Werner, Scott Chacon, Chris Wanstrath, and\nPJ Hyett.\nThis documentation is accurate as of Grit 2.3.\nRequirements\n\ngit (http://git-scm.com) tested with 1.7.2.1\n\nInstall\nEasiest install is via RubyGems:\n$ gem install grit\n\nSource\nGrit's Git repo is available on GitHub, which can be browsed at:\nhttp://github.com/mojombo/grit\n\nand cloned with:\ngit clone git://github.com/mojombo/grit.git\n\nDevelopment\nYou will need these gems to get tests to pass:\n\nmocha\n\nContributing\nIf you'd like to hack on Grit, follow these instructions. To get all of the dependencies, install the gem first.\n\nFork the project to your own account\nClone down your fork\nCreate a thoughtfully named topic branch to contain your change\nHack away\nAdd tests and make sure everything still passes by running rake\nIf you are adding new functionality, document it in README.md\nDo not change the version number, I will do that on my end\nIf necessary, rebase your commits into logical chunks, without errors\nPush the branch up to GitHub\nSend a pull request for your branch\n\nUsage\nGrit gives you object model access to your Git repositories. Once you have\ncreated a Repo object, you can traverse it to find parent commits,\ntrees, blobs, etc.\nInitialize a Repo object\nThe first step is to create a Grit::Repo object to represent your repo. In\nthis documentation I include the Grit module to reduce typing.\nrequire 'grit'\nrepo = Grit::Repo.new(\"/Users/tom/dev/grit\")\n\nIn the above example, the directory /Users/tom/dev/grit is my working\ndirectory and contains the .git directory. You can also initialize Grit with\na bare repo.\nrepo = Repo.new(\"/var/git/grit.git\")\n\nGetting a list of commits\nFrom the Repo object, you can get a list of commits as an array of Commit\nobjects.\nrepo.commits\n# => [#<Grit::Commit \"e80bbd2ce67651aa18e57fb0b43618ad4baf7750\">,\n      #<Grit::Commit \"91169e1f5fa4de2eaea3f176461f5dc784796769\">,\n      #<Grit::Commit \"038af8c329ef7c1bae4568b98bd5c58510465493\">,\n      #<Grit::Commit \"40d3057d09a7a4d61059bca9dca5ae698de58cbe\">,\n      #<Grit::Commit \"4ea50f4754937bf19461af58ce3b3d24c77311d9\">]\n\nCalled without arguments, Repo#commits returns a list of up to ten commits\nreachable by the master branch (starting at the latest commit). You can\nask for commits beginning at a different branch, commit, tag, etc.\nrepo.commits('mybranch')\nrepo.commits('40d3057d09a7a4d61059bca9dca5ae698de58cbe')\nrepo.commits('v0.1')\n\nYou can specify the maximum number of commits to return.\nrepo.commits('master', 100)\n\nIf you need paging, you can specify a number of commits to skip.\nrepo.commits('master', 10, 20)\n\nThe above will return commits 21-30 from the commit list.\nThe Commit object\nCommit objects contain information about that commit.\nhead = repo.commits.first\n\nhead.id\n# => \"e80bbd2ce67651aa18e57fb0b43618ad4baf7750\"\n\nhead.parents\n# => [#<Grit::Commit \"91169e1f5fa4de2eaea3f176461f5dc784796769\">]\n\nhead.tree\n# => #<Grit::Tree \"3536eb9abac69c3e4db583ad38f3d30f8db4771f\">\n\nhead.author\n# => #<Grit::Actor \"Tom Preston-Werner <tom@mojombo.com>\">\n\nhead.authored_date\n# => Wed Oct 24 22:02:31 -0700 2007\n\nhead.committer\n# => #<Grit::Actor \"Tom Preston-Werner <tom@mojombo.com>\">\n\nhead.committed_date\n# => Wed Oct 24 22:02:31 -0700 2007\n\nhead.message\n# => \"add Actor inspect\"\n\nYou can traverse a commit's ancestry by chaining calls to #parents.\nrepo.commits.first.parents[0].parents[0].parents[0]\n\nThe above corresponds to master^^^ or master~3 in Git parlance.\nThe Tree object\nA tree records pointers to the contents of a directory. Let's say you want\nthe root tree of the latest commit on the master branch.\ntree = repo.commits.first.tree\n# => #<Grit::Tree \"3536eb9abac69c3e4db583ad38f3d30f8db4771f\">\n\ntree.id\n# => \"3536eb9abac69c3e4db583ad38f3d30f8db4771f\"\n\nOnce you have a tree, you can get the contents.\ncontents = tree.contents\n# => [#<Grit::Blob \"4ebc8aea50e0a67e000ba29a30809d0a7b9b2666\">,\n      #<Grit::Blob \"81d2c27608b352814cbe979a6acd678d30219678\">,\n      #<Grit::Tree \"c3d07b0083f01a6e1ac969a0f32b8d06f20c62e5\">,\n      #<Grit::Tree \"4d00fe177a8407dbbc64a24dbfc564762c0922d8\">]\n\nThis tree contains two Blob objects and two Tree objects. The trees are\nsubdirectories and the blobs are files. Trees below the root have additional\nattributes.\ncontents.last.name\n# => \"lib\"\n\ncontents.last.mode\n# => \"040000\"\n\nThere is a convenience method that allows you to get a named sub-object\nfrom a tree.\ntree / \"lib\"\n# => #<Grit::Tree \"e74893a3d8a25cbb1367cf241cc741bfd503c4b2\">\n\nYou can also get a tree directly from the repo if you know its name.\nrepo.tree\n# => #<Grit::Tree \"master\">\n\nrepo.tree(\"91169e1f5fa4de2eaea3f176461f5dc784796769\")\n# => #<Grit::Tree \"91169e1f5fa4de2eaea3f176461f5dc784796769\">\n\nThe Blob object\nA blob represents a file. Trees often contain blobs.\nblob = tree.contents.first\n# => #<Grit::Blob \"4ebc8aea50e0a67e000ba29a30809d0a7b9b2666\">\n\nA blob has certain attributes.\nblob.id\n# => \"4ebc8aea50e0a67e000ba29a30809d0a7b9b2666\"\n\nblob.name\n# => \"README.txt\"\n\nblob.mode\n# => \"100644\"\n\nblob.size\n# => 7726\n\nYou can get the data of a blob as a string.\nblob.data\n# => \"Grit is a library to ...\"\n\nYou can also get a blob directly from the repo if you know its name.\nrepo.blob(\"4ebc8aea50e0a67e000ba29a30809d0a7b9b2666\")\n# => #<Grit::Blob \"4ebc8aea50e0a67e000ba29a30809d0a7b9b2666\">\n\nOther\nThere are many more API methods available that are not documented here. Please\nreference the code for more functionality.\nCopyright\nCopyright (c) 2010 Tom Preston-Werner. See LICENSE for details.",
      "repositories": [
        "discourse_docker"
      ],
      "top_languages": {
        "ruby": 1,
        "Ruby": 1,
        "R": 1,
        "go": 1,
        "Shell": 1,
        "Go": 1,
        "GO": 1
      },
      "top_topics": {
        "Ruby": 1,
        "Docker": 1,
        "Redis": 1,
        "R": 1,
        "Shell": 1,
        "Go": 1
      },
      "last_updated": "2025-02-22T17:23:38.687947"
    }
  }
}